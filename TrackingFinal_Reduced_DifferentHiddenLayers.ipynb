{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------mlp-LSTM- GRU- Tracking Models--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#From Pandas\n",
    "from pandas import read_csv\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "\n",
    "#From Keras\n",
    "from keras.models import Sequential, load_model \n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM,GRU\n",
    "\n",
    "\n",
    "#From sklearn \n",
    "from sklearn import preprocessing  \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error \n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "#Import Files\n",
    "import math\n",
    "import time\n",
    "import pandas as pd #define the data structures\n",
    "import matplotlib as plt #for visualization\n",
    "import numpy\n",
    "import numpy as np #for matrix multiplication\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go \n",
    "import seaborn as sns\n",
    "import plotly.offline as py\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler #for normalizing our data(scaling)\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.model_selection import train_test_split   \n",
    "\n",
    "\n",
    "from math import sqrt\n",
    "from numpy import concatenate \n",
    "from datetime import datetime\n",
    "from pandas.plotting import scatter_matrix\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from sklearn import preprocessing \n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.model_selection import train_test_split   \n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error \n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "\n",
    "\n",
    "#Import others\n",
    "py.init_notebook_mode(connected=True)\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------Read Data------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserCode</th>\n",
       "      <th>UAV1</th>\n",
       "      <th>UAV2</th>\n",
       "      <th>UAV3</th>\n",
       "      <th>AP1</th>\n",
       "      <th>AP2</th>\n",
       "      <th>AP3</th>\n",
       "      <th>AP4</th>\n",
       "      <th>AP5</th>\n",
       "      <th>AP6</th>\n",
       "      <th>...</th>\n",
       "      <th>AP29</th>\n",
       "      <th>AP30</th>\n",
       "      <th>CH1</th>\n",
       "      <th>CH2</th>\n",
       "      <th>CH3</th>\n",
       "      <th>CH4</th>\n",
       "      <th>CH5</th>\n",
       "      <th>Hight</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>-49.5</td>\n",
       "      <td>-64.4</td>\n",
       "      <td>-67.4</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>6.70</td>\n",
       "      <td>-1.39</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>157.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>-49.1</td>\n",
       "      <td>-66.6</td>\n",
       "      <td>-72.6</td>\n",
       "      <td>3.42</td>\n",
       "      <td>1.83</td>\n",
       "      <td>-1.63</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>7.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>-52.9</td>\n",
       "      <td>-67.9</td>\n",
       "      <td>-75.9</td>\n",
       "      <td>4.55</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.63</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>-49.5</td>\n",
       "      <td>-67.5</td>\n",
       "      <td>-72.5</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.06</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>-66.1</td>\n",
       "      <td>-76.1</td>\n",
       "      <td>5.04</td>\n",
       "      <td>1.39</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     UserCode  UAV1  UAV2  UAV3   AP1   AP2   AP3   AP4   AP5   AP6 ...   \\\n",
       "1.0     101.0 -49.5 -64.4 -67.4 -0.52  6.70 -1.39  0.64  0.29 -0.60 ...    \n",
       "2.0     101.0 -49.1 -66.6 -72.6  3.42  1.83 -1.63 -0.37 -0.30  0.03 ...    \n",
       "3.0     101.0 -52.9 -67.9 -75.9  4.55  3.17  0.08  0.69  0.63 -0.51 ...    \n",
       "4.0     101.0 -49.5 -67.5 -72.5  3.25  2.06 -0.88  0.80  0.31 -0.11 ...    \n",
       "5.0     101.0 -50.0 -66.1 -76.1  5.04  1.39 -0.57 -0.03 -0.10 -0.15 ...    \n",
       "\n",
       "     AP29  AP30    CH1    CH2    CH3    CH4    CH5  Hight      X    Y  \n",
       "1.0  0.05  0.04  157.0  157.0  157.0  157.0  157.0   60.0  200.0  1.0  \n",
       "2.0  0.03 -0.16    7.0  157.0    7.0    7.0  157.0   60.0  197.0  1.0  \n",
       "3.0 -0.23 -0.15    7.0    7.0    7.0    7.0  157.0   60.0  194.0  1.0  \n",
       "4.0  0.01 -0.09    7.0    7.0  157.0    1.0    7.0   60.0  191.0  1.0  \n",
       "5.0  0.09  0.07    7.0    1.0  157.0    7.0    2.0   60.0  188.0  1.0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import or read the datasets\n",
    "df2= pd.read_csv('TrackDataAllReduced.csv',index_col=0)\n",
    "#df2=df2.drop(df2.columns[df2.columns.str.contains('unnamed',case = False)],axis = 1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23445 23445\n"
     ]
    }
   ],
   "source": [
    "Y=df2.iloc[:,-2:]\n",
    "X=df2.iloc[:,:-2]\n",
    "\n",
    "# conversion to numpy array\n",
    "x, y = X.values, Y.values  \n",
    "\n",
    "# scaling values for model\n",
    "x_scale = MinMaxScaler()\n",
    "y_scale = MinMaxScaler() \n",
    "\n",
    "\n",
    "X = x_scale.fit_transform(x)\n",
    "Y = y_scale.fit_transform(y)   \n",
    "print(len(X),len(Y))\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.05) \n",
    "#print(X_train.shape,X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23445, 40) (23445, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train= X, Y \n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "#X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserCode</th>\n",
       "      <th>UAV1</th>\n",
       "      <th>UAV2</th>\n",
       "      <th>UAV3</th>\n",
       "      <th>AP1</th>\n",
       "      <th>AP2</th>\n",
       "      <th>AP3</th>\n",
       "      <th>AP4</th>\n",
       "      <th>AP5</th>\n",
       "      <th>AP6</th>\n",
       "      <th>...</th>\n",
       "      <th>AP29</th>\n",
       "      <th>AP30</th>\n",
       "      <th>CH1</th>\n",
       "      <th>CH2</th>\n",
       "      <th>CH3</th>\n",
       "      <th>CH4</th>\n",
       "      <th>CH5</th>\n",
       "      <th>Hight</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104</td>\n",
       "      <td>-48.3</td>\n",
       "      <td>-74.7</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>7.53</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>153</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104</td>\n",
       "      <td>-54.3</td>\n",
       "      <td>-82.1</td>\n",
       "      <td>-78.6</td>\n",
       "      <td>6.16</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111</td>\n",
       "      <td>-82.1</td>\n",
       "      <td>-70.8</td>\n",
       "      <td>-80.8</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103</td>\n",
       "      <td>-76.5</td>\n",
       "      <td>-78.8</td>\n",
       "      <td>-82.8</td>\n",
       "      <td>6.97</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>...</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>115</td>\n",
       "      <td>-48.3</td>\n",
       "      <td>-75.6</td>\n",
       "      <td>-73.5</td>\n",
       "      <td>5.17</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.43</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserCode  UAV1  UAV2  UAV3   AP1   AP2   AP3   AP4   AP5   AP6 ...  AP29  \\\n",
       "1       104 -48.3 -74.7 -67.0  7.53  0.62  1.51  0.68 -0.02 -0.65 ... -0.26   \n",
       "2       104 -54.3 -82.1 -78.6  6.16 -0.31 -0.08  0.62  0.34 -1.20 ... -0.21   \n",
       "3       111 -82.1 -70.8 -80.8  7.73 -0.28  1.25  0.18 -1.15  0.30 ...  0.50   \n",
       "4       103 -76.5 -78.8 -82.8  6.97 -0.23 -0.17  2.18  0.08 -0.98 ...  0.34   \n",
       "5       115 -48.3 -75.6 -73.5  5.17  1.16  0.96  1.47  1.43 -0.37 ... -0.33   \n",
       "\n",
       "   AP30  CH1  CH2  CH3  CH4  CH5  Hight  X   Y  \n",
       "1 -0.17    1   11  153    6    6     40  1   6  \n",
       "2  0.50    1   11    6    3    8     40  1  12  \n",
       "3 -0.35    1    1    1    1    6     60  1  15  \n",
       "4 -0.86    3    3    3    3  100     60  1  21  \n",
       "5 -0.58   11   11    1  153    2     40  1  24  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import or read the datasets\n",
    "pred = pd.read_csv('TestReduced.csv',index_col=0)\n",
    "#pred=pred2.drop(pred2.columns[pred2.columns.str.contains('unnamed',case = False)],axis = 1)\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abebe Belay\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning:\n",
      "\n",
      "Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_pred=pred.iloc[:,:-2]\n",
    "Y_pred=pred.iloc[:,-2:]\n",
    "x_pred, y_pred = X_pred.values, Y_pred.values  \n",
    "\n",
    "# scaling values for model\n",
    "x_pred_scale = MinMaxScaler()\n",
    "y_pred_scale = MinMaxScaler() \n",
    "\n",
    "Xx__pred = x_pred_scale.fit_transform(x_pred)\n",
    "Yy_pred = y_pred_scale.fit_transform(y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.21428571, 0.75877193, 0.17165669, ..., 0.02941176,\n",
       "         0.03030303, 0.        ]],\n",
       "\n",
       "       [[0.21428571, 0.62719298, 0.0239521 , ..., 0.01176471,\n",
       "         0.04242424, 0.        ]],\n",
       "\n",
       "       [[0.71428571, 0.01754386, 0.249501  , ..., 0.        ,\n",
       "         0.03030303, 1.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.5       , 0.73684211, 0.58283433, ..., 0.92941176,\n",
       "         0.10909091, 0.        ]],\n",
       "\n",
       "       [[1.        , 0.75877193, 0.67664671, ..., 0.88235294,\n",
       "         0.69090909, 1.        ]],\n",
       "\n",
       "       [[0.42857143, 0.75877193, 0.66866267, ..., 0.81176471,\n",
       "         0.95757576, 1.        ]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_x = np.reshape(Xx__pred, (Xx__pred.shape[0], 1, Xx__pred.shape[1])) \n",
    "pred_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------GRU model defining--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(feature):\n",
    "    input_shapes=(None, feature)\n",
    "    \n",
    "    print(input_shapes)\n",
    "    model_name3 = 'Tracking_GRU_' \n",
    "    model3='gru'\n",
    "    model3 = Sequential()\n",
    "    model3.add(GRU(units=512, return_sequences=True, input_shape=input_shapes))\n",
    "    model3.add(Dropout(0.2))\n",
    "    model3.add(GRU(units=256))\n",
    "    model3.add(Dropout(0.2))\n",
    "    model3.add(Dense(2, activation='tanh'))\n",
    "    #model.add(Dense(4, activation='relu'))\n",
    "    #model.add(Dense(4, activation='softmax'))\n",
    "    model3.compile(loss='MAE', optimizer='adamax', metrics=['accuracy']) \n",
    "    #model.compile(loss='mse',optimizer='rmsprop')\n",
    "    #model.compile(loss='mse', optimizer='sgd') \n",
    "    return model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(feature):\n",
    "    input_shapes=(None, feature)\n",
    "    \n",
    "    print(input_shapes)\n",
    "    model_name4 = 'Tracking_GRU_' \n",
    "    model4='gru'\n",
    "    model4 = Sequential()\n",
    "    model4.add(GRU(units=512, return_sequences=True, input_shape=input_shapes))\n",
    "    model4.add(Dropout(0.2))\n",
    "    model4.add(GRU(units=256,return_sequences=True))\n",
    "    model4.add(Dropout(0.2))\n",
    "    model4.add(GRU(units=256))\n",
    "    model4.add(Dropout(0.2))\n",
    "    model4.add(Dense(2, activation='tanh'))\n",
    "    #model.add(Dense(4, activation='relu'))\n",
    "    #model.add(Dense(4, activation='softmax'))\n",
    "    model4.compile(loss='MAE', optimizer='adamax', metrics=['accuracy']) \n",
    "    #model.compile(loss='mse',optimizer='rmsprop')\n",
    "    #model.compile(loss='mse', optimizer='sgd') \n",
    "    return model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(feature):\n",
    "    input_shapes=(None, feature)\n",
    "    \n",
    "    print(input_shapes)\n",
    "    model_name5 = 'Tracking_GRU_' \n",
    "    model5='gru'\n",
    "    model5 = Sequential()\n",
    "    model5.add(GRU(units=512, return_sequences=True, input_shape=input_shapes))\n",
    "    model5.add(Dropout(0.2))\n",
    "    model5.add(GRU(units=256,return_sequences=True))\n",
    "    model5.add(Dropout(0.2))\n",
    "    model5.add(GRU(units=256,return_sequences=True))\n",
    "    model5.add(Dropout(0.2))\n",
    "    model5.add(GRU(units=256))\n",
    "    model5.add(Dropout(0.2))\n",
    "    model5.add(Dense(2, activation='tanh'))\n",
    "    #model.add(Dense(4, activation='relu'))\n",
    "    #model.add(Dense(4, activation='softmax'))\n",
    "    model5.compile(loss='MAE', optimizer='adamax', metrics=['accuracy']) \n",
    "    #model.compile(loss='mse',optimizer='rmsprop')\n",
    "    #model.compile(loss='mse', optimizer='sgd') \n",
    "    return model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(feature):\n",
    "    input_shapes=(None, feature)\n",
    "    \n",
    "    print(input_shapes)\n",
    "    model_name6 = 'Tracking_GRU_' \n",
    "    model6='gru'\n",
    "    model6 = Sequential()\n",
    "    model6.add(GRU(units=512, return_sequences=True, input_shape=input_shapes))\n",
    "    model6.add(Dropout(0.2))\n",
    "    model6.add(GRU(units=256,return_sequences=True))\n",
    "    model6.add(Dropout(0.2))\n",
    "    model6.add(GRU(units=256,return_sequences=True))\n",
    "    model6.add(Dropout(0.2))\n",
    "    model6.add(GRU(units=256,return_sequences=True))\n",
    "    model6.add(Dropout(0.2))\n",
    "    model6.add(GRU(units=256))\n",
    "    model6.add(Dropout(0.2))\n",
    "    model6.add(Dense(2, activation='tanh'))\n",
    "    #model.add(Dense(4, activation='relu'))\n",
    "    #model.add(Dense(4, activation='softmax'))\n",
    "    model6.compile(loss='MAE', optimizer='adamax', metrics=['accuracy']) \n",
    "    #model.compile(loss='mse',optimizer='rmsprop')\n",
    "    #model.compile(loss='mse', optimizer='sgd') \n",
    "    return model6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 40)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_3 (GRU)                  (None, None, 512)         849408    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, None, 256)         590592    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, None, 256)         393984    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, None, 256)         393984    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 2,622,466\n",
      "Trainable params: 2,622,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3=gru(X_train.shape[2])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 40)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_8 (GRU)                  (None, None, 512)         849408    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, None, 256)         590592    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "gru_10 (GRU)                 (None, None, 256)         393984    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (None, None, 256)         393984    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 2,622,466\n",
      "Trainable params: 2,622,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4=gru(X_train.shape[2])\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 40)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_13 (GRU)                 (None, None, 512)         849408    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "gru_14 (GRU)                 (None, None, 256)         590592    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "gru_15 (GRU)                 (None, None, 256)         393984    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "gru_16 (GRU)                 (None, None, 256)         393984    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "gru_17 (GRU)                 (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 2,622,466\n",
      "Trainable params: 2,622,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5=gru(X_train.shape[2])\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 40)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_18 (GRU)                 (None, None, 512)         849408    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "gru_19 (GRU)                 (None, None, 256)         590592    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "gru_20 (GRU)                 (None, None, 256)         393984    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "gru_21 (GRU)                 (None, None, 256)         393984    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "gru_22 (GRU)                 (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 2,622,466\n",
      "Trainable params: 2,622,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model6=gru(X_train.shape[2])\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22272 samples, validate on 1173 samples\n",
      "Epoch 1/500\n",
      "22272/22272 [==============================] - 23s 1ms/step - loss: 0.2357 - acc: 0.7503 - val_loss: 0.2208 - val_acc: 0.7298\n",
      "Epoch 2/500\n",
      "22272/22272 [==============================] - 18s 827us/step - loss: 0.2125 - acc: 0.7504 - val_loss: 0.2155 - val_acc: 0.7212\n",
      "Epoch 3/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.2047 - acc: 0.7461 - val_loss: 0.2042 - val_acc: 0.7289\n",
      "Epoch 4/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1994 - acc: 0.7532 - val_loss: 0.2025 - val_acc: 0.7323\n",
      "Epoch 5/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.1974 - acc: 0.7566 - val_loss: 0.2040 - val_acc: 0.6939\n",
      "Epoch 6/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1964 - acc: 0.7545 - val_loss: 0.2028 - val_acc: 0.7460\n",
      "Epoch 7/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.1924 - acc: 0.7538 - val_loss: 0.1952 - val_acc: 0.7408\n",
      "Epoch 8/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1912 - acc: 0.7544 - val_loss: 0.1919 - val_acc: 0.7477\n",
      "Epoch 9/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.1865 - acc: 0.7564 - val_loss: 0.1905 - val_acc: 0.7315\n",
      "Epoch 10/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.1810 - acc: 0.7619 - val_loss: 0.1874 - val_acc: 0.7451\n",
      "Epoch 11/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.1795 - acc: 0.7650 - val_loss: 0.1828 - val_acc: 0.7383\n",
      "Epoch 12/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.1755 - acc: 0.7709 - val_loss: 0.1795 - val_acc: 0.7553\n",
      "Epoch 13/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.1738 - acc: 0.7770 - val_loss: 0.1762 - val_acc: 0.7681\n",
      "Epoch 14/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.1711 - acc: 0.7830 - val_loss: 0.1741 - val_acc: 0.7681\n",
      "Epoch 15/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.1696 - acc: 0.7841 - val_loss: 0.1743 - val_acc: 0.7681\n",
      "Epoch 16/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1678 - acc: 0.7848 - val_loss: 0.1723 - val_acc: 0.7758\n",
      "Epoch 17/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.1649 - acc: 0.7915 - val_loss: 0.1711 - val_acc: 0.7758\n",
      "Epoch 18/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.1640 - acc: 0.7915 - val_loss: 0.1695 - val_acc: 0.7630\n",
      "Epoch 19/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.1624 - acc: 0.7927 - val_loss: 0.1674 - val_acc: 0.7604\n",
      "Epoch 20/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.1609 - acc: 0.7942 - val_loss: 0.1718 - val_acc: 0.7792\n",
      "Epoch 21/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.1597 - acc: 0.7963 - val_loss: 0.1662 - val_acc: 0.7613\n",
      "Epoch 22/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1582 - acc: 0.7975 - val_loss: 0.1646 - val_acc: 0.7656\n",
      "Epoch 23/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.1573 - acc: 0.8007 - val_loss: 0.1618 - val_acc: 0.7826\n",
      "Epoch 24/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1558 - acc: 0.7990 - val_loss: 0.1620 - val_acc: 0.7826\n",
      "Epoch 25/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.1552 - acc: 0.7995 - val_loss: 0.1586 - val_acc: 0.7758\n",
      "Epoch 26/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.1528 - acc: 0.8043 - val_loss: 0.1569 - val_acc: 0.7809\n",
      "Epoch 27/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.1515 - acc: 0.8069 - val_loss: 0.1652 - val_acc: 0.7911\n",
      "Epoch 28/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.1508 - acc: 0.8066 - val_loss: 0.1550 - val_acc: 0.7903\n",
      "Epoch 29/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.1494 - acc: 0.8095 - val_loss: 0.1535 - val_acc: 0.7945\n",
      "Epoch 30/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.1491 - acc: 0.8085 - val_loss: 0.1519 - val_acc: 0.7980\n",
      "Epoch 31/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.1471 - acc: 0.8121 - val_loss: 0.1514 - val_acc: 0.7988\n",
      "Epoch 32/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1462 - acc: 0.8132 - val_loss: 0.1530 - val_acc: 0.7920\n",
      "Epoch 33/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1458 - acc: 0.8153 - val_loss: 0.1488 - val_acc: 0.8039\n",
      "Epoch 34/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.1450 - acc: 0.8134 - val_loss: 0.1503 - val_acc: 0.7980\n",
      "Epoch 35/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.1433 - acc: 0.8173 - val_loss: 0.1505 - val_acc: 0.8031\n",
      "Epoch 36/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.1417 - acc: 0.8213 - val_loss: 0.1515 - val_acc: 0.8039\n",
      "Epoch 37/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.1418 - acc: 0.8200 - val_loss: 0.1444 - val_acc: 0.8167\n",
      "Epoch 38/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.1397 - acc: 0.8231 - val_loss: 0.1470 - val_acc: 0.7988\n",
      "Epoch 39/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1394 - acc: 0.8264 - val_loss: 0.1466 - val_acc: 0.8107\n",
      "Epoch 40/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.1393 - acc: 0.8250 - val_loss: 0.1471 - val_acc: 0.8014\n",
      "Epoch 41/500\n",
      "22272/22272 [==============================] - 16s 710us/step - loss: 0.1376 - acc: 0.8266 - val_loss: 0.1429 - val_acc: 0.8218\n",
      "Epoch 42/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.1366 - acc: 0.8286 - val_loss: 0.1409 - val_acc: 0.8218\n",
      "Epoch 43/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.1355 - acc: 0.8288 - val_loss: 0.1512 - val_acc: 0.8201\n",
      "Epoch 44/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.1347 - acc: 0.8300 - val_loss: 0.1437 - val_acc: 0.8235\n",
      "Epoch 45/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.1336 - acc: 0.8365 - val_loss: 0.1408 - val_acc: 0.8269\n",
      "Epoch 46/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.1335 - acc: 0.8323 - val_loss: 0.1389 - val_acc: 0.8201\n",
      "Epoch 47/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.1310 - acc: 0.8385 - val_loss: 0.1332 - val_acc: 0.8380\n",
      "Epoch 48/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.1310 - acc: 0.8351 - val_loss: 0.1428 - val_acc: 0.8176\n",
      "Epoch 49/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.1303 - acc: 0.8381 - val_loss: 0.1362 - val_acc: 0.8389\n",
      "Epoch 50/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.1292 - acc: 0.8380 - val_loss: 0.1296 - val_acc: 0.8457\n",
      "Epoch 51/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.1285 - acc: 0.8417 - val_loss: 0.1274 - val_acc: 0.8448\n",
      "Epoch 52/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.1258 - acc: 0.8453 - val_loss: 0.1300 - val_acc: 0.8363\n",
      "Epoch 53/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.1247 - acc: 0.8464 - val_loss: 0.1302 - val_acc: 0.8474\n",
      "Epoch 54/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.1233 - acc: 0.8477 - val_loss: 0.1252 - val_acc: 0.8534\n",
      "Epoch 55/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.1230 - acc: 0.8489 - val_loss: 0.1433 - val_acc: 0.7894\n",
      "Epoch 56/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1223 - acc: 0.8480 - val_loss: 0.1190 - val_acc: 0.8517\n",
      "Epoch 57/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.1200 - acc: 0.8548 - val_loss: 0.1204 - val_acc: 0.8491\n",
      "Epoch 58/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.1197 - acc: 0.8531 - val_loss: 0.1202 - val_acc: 0.8559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.1171 - acc: 0.8564 - val_loss: 0.1169 - val_acc: 0.8627\n",
      "Epoch 60/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.1162 - acc: 0.8565 - val_loss: 0.1196 - val_acc: 0.8576\n",
      "Epoch 61/500\n",
      "22272/22272 [==============================] - 15s 677us/step - loss: 0.1162 - acc: 0.8576 - val_loss: 0.1116 - val_acc: 0.8687\n",
      "Epoch 62/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.1152 - acc: 0.8619 - val_loss: 0.1140 - val_acc: 0.8670\n",
      "Epoch 63/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.1136 - acc: 0.8632 - val_loss: 0.1135 - val_acc: 0.8627\n",
      "Epoch 64/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.1127 - acc: 0.8628 - val_loss: 0.1192 - val_acc: 0.8593\n",
      "Epoch 65/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.1111 - acc: 0.8668 - val_loss: 0.1140 - val_acc: 0.8602\n",
      "Epoch 66/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.1106 - acc: 0.8661 - val_loss: 0.1061 - val_acc: 0.8730\n",
      "Epoch 67/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.1097 - acc: 0.8692 - val_loss: 0.1111 - val_acc: 0.8517\n",
      "Epoch 68/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.1099 - acc: 0.8666 - val_loss: 0.1255 - val_acc: 0.8534\n",
      "Epoch 69/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.1084 - acc: 0.8731 - val_loss: 0.1052 - val_acc: 0.8645\n",
      "Epoch 70/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.1070 - acc: 0.8721 - val_loss: 0.1043 - val_acc: 0.8798\n",
      "Epoch 71/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.1036 - acc: 0.8754 - val_loss: 0.1023 - val_acc: 0.8764\n",
      "Epoch 72/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.1022 - acc: 0.8799 - val_loss: 0.1020 - val_acc: 0.8738\n",
      "Epoch 73/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.1036 - acc: 0.8772 - val_loss: 0.1000 - val_acc: 0.8755\n",
      "Epoch 74/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.1021 - acc: 0.8810 - val_loss: 0.1047 - val_acc: 0.8747\n",
      "Epoch 75/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.1006 - acc: 0.8802 - val_loss: 0.1004 - val_acc: 0.8789\n",
      "Epoch 76/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.1009 - acc: 0.8783 - val_loss: 0.1037 - val_acc: 0.8806\n",
      "Epoch 77/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.1001 - acc: 0.8822 - val_loss: 0.0993 - val_acc: 0.8841\n",
      "Epoch 78/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0987 - acc: 0.8838 - val_loss: 0.0967 - val_acc: 0.8900\n",
      "Epoch 79/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0966 - acc: 0.8873 - val_loss: 0.0933 - val_acc: 0.8883\n",
      "Epoch 80/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0977 - acc: 0.8844 - val_loss: 0.0973 - val_acc: 0.8900\n",
      "Epoch 81/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0955 - acc: 0.8922 - val_loss: 0.0924 - val_acc: 0.8960\n",
      "Epoch 82/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0952 - acc: 0.8912 - val_loss: 0.0950 - val_acc: 0.8738\n",
      "Epoch 83/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0950 - acc: 0.8917 - val_loss: 0.0945 - val_acc: 0.8943\n",
      "Epoch 84/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0933 - acc: 0.8950 - val_loss: 0.0940 - val_acc: 0.8875\n",
      "Epoch 85/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0933 - acc: 0.8938 - val_loss: 0.0920 - val_acc: 0.8986\n",
      "Epoch 86/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0915 - acc: 0.8961 - val_loss: 0.0936 - val_acc: 0.8934\n",
      "Epoch 87/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0904 - acc: 0.8983 - val_loss: 0.0881 - val_acc: 0.8977\n",
      "Epoch 88/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0904 - acc: 0.8961 - val_loss: 0.0870 - val_acc: 0.8994\n",
      "Epoch 89/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0905 - acc: 0.8979 - val_loss: 0.0846 - val_acc: 0.9062\n",
      "Epoch 90/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0879 - acc: 0.9005 - val_loss: 0.0864 - val_acc: 0.8977\n",
      "Epoch 91/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0880 - acc: 0.9001 - val_loss: 0.0887 - val_acc: 0.9003\n",
      "Epoch 92/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0868 - acc: 0.9032 - val_loss: 0.0922 - val_acc: 0.8883\n",
      "Epoch 93/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0868 - acc: 0.9035 - val_loss: 0.0849 - val_acc: 0.9028\n",
      "Epoch 94/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0854 - acc: 0.9054 - val_loss: 0.0815 - val_acc: 0.9122\n",
      "Epoch 95/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0844 - acc: 0.9080 - val_loss: 0.0807 - val_acc: 0.9199\n",
      "Epoch 96/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0831 - acc: 0.9088 - val_loss: 0.0789 - val_acc: 0.9199\n",
      "Epoch 97/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0834 - acc: 0.9089 - val_loss: 0.0808 - val_acc: 0.9062\n",
      "Epoch 98/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0829 - acc: 0.9099 - val_loss: 0.0806 - val_acc: 0.9062\n",
      "Epoch 99/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0822 - acc: 0.9106 - val_loss: 0.0789 - val_acc: 0.9045\n",
      "Epoch 100/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0807 - acc: 0.9130 - val_loss: 0.0746 - val_acc: 0.9224\n",
      "Epoch 101/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0805 - acc: 0.9130 - val_loss: 0.0758 - val_acc: 0.9130\n",
      "Epoch 102/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0810 - acc: 0.9115 - val_loss: 0.0755 - val_acc: 0.9156\n",
      "Epoch 103/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0797 - acc: 0.9151 - val_loss: 0.0767 - val_acc: 0.9139\n",
      "Epoch 104/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0788 - acc: 0.9165 - val_loss: 0.0738 - val_acc: 0.9207\n",
      "Epoch 105/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0781 - acc: 0.9169 - val_loss: 0.0750 - val_acc: 0.9182\n",
      "Epoch 106/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0770 - acc: 0.9192 - val_loss: 0.0798 - val_acc: 0.9130\n",
      "Epoch 107/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0777 - acc: 0.9183 - val_loss: 0.0710 - val_acc: 0.9284\n",
      "Epoch 108/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0759 - acc: 0.9200 - val_loss: 0.0779 - val_acc: 0.9173\n",
      "Epoch 109/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0758 - acc: 0.9184 - val_loss: 0.0759 - val_acc: 0.9139\n",
      "Epoch 110/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0753 - acc: 0.9211 - val_loss: 0.0737 - val_acc: 0.9147\n",
      "Epoch 111/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0749 - acc: 0.9213 - val_loss: 0.0709 - val_acc: 0.9224\n",
      "Epoch 112/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0736 - acc: 0.9231 - val_loss: 0.0719 - val_acc: 0.9284\n",
      "Epoch 113/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0737 - acc: 0.9251 - val_loss: 0.0729 - val_acc: 0.9267\n",
      "Epoch 114/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0724 - acc: 0.9263 - val_loss: 0.0705 - val_acc: 0.9267\n",
      "Epoch 115/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0720 - acc: 0.9255 - val_loss: 0.0643 - val_acc: 0.9378\n",
      "Epoch 116/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0710 - acc: 0.9272 - val_loss: 0.0682 - val_acc: 0.9309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0709 - acc: 0.9266 - val_loss: 0.0663 - val_acc: 0.9327\n",
      "Epoch 118/500\n",
      "22272/22272 [==============================] - 15s 680us/step - loss: 0.0708 - acc: 0.9275 - val_loss: 0.0676 - val_acc: 0.9335\n",
      "Epoch 119/500\n",
      "22272/22272 [==============================] - 15s 677us/step - loss: 0.0694 - acc: 0.9293 - val_loss: 0.0655 - val_acc: 0.9292\n",
      "Epoch 120/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0701 - acc: 0.9269 - val_loss: 0.0661 - val_acc: 0.9292\n",
      "Epoch 121/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0691 - acc: 0.9284 - val_loss: 0.0663 - val_acc: 0.9327\n",
      "Epoch 122/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0691 - acc: 0.9273 - val_loss: 0.0680 - val_acc: 0.9292\n",
      "Epoch 123/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0687 - acc: 0.9288 - val_loss: 0.0670 - val_acc: 0.9267\n",
      "Epoch 124/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0665 - acc: 0.9341 - val_loss: 0.0606 - val_acc: 0.9420\n",
      "Epoch 125/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0669 - acc: 0.9329 - val_loss: 0.0628 - val_acc: 0.9395\n",
      "Epoch 126/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0673 - acc: 0.9327 - val_loss: 0.0626 - val_acc: 0.9327\n",
      "Epoch 127/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0657 - acc: 0.9357 - val_loss: 0.0636 - val_acc: 0.9352\n",
      "Epoch 128/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0652 - acc: 0.9356 - val_loss: 0.0602 - val_acc: 0.9352\n",
      "Epoch 129/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0656 - acc: 0.9338 - val_loss: 0.0596 - val_acc: 0.9429\n",
      "Epoch 130/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0653 - acc: 0.9359 - val_loss: 0.0576 - val_acc: 0.9420\n",
      "Epoch 131/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0625 - acc: 0.9392 - val_loss: 0.0567 - val_acc: 0.9437\n",
      "Epoch 132/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0635 - acc: 0.9390 - val_loss: 0.0569 - val_acc: 0.9454\n",
      "Epoch 133/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0626 - acc: 0.9400 - val_loss: 0.0566 - val_acc: 0.9420\n",
      "Epoch 134/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0615 - acc: 0.9405 - val_loss: 0.0572 - val_acc: 0.9420\n",
      "Epoch 135/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0617 - acc: 0.9429 - val_loss: 0.0572 - val_acc: 0.9420\n",
      "Epoch 136/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0614 - acc: 0.9413 - val_loss: 0.0582 - val_acc: 0.9395\n",
      "Epoch 137/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0620 - acc: 0.9392 - val_loss: 0.0571 - val_acc: 0.9378\n",
      "Epoch 138/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0598 - acc: 0.9434 - val_loss: 0.0549 - val_acc: 0.9497\n",
      "Epoch 139/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0606 - acc: 0.9417 - val_loss: 0.0545 - val_acc: 0.9471\n",
      "Epoch 140/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0597 - acc: 0.9430 - val_loss: 0.0573 - val_acc: 0.9420\n",
      "Epoch 141/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0594 - acc: 0.9439 - val_loss: 0.0528 - val_acc: 0.9488\n",
      "Epoch 142/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0584 - acc: 0.9467 - val_loss: 0.0550 - val_acc: 0.9437\n",
      "Epoch 143/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0585 - acc: 0.9443 - val_loss: 0.0584 - val_acc: 0.9361\n",
      "Epoch 144/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0575 - acc: 0.9471 - val_loss: 0.0532 - val_acc: 0.9497\n",
      "Epoch 145/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0585 - acc: 0.9454 - val_loss: 0.0521 - val_acc: 0.9506\n",
      "Epoch 146/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0562 - acc: 0.9488 - val_loss: 0.0506 - val_acc: 0.9514\n",
      "Epoch 147/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0576 - acc: 0.9456 - val_loss: 0.0537 - val_acc: 0.9480\n",
      "Epoch 148/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0566 - acc: 0.9468 - val_loss: 0.0507 - val_acc: 0.9471\n",
      "Epoch 149/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0558 - acc: 0.9490 - val_loss: 0.0503 - val_acc: 0.9471\n",
      "Epoch 150/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0565 - acc: 0.9490 - val_loss: 0.0514 - val_acc: 0.9429\n",
      "Epoch 151/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0557 - acc: 0.9493 - val_loss: 0.0518 - val_acc: 0.9454\n",
      "Epoch 152/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0555 - acc: 0.9494 - val_loss: 0.0503 - val_acc: 0.9471\n",
      "Epoch 153/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0552 - acc: 0.9515 - val_loss: 0.0500 - val_acc: 0.9531\n",
      "Epoch 154/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0554 - acc: 0.9495 - val_loss: 0.0518 - val_acc: 0.9420\n",
      "Epoch 155/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0544 - acc: 0.9516 - val_loss: 0.0505 - val_acc: 0.9557\n",
      "Epoch 156/500\n",
      "22272/22272 [==============================] - 16s 720us/step - loss: 0.0540 - acc: 0.9525 - val_loss: 0.0520 - val_acc: 0.9437\n",
      "Epoch 157/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0540 - acc: 0.9524 - val_loss: 0.0480 - val_acc: 0.9514\n",
      "Epoch 158/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0533 - acc: 0.9525 - val_loss: 0.0474 - val_acc: 0.9488\n",
      "Epoch 159/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0542 - acc: 0.9511 - val_loss: 0.0460 - val_acc: 0.9591\n",
      "Epoch 160/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0527 - acc: 0.9540 - val_loss: 0.0471 - val_acc: 0.9506\n",
      "Epoch 161/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0531 - acc: 0.9537 - val_loss: 0.0461 - val_acc: 0.9540\n",
      "Epoch 162/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0519 - acc: 0.9533 - val_loss: 0.0473 - val_acc: 0.9548\n",
      "Epoch 163/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0522 - acc: 0.9533 - val_loss: 0.0461 - val_acc: 0.9514\n",
      "Epoch 164/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0526 - acc: 0.9532 - val_loss: 0.0479 - val_acc: 0.9531\n",
      "Epoch 165/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0514 - acc: 0.9545 - val_loss: 0.0438 - val_acc: 0.9591\n",
      "Epoch 166/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0510 - acc: 0.9564 - val_loss: 0.0463 - val_acc: 0.9514\n",
      "Epoch 167/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0498 - acc: 0.9583 - val_loss: 0.0439 - val_acc: 0.9565\n",
      "Epoch 168/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0499 - acc: 0.9573 - val_loss: 0.0447 - val_acc: 0.9506\n",
      "Epoch 169/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0507 - acc: 0.9569 - val_loss: 0.0495 - val_acc: 0.9506\n",
      "Epoch 170/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0501 - acc: 0.9555 - val_loss: 0.0436 - val_acc: 0.9565\n",
      "Epoch 171/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0495 - acc: 0.9582 - val_loss: 0.0465 - val_acc: 0.9557\n",
      "Epoch 172/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0489 - acc: 0.9595 - val_loss: 0.0453 - val_acc: 0.9488\n",
      "Epoch 173/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0481 - acc: 0.9609 - val_loss: 0.0411 - val_acc: 0.9591\n",
      "Epoch 174/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0487 - acc: 0.9591 - val_loss: 0.0410 - val_acc: 0.9633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0486 - acc: 0.9595 - val_loss: 0.0480 - val_acc: 0.9488\n",
      "Epoch 176/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0483 - acc: 0.9591 - val_loss: 0.0409 - val_acc: 0.9599\n",
      "Epoch 177/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0481 - acc: 0.9594 - val_loss: 0.0441 - val_acc: 0.9540\n",
      "Epoch 178/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0477 - acc: 0.9589 - val_loss: 0.0405 - val_acc: 0.9599\n",
      "Epoch 179/500\n",
      "22272/22272 [==============================] - 15s 680us/step - loss: 0.0479 - acc: 0.9595 - val_loss: 0.0458 - val_acc: 0.9523\n",
      "Epoch 180/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0473 - acc: 0.9622 - val_loss: 0.0420 - val_acc: 0.9582\n",
      "Epoch 181/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0471 - acc: 0.9612 - val_loss: 0.0421 - val_acc: 0.9625\n",
      "Epoch 182/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0471 - acc: 0.9613 - val_loss: 0.0415 - val_acc: 0.9633\n",
      "Epoch 183/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0464 - acc: 0.9612 - val_loss: 0.0433 - val_acc: 0.9540\n",
      "Epoch 184/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0462 - acc: 0.9612 - val_loss: 0.0393 - val_acc: 0.9642\n",
      "Epoch 185/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0465 - acc: 0.9626 - val_loss: 0.0403 - val_acc: 0.9616\n",
      "Epoch 186/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0459 - acc: 0.9619 - val_loss: 0.0412 - val_acc: 0.9599\n",
      "Epoch 187/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0456 - acc: 0.9633 - val_loss: 0.0402 - val_acc: 0.9650\n",
      "Epoch 188/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0451 - acc: 0.9638 - val_loss: 0.0378 - val_acc: 0.9642\n",
      "Epoch 189/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0442 - acc: 0.9654 - val_loss: 0.0403 - val_acc: 0.9642\n",
      "Epoch 190/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0453 - acc: 0.9636 - val_loss: 0.0403 - val_acc: 0.9599\n",
      "Epoch 191/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0445 - acc: 0.9664 - val_loss: 0.0378 - val_acc: 0.9633\n",
      "Epoch 192/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.0440 - acc: 0.9644 - val_loss: 0.0376 - val_acc: 0.9625\n",
      "Epoch 193/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0444 - acc: 0.9635 - val_loss: 0.0376 - val_acc: 0.9642\n",
      "Epoch 194/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0443 - acc: 0.9652 - val_loss: 0.0366 - val_acc: 0.9642\n",
      "Epoch 195/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0430 - acc: 0.9688 - val_loss: 0.0373 - val_acc: 0.9642\n",
      "Epoch 196/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0441 - acc: 0.9635 - val_loss: 0.0386 - val_acc: 0.9591\n",
      "Epoch 197/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0430 - acc: 0.9667 - val_loss: 0.0370 - val_acc: 0.9650\n",
      "Epoch 198/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0436 - acc: 0.9647 - val_loss: 0.0409 - val_acc: 0.9599\n",
      "Epoch 199/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0427 - acc: 0.9651 - val_loss: 0.0397 - val_acc: 0.9608\n",
      "Epoch 200/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0426 - acc: 0.9664 - val_loss: 0.0356 - val_acc: 0.9685\n",
      "Epoch 201/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0416 - acc: 0.9682 - val_loss: 0.0387 - val_acc: 0.9625\n",
      "Epoch 202/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0419 - acc: 0.9669 - val_loss: 0.0361 - val_acc: 0.9744\n",
      "Epoch 203/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0424 - acc: 0.9675 - val_loss: 0.0360 - val_acc: 0.9693\n",
      "Epoch 204/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0420 - acc: 0.9668 - val_loss: 0.0346 - val_acc: 0.9702\n",
      "Epoch 205/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0421 - acc: 0.9660 - val_loss: 0.0351 - val_acc: 0.9685\n",
      "Epoch 206/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0413 - acc: 0.9664 - val_loss: 0.0362 - val_acc: 0.9668\n",
      "Epoch 207/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0418 - acc: 0.9676 - val_loss: 0.0337 - val_acc: 0.9702\n",
      "Epoch 208/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0409 - acc: 0.9685 - val_loss: 0.0343 - val_acc: 0.9710\n",
      "Epoch 209/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0405 - acc: 0.9686 - val_loss: 0.0332 - val_acc: 0.9710\n",
      "Epoch 210/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0410 - acc: 0.9684 - val_loss: 0.0368 - val_acc: 0.9676\n",
      "Epoch 211/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0406 - acc: 0.9696 - val_loss: 0.0350 - val_acc: 0.9668\n",
      "Epoch 212/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0398 - acc: 0.9709 - val_loss: 0.0376 - val_acc: 0.9693\n",
      "Epoch 213/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0409 - acc: 0.9681 - val_loss: 0.0327 - val_acc: 0.9727\n",
      "Epoch 214/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0396 - acc: 0.9708 - val_loss: 0.0355 - val_acc: 0.9659\n",
      "Epoch 215/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0399 - acc: 0.9702 - val_loss: 0.0342 - val_acc: 0.9710\n",
      "Epoch 216/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0393 - acc: 0.9695 - val_loss: 0.0337 - val_acc: 0.9685\n",
      "Epoch 217/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0395 - acc: 0.9728 - val_loss: 0.0322 - val_acc: 0.9719\n",
      "Epoch 218/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0394 - acc: 0.9689 - val_loss: 0.0342 - val_acc: 0.9693\n",
      "Epoch 219/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0392 - acc: 0.9703 - val_loss: 0.0358 - val_acc: 0.9676\n",
      "Epoch 220/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0389 - acc: 0.9713 - val_loss: 0.0351 - val_acc: 0.9650\n",
      "Epoch 221/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0393 - acc: 0.9709 - val_loss: 0.0312 - val_acc: 0.9693\n",
      "Epoch 222/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0386 - acc: 0.9711 - val_loss: 0.0315 - val_acc: 0.9753\n",
      "Epoch 223/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0382 - acc: 0.9717 - val_loss: 0.0307 - val_acc: 0.9744\n",
      "Epoch 224/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0388 - acc: 0.9713 - val_loss: 0.0338 - val_acc: 0.9710\n",
      "Epoch 225/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0385 - acc: 0.9723 - val_loss: 0.0338 - val_acc: 0.9710\n",
      "Epoch 226/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0386 - acc: 0.9720 - val_loss: 0.0326 - val_acc: 0.9685\n",
      "Epoch 227/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0383 - acc: 0.9701 - val_loss: 0.0306 - val_acc: 0.9727\n",
      "Epoch 228/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0377 - acc: 0.9732 - val_loss: 0.0342 - val_acc: 0.9685\n",
      "Epoch 229/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0384 - acc: 0.9725 - val_loss: 0.0345 - val_acc: 0.9668\n",
      "Epoch 230/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0374 - acc: 0.9717 - val_loss: 0.0320 - val_acc: 0.9719\n",
      "Epoch 231/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0376 - acc: 0.9734 - val_loss: 0.0328 - val_acc: 0.9702\n",
      "Epoch 232/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0370 - acc: 0.9735 - val_loss: 0.0304 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/500\n",
      "22272/22272 [==============================] - 15s 677us/step - loss: 0.0371 - acc: 0.9726 - val_loss: 0.0332 - val_acc: 0.9668\n",
      "Epoch 234/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0374 - acc: 0.9733 - val_loss: 0.0299 - val_acc: 0.9736\n",
      "Epoch 235/500\n",
      "22272/22272 [==============================] - 15s 676us/step - loss: 0.0366 - acc: 0.9732 - val_loss: 0.0308 - val_acc: 0.9744\n",
      "Epoch 236/500\n",
      "22272/22272 [==============================] - 15s 676us/step - loss: 0.0369 - acc: 0.9731 - val_loss: 0.0307 - val_acc: 0.9770\n",
      "Epoch 237/500\n",
      "22272/22272 [==============================] - 15s 678us/step - loss: 0.0367 - acc: 0.9746 - val_loss: 0.0308 - val_acc: 0.9736\n",
      "Epoch 238/500\n",
      "22272/22272 [==============================] - 15s 676us/step - loss: 0.0365 - acc: 0.9743 - val_loss: 0.0313 - val_acc: 0.9710\n",
      "Epoch 239/500\n",
      "22272/22272 [==============================] - 15s 678us/step - loss: 0.0370 - acc: 0.9736 - val_loss: 0.0290 - val_acc: 0.9753\n",
      "Epoch 240/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0363 - acc: 0.9734 - val_loss: 0.0285 - val_acc: 0.9744\n",
      "Epoch 241/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0360 - acc: 0.9739 - val_loss: 0.0314 - val_acc: 0.9685\n",
      "Epoch 242/500\n",
      "22272/22272 [==============================] - 15s 680us/step - loss: 0.0359 - acc: 0.9745 - val_loss: 0.0301 - val_acc: 0.9719\n",
      "Epoch 243/500\n",
      "22272/22272 [==============================] - 15s 678us/step - loss: 0.0356 - acc: 0.9743 - val_loss: 0.0311 - val_acc: 0.9727\n",
      "Epoch 244/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0359 - acc: 0.9733 - val_loss: 0.0306 - val_acc: 0.9727\n",
      "Epoch 245/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0355 - acc: 0.9742 - val_loss: 0.0291 - val_acc: 0.9795\n",
      "Epoch 246/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0352 - acc: 0.9763 - val_loss: 0.0286 - val_acc: 0.9761\n",
      "Epoch 247/500\n",
      "22272/22272 [==============================] - 15s 678us/step - loss: 0.0353 - acc: 0.9738 - val_loss: 0.0299 - val_acc: 0.9710\n",
      "Epoch 248/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0354 - acc: 0.9748 - val_loss: 0.0276 - val_acc: 0.9753\n",
      "Epoch 249/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0342 - acc: 0.9751 - val_loss: 0.0282 - val_acc: 0.9804\n",
      "Epoch 250/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0345 - acc: 0.9759 - val_loss: 0.0288 - val_acc: 0.9770\n",
      "Epoch 251/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0350 - acc: 0.9749 - val_loss: 0.0264 - val_acc: 0.9753\n",
      "Epoch 252/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0345 - acc: 0.9744 - val_loss: 0.0289 - val_acc: 0.9753\n",
      "Epoch 253/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0343 - acc: 0.9758 - val_loss: 0.0272 - val_acc: 0.9770\n",
      "Epoch 254/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0345 - acc: 0.9744 - val_loss: 0.0296 - val_acc: 0.9753\n",
      "Epoch 255/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0346 - acc: 0.9759 - val_loss: 0.0280 - val_acc: 0.9778\n",
      "Epoch 256/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0345 - acc: 0.9758 - val_loss: 0.0265 - val_acc: 0.9778\n",
      "Epoch 257/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0342 - acc: 0.9762 - val_loss: 0.0287 - val_acc: 0.9770\n",
      "Epoch 258/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0345 - acc: 0.9758 - val_loss: 0.0268 - val_acc: 0.9778\n",
      "Epoch 259/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0337 - acc: 0.9771 - val_loss: 0.0289 - val_acc: 0.9736\n",
      "Epoch 260/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0338 - acc: 0.9759 - val_loss: 0.0282 - val_acc: 0.9744\n",
      "Epoch 261/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0348 - acc: 0.9733 - val_loss: 0.0274 - val_acc: 0.9787\n",
      "Epoch 262/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0335 - acc: 0.9765 - val_loss: 0.0263 - val_acc: 0.9761\n",
      "Epoch 263/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0348 - acc: 0.9736 - val_loss: 0.0266 - val_acc: 0.9761\n",
      "Epoch 264/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0338 - acc: 0.9764 - val_loss: 0.0264 - val_acc: 0.9744\n",
      "Epoch 265/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0338 - acc: 0.9748 - val_loss: 0.0293 - val_acc: 0.9727\n",
      "Epoch 266/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0335 - acc: 0.9764 - val_loss: 0.0294 - val_acc: 0.9753\n",
      "Epoch 267/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0334 - acc: 0.9772 - val_loss: 0.0263 - val_acc: 0.9736\n",
      "Epoch 268/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0332 - acc: 0.9762 - val_loss: 0.0271 - val_acc: 0.9744\n",
      "Epoch 269/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0332 - acc: 0.9765 - val_loss: 0.0273 - val_acc: 0.9761\n",
      "Epoch 270/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0332 - acc: 0.9760 - val_loss: 0.0270 - val_acc: 0.9770\n",
      "Epoch 271/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0326 - acc: 0.9773 - val_loss: 0.0279 - val_acc: 0.9727\n",
      "Epoch 272/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0328 - acc: 0.9768 - val_loss: 0.0274 - val_acc: 0.9770\n",
      "Epoch 273/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0327 - acc: 0.9769 - val_loss: 0.0281 - val_acc: 0.9778\n",
      "Epoch 274/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0328 - acc: 0.9774 - val_loss: 0.0254 - val_acc: 0.9770\n",
      "Epoch 275/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0323 - acc: 0.9770 - val_loss: 0.0264 - val_acc: 0.9804\n",
      "Epoch 276/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0328 - acc: 0.9780 - val_loss: 0.0267 - val_acc: 0.9778\n",
      "Epoch 277/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0338 - acc: 0.9751 - val_loss: 0.0284 - val_acc: 0.9727\n",
      "Epoch 278/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0317 - acc: 0.9788 - val_loss: 0.0268 - val_acc: 0.9778\n",
      "Epoch 279/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0322 - acc: 0.9783 - val_loss: 0.0274 - val_acc: 0.9744\n",
      "Epoch 280/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0316 - acc: 0.9789 - val_loss: 0.0256 - val_acc: 0.9770\n",
      "Epoch 281/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0326 - acc: 0.9767 - val_loss: 0.0301 - val_acc: 0.9761\n",
      "Epoch 282/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0317 - acc: 0.9785 - val_loss: 0.0255 - val_acc: 0.9770\n",
      "Epoch 283/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0321 - acc: 0.9776 - val_loss: 0.0262 - val_acc: 0.9761\n",
      "Epoch 284/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0320 - acc: 0.9767 - val_loss: 0.0249 - val_acc: 0.9770\n",
      "Epoch 285/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0317 - acc: 0.9792 - val_loss: 0.0294 - val_acc: 0.9676\n",
      "Epoch 286/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0315 - acc: 0.9786 - val_loss: 0.0247 - val_acc: 0.9787\n",
      "Epoch 287/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0310 - acc: 0.9792 - val_loss: 0.0244 - val_acc: 0.9812\n",
      "Epoch 288/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0316 - acc: 0.9777 - val_loss: 0.0230 - val_acc: 0.9787\n",
      "Epoch 289/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0314 - acc: 0.9778 - val_loss: 0.0248 - val_acc: 0.9804\n",
      "Epoch 290/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0310 - acc: 0.9809 - val_loss: 0.0240 - val_acc: 0.9787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/500\n",
      "22272/22272 [==============================] - 15s 674us/step - loss: 0.0315 - acc: 0.9783 - val_loss: 0.0276 - val_acc: 0.9761\n",
      "Epoch 292/500\n",
      "22272/22272 [==============================] - 15s 675us/step - loss: 0.0317 - acc: 0.9784 - val_loss: 0.0261 - val_acc: 0.9761\n",
      "Epoch 293/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0312 - acc: 0.9791 - val_loss: 0.0261 - val_acc: 0.9736\n",
      "Epoch 294/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0306 - acc: 0.9780 - val_loss: 0.0253 - val_acc: 0.9812\n",
      "Epoch 295/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0303 - acc: 0.9787 - val_loss: 0.0242 - val_acc: 0.9795\n",
      "Epoch 296/500\n",
      "22272/22272 [==============================] - 15s 680us/step - loss: 0.0309 - acc: 0.9791 - val_loss: 0.0251 - val_acc: 0.9761\n",
      "Epoch 297/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0310 - acc: 0.9781 - val_loss: 0.0252 - val_acc: 0.9736\n",
      "Epoch 298/500\n",
      "22272/22272 [==============================] - 15s 678us/step - loss: 0.0308 - acc: 0.9786 - val_loss: 0.0243 - val_acc: 0.9787\n",
      "Epoch 299/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0304 - acc: 0.9788 - val_loss: 0.0243 - val_acc: 0.9821\n",
      "Epoch 300/500\n",
      "22272/22272 [==============================] - 15s 677us/step - loss: 0.0301 - acc: 0.9800 - val_loss: 0.0256 - val_acc: 0.9761\n",
      "Epoch 301/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0301 - acc: 0.9806 - val_loss: 0.0244 - val_acc: 0.9778\n",
      "Epoch 302/500\n",
      "22272/22272 [==============================] - 15s 676us/step - loss: 0.0301 - acc: 0.9799 - val_loss: 0.0262 - val_acc: 0.9795\n",
      "Epoch 303/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0302 - acc: 0.9794 - val_loss: 0.0254 - val_acc: 0.9753\n",
      "Epoch 304/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0299 - acc: 0.9794 - val_loss: 0.0242 - val_acc: 0.9804\n",
      "Epoch 305/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0299 - acc: 0.9793 - val_loss: 0.0248 - val_acc: 0.9829\n",
      "Epoch 306/500\n",
      "22272/22272 [==============================] - 15s 680us/step - loss: 0.0299 - acc: 0.9796 - val_loss: 0.0235 - val_acc: 0.9821\n",
      "Epoch 307/500\n",
      "22272/22272 [==============================] - 15s 680us/step - loss: 0.0298 - acc: 0.9805 - val_loss: 0.0254 - val_acc: 0.9787\n",
      "Epoch 308/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0298 - acc: 0.9785 - val_loss: 0.0234 - val_acc: 0.9795\n",
      "Epoch 309/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0304 - acc: 0.9793 - val_loss: 0.0240 - val_acc: 0.9795\n",
      "Epoch 310/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0294 - acc: 0.9801 - val_loss: 0.0234 - val_acc: 0.9804\n",
      "Epoch 311/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0296 - acc: 0.9807 - val_loss: 0.0237 - val_acc: 0.9804\n",
      "Epoch 312/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0297 - acc: 0.9804 - val_loss: 0.0263 - val_acc: 0.9787\n",
      "Epoch 313/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0296 - acc: 0.9790 - val_loss: 0.0218 - val_acc: 0.9812\n",
      "Epoch 314/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0292 - acc: 0.9808 - val_loss: 0.0238 - val_acc: 0.9804\n",
      "Epoch 315/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0294 - acc: 0.9791 - val_loss: 0.0228 - val_acc: 0.9795\n",
      "Epoch 316/500\n",
      "22272/22272 [==============================] - 15s 680us/step - loss: 0.0297 - acc: 0.9799 - val_loss: 0.0220 - val_acc: 0.9804\n",
      "Epoch 317/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0293 - acc: 0.9809 - val_loss: 0.0225 - val_acc: 0.9804\n",
      "Epoch 318/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0288 - acc: 0.9807 - val_loss: 0.0249 - val_acc: 0.9727\n",
      "Epoch 319/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0289 - acc: 0.9810 - val_loss: 0.0220 - val_acc: 0.9787\n",
      "Epoch 320/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0285 - acc: 0.9813 - val_loss: 0.0229 - val_acc: 0.9795\n",
      "Epoch 321/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0287 - acc: 0.9805 - val_loss: 0.0235 - val_acc: 0.9770\n",
      "Epoch 322/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0289 - acc: 0.9807 - val_loss: 0.0241 - val_acc: 0.9778\n",
      "Epoch 323/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0289 - acc: 0.9815 - val_loss: 0.0236 - val_acc: 0.9778\n",
      "Epoch 324/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0289 - acc: 0.9809 - val_loss: 0.0244 - val_acc: 0.9744\n",
      "Epoch 325/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0290 - acc: 0.9824 - val_loss: 0.0237 - val_acc: 0.9778\n",
      "Epoch 326/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0285 - acc: 0.9808 - val_loss: 0.0223 - val_acc: 0.9804\n",
      "Epoch 327/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0282 - acc: 0.9811 - val_loss: 0.0220 - val_acc: 0.9812\n",
      "Epoch 328/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0283 - acc: 0.9807 - val_loss: 0.0240 - val_acc: 0.9795\n",
      "Epoch 329/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0283 - acc: 0.9815 - val_loss: 0.0231 - val_acc: 0.9787\n",
      "Epoch 330/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0283 - acc: 0.9806 - val_loss: 0.0230 - val_acc: 0.9787\n",
      "Epoch 331/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0285 - acc: 0.9806 - val_loss: 0.0213 - val_acc: 0.9795\n",
      "Epoch 332/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0283 - acc: 0.9821 - val_loss: 0.0227 - val_acc: 0.9829\n",
      "Epoch 333/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0282 - acc: 0.9809 - val_loss: 0.0226 - val_acc: 0.9821\n",
      "Epoch 334/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0284 - acc: 0.9798 - val_loss: 0.0216 - val_acc: 0.9821\n",
      "Epoch 335/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0279 - acc: 0.9812 - val_loss: 0.0221 - val_acc: 0.9838\n",
      "Epoch 336/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0281 - acc: 0.9815 - val_loss: 0.0220 - val_acc: 0.9821\n",
      "Epoch 337/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0282 - acc: 0.9820 - val_loss: 0.0208 - val_acc: 0.9821\n",
      "Epoch 338/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0277 - acc: 0.9822 - val_loss: 0.0229 - val_acc: 0.9804\n",
      "Epoch 339/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0280 - acc: 0.9816 - val_loss: 0.0238 - val_acc: 0.9770\n",
      "Epoch 340/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0279 - acc: 0.9809 - val_loss: 0.0204 - val_acc: 0.9812\n",
      "Epoch 341/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0275 - acc: 0.9818 - val_loss: 0.0224 - val_acc: 0.9829\n",
      "Epoch 342/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0273 - acc: 0.9822 - val_loss: 0.0213 - val_acc: 0.9812\n",
      "Epoch 343/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0280 - acc: 0.9816 - val_loss: 0.0221 - val_acc: 0.9847\n",
      "Epoch 344/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0280 - acc: 0.9825 - val_loss: 0.0214 - val_acc: 0.9812\n",
      "Epoch 345/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0277 - acc: 0.9823 - val_loss: 0.0216 - val_acc: 0.9821\n",
      "Epoch 346/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0272 - acc: 0.9824 - val_loss: 0.0207 - val_acc: 0.9821\n",
      "Epoch 347/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0277 - acc: 0.9809 - val_loss: 0.0202 - val_acc: 0.9838\n",
      "Epoch 348/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0272 - acc: 0.9819 - val_loss: 0.0211 - val_acc: 0.9829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349/500\n",
      "22272/22272 [==============================] - 15s 677us/step - loss: 0.0278 - acc: 0.9809 - val_loss: 0.0220 - val_acc: 0.9787\n",
      "Epoch 350/500\n",
      "22272/22272 [==============================] - 15s 677us/step - loss: 0.0273 - acc: 0.9818 - val_loss: 0.0221 - val_acc: 0.9787\n",
      "Epoch 351/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0264 - acc: 0.9819 - val_loss: 0.0204 - val_acc: 0.9804\n",
      "Epoch 352/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0273 - acc: 0.9813 - val_loss: 0.0240 - val_acc: 0.9812\n",
      "Epoch 353/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0271 - acc: 0.9816 - val_loss: 0.0207 - val_acc: 0.9829\n",
      "Epoch 354/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0268 - acc: 0.9828 - val_loss: 0.0202 - val_acc: 0.9872\n",
      "Epoch 355/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0265 - acc: 0.9817 - val_loss: 0.0208 - val_acc: 0.9812\n",
      "Epoch 356/500\n",
      "22272/22272 [==============================] - 15s 677us/step - loss: 0.0269 - acc: 0.9827 - val_loss: 0.0196 - val_acc: 0.9829\n",
      "Epoch 357/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0270 - acc: 0.9827 - val_loss: 0.0215 - val_acc: 0.9812\n",
      "Epoch 358/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0267 - acc: 0.9830 - val_loss: 0.0209 - val_acc: 0.9812\n",
      "Epoch 359/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0261 - acc: 0.9824 - val_loss: 0.0220 - val_acc: 0.9787\n",
      "Epoch 360/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0271 - acc: 0.9816 - val_loss: 0.0233 - val_acc: 0.9778\n",
      "Epoch 361/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0265 - acc: 0.9816 - val_loss: 0.0199 - val_acc: 0.9838\n",
      "Epoch 362/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0266 - acc: 0.9818 - val_loss: 0.0205 - val_acc: 0.9821\n",
      "Epoch 363/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0266 - acc: 0.9825 - val_loss: 0.0215 - val_acc: 0.9795\n",
      "Epoch 364/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0262 - acc: 0.9826 - val_loss: 0.0195 - val_acc: 0.9812\n",
      "Epoch 365/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0264 - acc: 0.9838 - val_loss: 0.0239 - val_acc: 0.9795\n",
      "Epoch 366/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0266 - acc: 0.9831 - val_loss: 0.0191 - val_acc: 0.9838\n",
      "Epoch 367/500\n",
      "22272/22272 [==============================] - 15s 677us/step - loss: 0.0264 - acc: 0.9818 - val_loss: 0.0210 - val_acc: 0.9804\n",
      "Epoch 368/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0264 - acc: 0.9824 - val_loss: 0.0211 - val_acc: 0.9847\n",
      "Epoch 369/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0263 - acc: 0.9826 - val_loss: 0.0201 - val_acc: 0.9829\n",
      "Epoch 370/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0260 - acc: 0.9828 - val_loss: 0.0193 - val_acc: 0.9838\n",
      "Epoch 371/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0258 - acc: 0.9829 - val_loss: 0.0201 - val_acc: 0.9804\n",
      "Epoch 372/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0261 - acc: 0.9831 - val_loss: 0.0199 - val_acc: 0.9838\n",
      "Epoch 373/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0258 - acc: 0.9833 - val_loss: 0.0202 - val_acc: 0.9829\n",
      "Epoch 374/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0257 - acc: 0.9832 - val_loss: 0.0196 - val_acc: 0.9838\n",
      "Epoch 375/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0255 - acc: 0.9824 - val_loss: 0.0201 - val_acc: 0.9829\n",
      "Epoch 376/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0257 - acc: 0.9833 - val_loss: 0.0201 - val_acc: 0.9821\n",
      "Epoch 377/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0255 - acc: 0.9838 - val_loss: 0.0216 - val_acc: 0.9829\n",
      "Epoch 378/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0259 - acc: 0.9822 - val_loss: 0.0198 - val_acc: 0.9838\n",
      "Epoch 379/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0256 - acc: 0.9843 - val_loss: 0.0196 - val_acc: 0.9829\n",
      "Epoch 380/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0260 - acc: 0.9834 - val_loss: 0.0187 - val_acc: 0.9812\n",
      "Epoch 381/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0256 - acc: 0.9823 - val_loss: 0.0197 - val_acc: 0.9795\n",
      "Epoch 382/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0260 - acc: 0.9830 - val_loss: 0.0217 - val_acc: 0.9804\n",
      "Epoch 383/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0257 - acc: 0.9835 - val_loss: 0.0196 - val_acc: 0.9847\n",
      "Epoch 384/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0257 - acc: 0.9821 - val_loss: 0.0192 - val_acc: 0.9838\n",
      "Epoch 385/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0261 - acc: 0.9836 - val_loss: 0.0197 - val_acc: 0.9838\n",
      "Epoch 386/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0254 - acc: 0.9848 - val_loss: 0.0191 - val_acc: 0.9770\n",
      "Epoch 387/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0251 - acc: 0.9844 - val_loss: 0.0208 - val_acc: 0.9727\n",
      "Epoch 388/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0250 - acc: 0.9842 - val_loss: 0.0200 - val_acc: 0.9812\n",
      "Epoch 389/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0253 - acc: 0.9831 - val_loss: 0.0214 - val_acc: 0.9829\n",
      "Epoch 390/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0252 - acc: 0.9826 - val_loss: 0.0230 - val_acc: 0.9804\n",
      "Epoch 391/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0251 - acc: 0.9832 - val_loss: 0.0201 - val_acc: 0.9812\n",
      "Epoch 392/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0249 - acc: 0.9831 - val_loss: 0.0203 - val_acc: 0.9804\n",
      "Epoch 393/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0247 - acc: 0.9841 - val_loss: 0.0216 - val_acc: 0.9821\n",
      "Epoch 394/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0249 - acc: 0.9843 - val_loss: 0.0179 - val_acc: 0.9838\n",
      "Epoch 395/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0249 - acc: 0.9841 - val_loss: 0.0190 - val_acc: 0.9847\n",
      "Epoch 396/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0252 - acc: 0.9833 - val_loss: 0.0187 - val_acc: 0.9838\n",
      "Epoch 397/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0249 - acc: 0.9842 - val_loss: 0.0204 - val_acc: 0.9829\n",
      "Epoch 398/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0250 - acc: 0.9841 - val_loss: 0.0199 - val_acc: 0.9812\n",
      "Epoch 399/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0246 - acc: 0.9842 - val_loss: 0.0186 - val_acc: 0.9804\n",
      "Epoch 400/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0250 - acc: 0.9830 - val_loss: 0.0202 - val_acc: 0.9838\n",
      "Epoch 401/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0248 - acc: 0.9841 - val_loss: 0.0189 - val_acc: 0.9829\n",
      "Epoch 402/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0249 - acc: 0.9846 - val_loss: 0.0194 - val_acc: 0.9847\n",
      "Epoch 403/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0250 - acc: 0.9840 - val_loss: 0.0181 - val_acc: 0.9804\n",
      "Epoch 404/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0242 - acc: 0.9843 - val_loss: 0.0206 - val_acc: 0.9795\n",
      "Epoch 405/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0246 - acc: 0.9844 - val_loss: 0.0197 - val_acc: 0.9821\n",
      "Epoch 406/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0242 - acc: 0.9852 - val_loss: 0.0186 - val_acc: 0.9838\n",
      "Epoch 407/500\n",
      "22272/22272 [==============================] - 15s 676us/step - loss: 0.0245 - acc: 0.9841 - val_loss: 0.0183 - val_acc: 0.9838\n",
      "Epoch 408/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0243 - acc: 0.9837 - val_loss: 0.0185 - val_acc: 0.9804\n",
      "Epoch 409/500\n",
      "22272/22272 [==============================] - 15s 678us/step - loss: 0.0245 - acc: 0.9842 - val_loss: 0.0185 - val_acc: 0.9787\n",
      "Epoch 410/500\n",
      "22272/22272 [==============================] - 15s 677us/step - loss: 0.0240 - acc: 0.9842 - val_loss: 0.0184 - val_acc: 0.9838\n",
      "Epoch 411/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0242 - acc: 0.9835 - val_loss: 0.0204 - val_acc: 0.9804\n",
      "Epoch 412/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0243 - acc: 0.9840 - val_loss: 0.0196 - val_acc: 0.9847\n",
      "Epoch 413/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0242 - acc: 0.9843 - val_loss: 0.0191 - val_acc: 0.9804\n",
      "Epoch 414/500\n",
      "22272/22272 [==============================] - 15s 678us/step - loss: 0.0246 - acc: 0.9842 - val_loss: 0.0189 - val_acc: 0.9821\n",
      "Epoch 415/500\n",
      "22272/22272 [==============================] - 15s 680us/step - loss: 0.0247 - acc: 0.9839 - val_loss: 0.0182 - val_acc: 0.9838\n",
      "Epoch 416/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0241 - acc: 0.9840 - val_loss: 0.0194 - val_acc: 0.9829\n",
      "Epoch 417/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0243 - acc: 0.9840 - val_loss: 0.0190 - val_acc: 0.9795\n",
      "Epoch 418/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0240 - acc: 0.9841 - val_loss: 0.0172 - val_acc: 0.9829\n",
      "Epoch 419/500\n",
      "22272/22272 [==============================] - 15s 677us/step - loss: 0.0239 - acc: 0.9845 - val_loss: 0.0176 - val_acc: 0.9872\n",
      "Epoch 420/500\n",
      "22272/22272 [==============================] - 15s 678us/step - loss: 0.0245 - acc: 0.9837 - val_loss: 0.0187 - val_acc: 0.9864\n",
      "Epoch 421/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0239 - acc: 0.9837 - val_loss: 0.0178 - val_acc: 0.9838\n",
      "Epoch 422/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0240 - acc: 0.9848 - val_loss: 0.0192 - val_acc: 0.9812\n",
      "Epoch 423/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0242 - acc: 0.9843 - val_loss: 0.0179 - val_acc: 0.9838\n",
      "Epoch 424/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0242 - acc: 0.9849 - val_loss: 0.0173 - val_acc: 0.9821\n",
      "Epoch 425/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0240 - acc: 0.9849 - val_loss: 0.0185 - val_acc: 0.9812\n",
      "Epoch 426/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0239 - acc: 0.9855 - val_loss: 0.0174 - val_acc: 0.9821\n",
      "Epoch 427/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.0237 - acc: 0.9848 - val_loss: 0.0168 - val_acc: 0.9804\n",
      "Epoch 428/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0239 - acc: 0.9840 - val_loss: 0.0177 - val_acc: 0.9872\n",
      "Epoch 429/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0238 - acc: 0.9847 - val_loss: 0.0173 - val_acc: 0.9829\n",
      "Epoch 430/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0235 - acc: 0.9850 - val_loss: 0.0168 - val_acc: 0.9821\n",
      "Epoch 431/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0233 - acc: 0.9852 - val_loss: 0.0180 - val_acc: 0.9804\n",
      "Epoch 432/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0234 - acc: 0.9851 - val_loss: 0.0158 - val_acc: 0.9855\n",
      "Epoch 433/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0236 - acc: 0.9858 - val_loss: 0.0182 - val_acc: 0.9804\n",
      "Epoch 434/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0233 - acc: 0.9859 - val_loss: 0.0191 - val_acc: 0.9829\n",
      "Epoch 435/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0236 - acc: 0.9836 - val_loss: 0.0166 - val_acc: 0.9804\n",
      "Epoch 436/500\n",
      "22272/22272 [==============================] - 15s 680us/step - loss: 0.0237 - acc: 0.9867 - val_loss: 0.0171 - val_acc: 0.9838\n",
      "Epoch 437/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0241 - acc: 0.9842 - val_loss: 0.0177 - val_acc: 0.9838\n",
      "Epoch 438/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0238 - acc: 0.9840 - val_loss: 0.0169 - val_acc: 0.9864\n",
      "Epoch 439/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0234 - acc: 0.9853 - val_loss: 0.0170 - val_acc: 0.9838\n",
      "Epoch 440/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0231 - acc: 0.9859 - val_loss: 0.0162 - val_acc: 0.9872\n",
      "Epoch 441/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0230 - acc: 0.9860 - val_loss: 0.0164 - val_acc: 0.9889\n",
      "Epoch 442/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0229 - acc: 0.9855 - val_loss: 0.0172 - val_acc: 0.9821\n",
      "Epoch 443/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0233 - acc: 0.9842 - val_loss: 0.0175 - val_acc: 0.9889\n",
      "Epoch 444/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0230 - acc: 0.9854 - val_loss: 0.0159 - val_acc: 0.9881\n",
      "Epoch 445/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0233 - acc: 0.9847 - val_loss: 0.0164 - val_acc: 0.9864\n",
      "Epoch 446/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0228 - acc: 0.9834 - val_loss: 0.0176 - val_acc: 0.9855\n",
      "Epoch 447/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0237 - acc: 0.9844 - val_loss: 0.0177 - val_acc: 0.9872\n",
      "Epoch 448/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0231 - acc: 0.9847 - val_loss: 0.0163 - val_acc: 0.9881\n",
      "Epoch 449/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0232 - acc: 0.9843 - val_loss: 0.0178 - val_acc: 0.9855\n",
      "Epoch 450/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0234 - acc: 0.9846 - val_loss: 0.0165 - val_acc: 0.9838\n",
      "Epoch 451/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0233 - acc: 0.9848 - val_loss: 0.0175 - val_acc: 0.9855\n",
      "Epoch 452/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0227 - acc: 0.9856 - val_loss: 0.0172 - val_acc: 0.9881\n",
      "Epoch 453/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0232 - acc: 0.9849 - val_loss: 0.0158 - val_acc: 0.9855\n",
      "Epoch 454/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0228 - acc: 0.9856 - val_loss: 0.0165 - val_acc: 0.9838\n",
      "Epoch 455/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0230 - acc: 0.9853 - val_loss: 0.0164 - val_acc: 0.9864\n",
      "Epoch 456/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0230 - acc: 0.9860 - val_loss: 0.0178 - val_acc: 0.9829\n",
      "Epoch 457/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0229 - acc: 0.9848 - val_loss: 0.0170 - val_acc: 0.9855\n",
      "Epoch 458/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0225 - acc: 0.9857 - val_loss: 0.0157 - val_acc: 0.9855\n",
      "Epoch 459/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0227 - acc: 0.9849 - val_loss: 0.0188 - val_acc: 0.9829\n",
      "Epoch 460/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0225 - acc: 0.9848 - val_loss: 0.0158 - val_acc: 0.9829\n",
      "Epoch 461/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0226 - acc: 0.9850 - val_loss: 0.0169 - val_acc: 0.9855\n",
      "Epoch 462/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0222 - acc: 0.9859 - val_loss: 0.0159 - val_acc: 0.9872\n",
      "Epoch 463/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0230 - acc: 0.9844 - val_loss: 0.0159 - val_acc: 0.9855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 464/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0226 - acc: 0.9864 - val_loss: 0.0166 - val_acc: 0.9847\n",
      "Epoch 465/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0228 - acc: 0.9859 - val_loss: 0.0157 - val_acc: 0.9838\n",
      "Epoch 466/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0223 - acc: 0.9857 - val_loss: 0.0156 - val_acc: 0.9855\n",
      "Epoch 467/500\n",
      "22272/22272 [==============================] - 16s 735us/step - loss: 0.0227 - acc: 0.9846 - val_loss: 0.0172 - val_acc: 0.9829\n",
      "Epoch 468/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0227 - acc: 0.9854 - val_loss: 0.0161 - val_acc: 0.9889\n",
      "Epoch 469/500\n",
      "22272/22272 [==============================] - 15s 677us/step - loss: 0.0227 - acc: 0.9866 - val_loss: 0.0164 - val_acc: 0.9898\n",
      "Epoch 470/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0227 - acc: 0.9857 - val_loss: 0.0168 - val_acc: 0.9847\n",
      "Epoch 471/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0226 - acc: 0.9854 - val_loss: 0.0160 - val_acc: 0.9864\n",
      "Epoch 472/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0222 - acc: 0.9851 - val_loss: 0.0163 - val_acc: 0.9838\n",
      "Epoch 473/500\n",
      "22272/22272 [==============================] - 15s 680us/step - loss: 0.0226 - acc: 0.9862 - val_loss: 0.0165 - val_acc: 0.9855\n",
      "Epoch 474/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0222 - acc: 0.9859 - val_loss: 0.0172 - val_acc: 0.9881\n",
      "Epoch 475/500\n",
      "22272/22272 [==============================] - 15s 680us/step - loss: 0.0226 - acc: 0.9855 - val_loss: 0.0155 - val_acc: 0.9889\n",
      "Epoch 476/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0220 - acc: 0.9858 - val_loss: 0.0172 - val_acc: 0.9847\n",
      "Epoch 477/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0219 - acc: 0.9862 - val_loss: 0.0162 - val_acc: 0.9881\n",
      "Epoch 478/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0223 - acc: 0.9855 - val_loss: 0.0154 - val_acc: 0.9881\n",
      "Epoch 479/500\n",
      "22272/22272 [==============================] - 15s 678us/step - loss: 0.0219 - acc: 0.9863 - val_loss: 0.0167 - val_acc: 0.9872\n",
      "Epoch 480/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0218 - acc: 0.9853 - val_loss: 0.0161 - val_acc: 0.9898\n",
      "Epoch 481/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0222 - acc: 0.9865 - val_loss: 0.0153 - val_acc: 0.9855\n",
      "Epoch 482/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0219 - acc: 0.9863 - val_loss: 0.0173 - val_acc: 0.9838\n",
      "Epoch 483/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0222 - acc: 0.9863 - val_loss: 0.0166 - val_acc: 0.9847\n",
      "Epoch 484/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0219 - acc: 0.9870 - val_loss: 0.0155 - val_acc: 0.9889\n",
      "Epoch 485/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0220 - acc: 0.9859 - val_loss: 0.0165 - val_acc: 0.9855\n",
      "Epoch 486/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0219 - acc: 0.9860 - val_loss: 0.0157 - val_acc: 0.9838\n",
      "Epoch 487/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0218 - acc: 0.9868 - val_loss: 0.0151 - val_acc: 0.9847\n",
      "Epoch 488/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0221 - acc: 0.9862 - val_loss: 0.0173 - val_acc: 0.9855\n",
      "Epoch 489/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0221 - acc: 0.9850 - val_loss: 0.0159 - val_acc: 0.9864\n",
      "Epoch 490/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0222 - acc: 0.9849 - val_loss: 0.0159 - val_acc: 0.9847\n",
      "Epoch 491/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0222 - acc: 0.9852 - val_loss: 0.0169 - val_acc: 0.9915\n",
      "Epoch 492/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0220 - acc: 0.9846 - val_loss: 0.0167 - val_acc: 0.9847\n",
      "Epoch 493/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0221 - acc: 0.9851 - val_loss: 0.0183 - val_acc: 0.9855\n",
      "Epoch 494/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0223 - acc: 0.9861 - val_loss: 0.0165 - val_acc: 0.9872\n",
      "Epoch 495/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0218 - acc: 0.9864 - val_loss: 0.0154 - val_acc: 0.9889\n",
      "Epoch 496/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0217 - acc: 0.9856 - val_loss: 0.0164 - val_acc: 0.9906\n",
      "Epoch 497/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0219 - acc: 0.9856 - val_loss: 0.0165 - val_acc: 0.9889\n",
      "Epoch 498/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0214 - acc: 0.9859 - val_loss: 0.0146 - val_acc: 0.9881\n",
      "Epoch 499/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0220 - acc: 0.9870 - val_loss: 0.0168 - val_acc: 0.9855\n",
      "Epoch 500/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0220 - acc: 0.9865 - val_loss: 0.0149 - val_acc: 0.9838\n"
     ]
    }
   ],
   "source": [
    "##Fitting the GRU to the Training set\n",
    "hist3=model3.fit(X_train, y_train, batch_size=300, epochs=500, validation_split=0.05, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22272 samples, validate on 1173 samples\n",
      "Epoch 1/500\n",
      "22272/22272 [==============================] - 19s 860us/step - loss: 0.2413 - acc: 0.7489 - val_loss: 0.2232 - val_acc: 0.7255\n",
      "Epoch 2/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.2123 - acc: 0.7466 - val_loss: 0.2114 - val_acc: 0.7170\n",
      "Epoch 3/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.2022 - acc: 0.7481 - val_loss: 0.2046 - val_acc: 0.7289\n",
      "Epoch 4/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.1987 - acc: 0.7531 - val_loss: 0.2020 - val_acc: 0.7323\n",
      "Epoch 5/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.1967 - acc: 0.7546 - val_loss: 0.2027 - val_acc: 0.7417\n",
      "Epoch 6/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.1950 - acc: 0.7585 - val_loss: 0.2005 - val_acc: 0.7340\n",
      "Epoch 7/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.1953 - acc: 0.7542 - val_loss: 0.1957 - val_acc: 0.7374\n",
      "Epoch 8/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.1908 - acc: 0.7522 - val_loss: 0.1967 - val_acc: 0.7298\n",
      "Epoch 9/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.1887 - acc: 0.7553 - val_loss: 0.1878 - val_acc: 0.7485\n",
      "Epoch 10/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.1825 - acc: 0.7623 - val_loss: 0.1829 - val_acc: 0.7536\n",
      "Epoch 11/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1774 - acc: 0.7697 - val_loss: 0.1789 - val_acc: 0.7562\n",
      "Epoch 12/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.1752 - acc: 0.7737 - val_loss: 0.1787 - val_acc: 0.7502\n",
      "Epoch 13/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.1720 - acc: 0.7783 - val_loss: 0.1786 - val_acc: 0.7656\n",
      "Epoch 14/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.1714 - acc: 0.7831 - val_loss: 0.1785 - val_acc: 0.7656\n",
      "Epoch 15/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.1700 - acc: 0.7838 - val_loss: 0.1756 - val_acc: 0.7741\n",
      "Epoch 16/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.1666 - acc: 0.7882 - val_loss: 0.1701 - val_acc: 0.7749\n",
      "Epoch 17/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.1660 - acc: 0.7888 - val_loss: 0.1816 - val_acc: 0.7741\n",
      "Epoch 18/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.1635 - acc: 0.7914 - val_loss: 0.1663 - val_acc: 0.7749\n",
      "Epoch 19/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.1614 - acc: 0.7945 - val_loss: 0.1679 - val_acc: 0.7809\n",
      "Epoch 20/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1594 - acc: 0.7947 - val_loss: 0.1648 - val_acc: 0.7681\n",
      "Epoch 21/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.1588 - acc: 0.7968 - val_loss: 0.1683 - val_acc: 0.7732\n",
      "Epoch 22/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.1571 - acc: 0.8024 - val_loss: 0.1641 - val_acc: 0.7741\n",
      "Epoch 23/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.1558 - acc: 0.7985 - val_loss: 0.1598 - val_acc: 0.7843\n",
      "Epoch 24/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.1557 - acc: 0.8004 - val_loss: 0.1618 - val_acc: 0.7809\n",
      "Epoch 25/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1546 - acc: 0.8031 - val_loss: 0.1604 - val_acc: 0.7664\n",
      "Epoch 26/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.1525 - acc: 0.8023 - val_loss: 0.1621 - val_acc: 0.7792\n",
      "Epoch 27/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.1519 - acc: 0.8038 - val_loss: 0.1625 - val_acc: 0.7826\n",
      "Epoch 28/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.1495 - acc: 0.8077 - val_loss: 0.1587 - val_acc: 0.7775\n",
      "Epoch 29/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.1485 - acc: 0.8107 - val_loss: 0.1514 - val_acc: 0.8005\n",
      "Epoch 30/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.1465 - acc: 0.8129 - val_loss: 0.1565 - val_acc: 0.7962\n",
      "Epoch 31/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.1454 - acc: 0.8161 - val_loss: 0.1569 - val_acc: 0.7869\n",
      "Epoch 32/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.1460 - acc: 0.8162 - val_loss: 0.1540 - val_acc: 0.8031\n",
      "Epoch 33/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.1455 - acc: 0.8153 - val_loss: 0.1526 - val_acc: 0.7980\n",
      "Epoch 34/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1441 - acc: 0.8193 - val_loss: 0.1500 - val_acc: 0.8082\n",
      "Epoch 35/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.1423 - acc: 0.8175 - val_loss: 0.1488 - val_acc: 0.8056\n",
      "Epoch 36/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.1418 - acc: 0.8188 - val_loss: 0.1458 - val_acc: 0.8073\n",
      "Epoch 37/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1416 - acc: 0.8211 - val_loss: 0.1536 - val_acc: 0.8056\n",
      "Epoch 38/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.1405 - acc: 0.8201 - val_loss: 0.1557 - val_acc: 0.8005\n",
      "Epoch 39/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.1394 - acc: 0.8252 - val_loss: 0.1467 - val_acc: 0.7997\n",
      "Epoch 40/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1373 - acc: 0.8275 - val_loss: 0.1438 - val_acc: 0.8116\n",
      "Epoch 41/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1372 - acc: 0.8257 - val_loss: 0.1463 - val_acc: 0.8167\n",
      "Epoch 42/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.1364 - acc: 0.8280 - val_loss: 0.1420 - val_acc: 0.8142\n",
      "Epoch 43/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.1360 - acc: 0.8290 - val_loss: 0.1426 - val_acc: 0.8227\n",
      "Epoch 44/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.1342 - acc: 0.8319 - val_loss: 0.1404 - val_acc: 0.8193\n",
      "Epoch 45/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.1345 - acc: 0.8343 - val_loss: 0.1374 - val_acc: 0.8312\n",
      "Epoch 46/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.1329 - acc: 0.8341 - val_loss: 0.1364 - val_acc: 0.8321\n",
      "Epoch 47/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.1328 - acc: 0.8349 - val_loss: 0.1343 - val_acc: 0.8329\n",
      "Epoch 48/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.1305 - acc: 0.8381 - val_loss: 0.1340 - val_acc: 0.8321\n",
      "Epoch 49/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.1303 - acc: 0.8398 - val_loss: 0.1352 - val_acc: 0.8355\n",
      "Epoch 50/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.1306 - acc: 0.8405 - val_loss: 0.1329 - val_acc: 0.8372\n",
      "Epoch 51/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.1275 - acc: 0.8419 - val_loss: 0.1307 - val_acc: 0.8414\n",
      "Epoch 52/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.1261 - acc: 0.8441 - val_loss: 0.1272 - val_acc: 0.8406\n",
      "Epoch 53/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.1265 - acc: 0.8427 - val_loss: 0.1297 - val_acc: 0.8380\n",
      "Epoch 54/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.1235 - acc: 0.8504 - val_loss: 0.1291 - val_acc: 0.8414\n",
      "Epoch 55/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.1230 - acc: 0.8490 - val_loss: 0.1271 - val_acc: 0.8372\n",
      "Epoch 56/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1230 - acc: 0.8491 - val_loss: 0.1278 - val_acc: 0.8414\n",
      "Epoch 57/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.1208 - acc: 0.8524 - val_loss: 0.1231 - val_acc: 0.8483\n",
      "Epoch 58/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.1205 - acc: 0.8529 - val_loss: 0.1217 - val_acc: 0.8414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1194 - acc: 0.8526 - val_loss: 0.1199 - val_acc: 0.8585\n",
      "Epoch 60/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.1181 - acc: 0.8571 - val_loss: 0.1221 - val_acc: 0.8483\n",
      "Epoch 61/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1181 - acc: 0.8554 - val_loss: 0.1180 - val_acc: 0.8508\n",
      "Epoch 62/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.1147 - acc: 0.8595 - val_loss: 0.1197 - val_acc: 0.8372\n",
      "Epoch 63/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.1152 - acc: 0.8600 - val_loss: 0.1222 - val_acc: 0.8508\n",
      "Epoch 64/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1130 - acc: 0.8653 - val_loss: 0.1121 - val_acc: 0.8576\n",
      "Epoch 65/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1124 - acc: 0.8630 - val_loss: 0.1115 - val_acc: 0.8636\n",
      "Epoch 66/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1128 - acc: 0.8635 - val_loss: 0.1108 - val_acc: 0.8585\n",
      "Epoch 67/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1103 - acc: 0.8678 - val_loss: 0.1090 - val_acc: 0.8525\n",
      "Epoch 68/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.1109 - acc: 0.8675 - val_loss: 0.1065 - val_acc: 0.8721\n",
      "Epoch 69/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.1079 - acc: 0.8705 - val_loss: 0.1137 - val_acc: 0.8662\n",
      "Epoch 70/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.1062 - acc: 0.8723 - val_loss: 0.1054 - val_acc: 0.8679\n",
      "Epoch 71/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.1067 - acc: 0.8715 - val_loss: 0.1080 - val_acc: 0.8721\n",
      "Epoch 72/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.1051 - acc: 0.8732 - val_loss: 0.1186 - val_acc: 0.8542\n",
      "Epoch 73/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.1046 - acc: 0.8738 - val_loss: 0.1033 - val_acc: 0.8781\n",
      "Epoch 74/500\n",
      "22272/22272 [==============================] - 16s 711us/step - loss: 0.1027 - acc: 0.8771 - val_loss: 0.1053 - val_acc: 0.8704\n",
      "Epoch 75/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.1027 - acc: 0.8778 - val_loss: 0.0989 - val_acc: 0.8832\n",
      "Epoch 76/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.1002 - acc: 0.8807 - val_loss: 0.1001 - val_acc: 0.8824\n",
      "Epoch 77/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0998 - acc: 0.8819 - val_loss: 0.0965 - val_acc: 0.8696\n",
      "Epoch 78/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0997 - acc: 0.8825 - val_loss: 0.1031 - val_acc: 0.8713\n",
      "Epoch 79/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0986 - acc: 0.8835 - val_loss: 0.0970 - val_acc: 0.8815\n",
      "Epoch 80/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0967 - acc: 0.8849 - val_loss: 0.0966 - val_acc: 0.8849\n",
      "Epoch 81/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0970 - acc: 0.8880 - val_loss: 0.0940 - val_acc: 0.8900\n",
      "Epoch 82/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0956 - acc: 0.8897 - val_loss: 0.0949 - val_acc: 0.8824\n",
      "Epoch 83/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0948 - acc: 0.8884 - val_loss: 0.0945 - val_acc: 0.8866\n",
      "Epoch 84/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0932 - acc: 0.8921 - val_loss: 0.0894 - val_acc: 0.8900\n",
      "Epoch 85/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0924 - acc: 0.8926 - val_loss: 0.0912 - val_acc: 0.8934\n",
      "Epoch 86/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0915 - acc: 0.8930 - val_loss: 0.0923 - val_acc: 0.8832\n",
      "Epoch 87/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0919 - acc: 0.8920 - val_loss: 0.0981 - val_acc: 0.8798\n",
      "Epoch 88/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0909 - acc: 0.8955 - val_loss: 0.0879 - val_acc: 0.8943\n",
      "Epoch 89/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0900 - acc: 0.8991 - val_loss: 0.0874 - val_acc: 0.8986\n",
      "Epoch 90/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0890 - acc: 0.8958 - val_loss: 0.0967 - val_acc: 0.8934\n",
      "Epoch 91/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0883 - acc: 0.8988 - val_loss: 0.0874 - val_acc: 0.8909\n",
      "Epoch 92/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0866 - acc: 0.9006 - val_loss: 0.0850 - val_acc: 0.8977\n",
      "Epoch 93/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0867 - acc: 0.9013 - val_loss: 0.0837 - val_acc: 0.9020\n",
      "Epoch 94/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0860 - acc: 0.9003 - val_loss: 0.0825 - val_acc: 0.8909\n",
      "Epoch 95/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0853 - acc: 0.9033 - val_loss: 0.0891 - val_acc: 0.8977\n",
      "Epoch 96/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0855 - acc: 0.9028 - val_loss: 0.0820 - val_acc: 0.9003\n",
      "Epoch 97/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0835 - acc: 0.9069 - val_loss: 0.0766 - val_acc: 0.9147\n",
      "Epoch 98/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0829 - acc: 0.9069 - val_loss: 0.0863 - val_acc: 0.8943\n",
      "Epoch 99/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0826 - acc: 0.9062 - val_loss: 0.0848 - val_acc: 0.9054\n",
      "Epoch 100/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0822 - acc: 0.9090 - val_loss: 0.0805 - val_acc: 0.9071\n",
      "Epoch 101/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0799 - acc: 0.9115 - val_loss: 0.0772 - val_acc: 0.9130\n",
      "Epoch 102/500\n",
      "22272/22272 [==============================] - 16s 711us/step - loss: 0.0792 - acc: 0.9121 - val_loss: 0.0765 - val_acc: 0.9088\n",
      "Epoch 103/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.0793 - acc: 0.9115 - val_loss: 0.0791 - val_acc: 0.9088\n",
      "Epoch 104/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0792 - acc: 0.9122 - val_loss: 0.0761 - val_acc: 0.9139\n",
      "Epoch 105/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.0780 - acc: 0.9139 - val_loss: 0.0720 - val_acc: 0.9207\n",
      "Epoch 106/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0779 - acc: 0.9141 - val_loss: 0.0723 - val_acc: 0.9216\n",
      "Epoch 107/500\n",
      "22272/22272 [==============================] - 16s 727us/step - loss: 0.0772 - acc: 0.9152 - val_loss: 0.0713 - val_acc: 0.9233\n",
      "Epoch 108/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0765 - acc: 0.9159 - val_loss: 0.0733 - val_acc: 0.9156\n",
      "Epoch 109/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0751 - acc: 0.9179 - val_loss: 0.0716 - val_acc: 0.9224\n",
      "Epoch 110/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0758 - acc: 0.9162 - val_loss: 0.0693 - val_acc: 0.9216\n",
      "Epoch 111/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0738 - acc: 0.9189 - val_loss: 0.0703 - val_acc: 0.9267\n",
      "Epoch 112/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0740 - acc: 0.9203 - val_loss: 0.0718 - val_acc: 0.9207\n",
      "Epoch 113/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0725 - acc: 0.9208 - val_loss: 0.0676 - val_acc: 0.9233\n",
      "Epoch 114/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0727 - acc: 0.9230 - val_loss: 0.0743 - val_acc: 0.9190\n",
      "Epoch 115/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0722 - acc: 0.9216 - val_loss: 0.0723 - val_acc: 0.9139\n",
      "Epoch 116/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0723 - acc: 0.9225 - val_loss: 0.0696 - val_acc: 0.9275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0719 - acc: 0.9238 - val_loss: 0.0647 - val_acc: 0.9241\n",
      "Epoch 118/500\n",
      "22272/22272 [==============================] - 15s 680us/step - loss: 0.0707 - acc: 0.9238 - val_loss: 0.0713 - val_acc: 0.9241\n",
      "Epoch 119/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0696 - acc: 0.9251 - val_loss: 0.0650 - val_acc: 0.9284\n",
      "Epoch 120/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0683 - acc: 0.9276 - val_loss: 0.0661 - val_acc: 0.9292\n",
      "Epoch 121/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0690 - acc: 0.9261 - val_loss: 0.0677 - val_acc: 0.9224\n",
      "Epoch 122/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0683 - acc: 0.9300 - val_loss: 0.0632 - val_acc: 0.9335\n",
      "Epoch 123/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0676 - acc: 0.9322 - val_loss: 0.0640 - val_acc: 0.9284\n",
      "Epoch 124/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0674 - acc: 0.9293 - val_loss: 0.0618 - val_acc: 0.9352\n",
      "Epoch 125/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0669 - acc: 0.9303 - val_loss: 0.0650 - val_acc: 0.9275\n",
      "Epoch 126/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0660 - acc: 0.9324 - val_loss: 0.0713 - val_acc: 0.9216\n",
      "Epoch 127/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0657 - acc: 0.9335 - val_loss: 0.0604 - val_acc: 0.9361\n",
      "Epoch 128/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0654 - acc: 0.9338 - val_loss: 0.0605 - val_acc: 0.9327\n",
      "Epoch 129/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0651 - acc: 0.9300 - val_loss: 0.0588 - val_acc: 0.9335\n",
      "Epoch 130/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0643 - acc: 0.9342 - val_loss: 0.0605 - val_acc: 0.9335\n",
      "Epoch 131/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0639 - acc: 0.9349 - val_loss: 0.0594 - val_acc: 0.9361\n",
      "Epoch 132/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0644 - acc: 0.9355 - val_loss: 0.0576 - val_acc: 0.9344\n",
      "Epoch 133/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0626 - acc: 0.9369 - val_loss: 0.0573 - val_acc: 0.9378\n",
      "Epoch 134/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0637 - acc: 0.9341 - val_loss: 0.0590 - val_acc: 0.9344\n",
      "Epoch 135/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0620 - acc: 0.9379 - val_loss: 0.0560 - val_acc: 0.9429\n",
      "Epoch 136/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0616 - acc: 0.9385 - val_loss: 0.0579 - val_acc: 0.9344\n",
      "Epoch 137/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0612 - acc: 0.9399 - val_loss: 0.0569 - val_acc: 0.9386\n",
      "Epoch 138/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0616 - acc: 0.9370 - val_loss: 0.0600 - val_acc: 0.9386\n",
      "Epoch 139/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0612 - acc: 0.9397 - val_loss: 0.0556 - val_acc: 0.9412\n",
      "Epoch 140/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0604 - acc: 0.9389 - val_loss: 0.0542 - val_acc: 0.9454\n",
      "Epoch 141/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0596 - acc: 0.9412 - val_loss: 0.0548 - val_acc: 0.9420\n",
      "Epoch 142/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0592 - acc: 0.9397 - val_loss: 0.0529 - val_acc: 0.9454\n",
      "Epoch 143/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0585 - acc: 0.9434 - val_loss: 0.0528 - val_acc: 0.9429\n",
      "Epoch 144/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0583 - acc: 0.9440 - val_loss: 0.0516 - val_acc: 0.9497\n",
      "Epoch 145/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0580 - acc: 0.9450 - val_loss: 0.0513 - val_acc: 0.9471\n",
      "Epoch 146/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0570 - acc: 0.9451 - val_loss: 0.0489 - val_acc: 0.9540\n",
      "Epoch 147/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0566 - acc: 0.9443 - val_loss: 0.0533 - val_acc: 0.9446\n",
      "Epoch 148/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0568 - acc: 0.9457 - val_loss: 0.0539 - val_acc: 0.9463\n",
      "Epoch 149/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0566 - acc: 0.9440 - val_loss: 0.0507 - val_acc: 0.9480\n",
      "Epoch 150/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0561 - acc: 0.9460 - val_loss: 0.0489 - val_acc: 0.9488\n",
      "Epoch 151/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0568 - acc: 0.9449 - val_loss: 0.0485 - val_acc: 0.9497\n",
      "Epoch 152/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0546 - acc: 0.9485 - val_loss: 0.0505 - val_acc: 0.9471\n",
      "Epoch 153/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0546 - acc: 0.9471 - val_loss: 0.0527 - val_acc: 0.9420\n",
      "Epoch 154/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0552 - acc: 0.9478 - val_loss: 0.0529 - val_acc: 0.9463\n",
      "Epoch 155/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0545 - acc: 0.9483 - val_loss: 0.0515 - val_acc: 0.9454\n",
      "Epoch 156/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0537 - acc: 0.9501 - val_loss: 0.0510 - val_acc: 0.9446\n",
      "Epoch 157/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0533 - acc: 0.9515 - val_loss: 0.0494 - val_acc: 0.9523\n",
      "Epoch 158/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0536 - acc: 0.9496 - val_loss: 0.0479 - val_acc: 0.9471\n",
      "Epoch 159/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0523 - acc: 0.9524 - val_loss: 0.0460 - val_acc: 0.9548\n",
      "Epoch 160/500\n",
      "22272/22272 [==============================] - 16s 721us/step - loss: 0.0529 - acc: 0.9514 - val_loss: 0.0476 - val_acc: 0.9523\n",
      "Epoch 161/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0527 - acc: 0.9517 - val_loss: 0.0463 - val_acc: 0.9565\n",
      "Epoch 162/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0520 - acc: 0.9516 - val_loss: 0.0465 - val_acc: 0.9574\n",
      "Epoch 163/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0525 - acc: 0.9515 - val_loss: 0.0489 - val_acc: 0.9531\n",
      "Epoch 164/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0515 - acc: 0.9525 - val_loss: 0.0447 - val_acc: 0.9531\n",
      "Epoch 165/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0516 - acc: 0.9519 - val_loss: 0.0464 - val_acc: 0.9599\n",
      "Epoch 166/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0506 - acc: 0.9551 - val_loss: 0.0446 - val_acc: 0.9540\n",
      "Epoch 167/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0502 - acc: 0.9536 - val_loss: 0.0456 - val_acc: 0.9565\n",
      "Epoch 168/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0511 - acc: 0.9525 - val_loss: 0.0466 - val_acc: 0.9514\n",
      "Epoch 169/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0506 - acc: 0.9543 - val_loss: 0.0455 - val_acc: 0.9557\n",
      "Epoch 170/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0493 - acc: 0.9555 - val_loss: 0.0425 - val_acc: 0.9565\n",
      "Epoch 171/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0494 - acc: 0.9547 - val_loss: 0.0422 - val_acc: 0.9582\n",
      "Epoch 172/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0497 - acc: 0.9541 - val_loss: 0.0441 - val_acc: 0.9591\n",
      "Epoch 173/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0486 - acc: 0.9565 - val_loss: 0.0443 - val_acc: 0.9506\n",
      "Epoch 174/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0486 - acc: 0.9566 - val_loss: 0.0437 - val_acc: 0.9582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0490 - acc: 0.9578 - val_loss: 0.0443 - val_acc: 0.9625\n",
      "Epoch 176/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0482 - acc: 0.9589 - val_loss: 0.0414 - val_acc: 0.9650\n",
      "Epoch 177/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0482 - acc: 0.9561 - val_loss: 0.0415 - val_acc: 0.9591\n",
      "Epoch 178/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0468 - acc: 0.9599 - val_loss: 0.0421 - val_acc: 0.9608\n",
      "Epoch 179/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0475 - acc: 0.9605 - val_loss: 0.0424 - val_acc: 0.9608\n",
      "Epoch 180/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0469 - acc: 0.9600 - val_loss: 0.0394 - val_acc: 0.9591\n",
      "Epoch 181/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0464 - acc: 0.9598 - val_loss: 0.0413 - val_acc: 0.9582\n",
      "Epoch 182/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0469 - acc: 0.9588 - val_loss: 0.0385 - val_acc: 0.9625\n",
      "Epoch 183/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0468 - acc: 0.9594 - val_loss: 0.0412 - val_acc: 0.9582\n",
      "Epoch 184/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0464 - acc: 0.9603 - val_loss: 0.0439 - val_acc: 0.9582\n",
      "Epoch 185/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0470 - acc: 0.9595 - val_loss: 0.0412 - val_acc: 0.9642\n",
      "Epoch 186/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0455 - acc: 0.9603 - val_loss: 0.0406 - val_acc: 0.9591\n",
      "Epoch 187/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0459 - acc: 0.9606 - val_loss: 0.0401 - val_acc: 0.9599\n",
      "Epoch 188/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0451 - acc: 0.9619 - val_loss: 0.0415 - val_acc: 0.9599\n",
      "Epoch 189/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0445 - acc: 0.9627 - val_loss: 0.0388 - val_acc: 0.9650\n",
      "Epoch 190/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0451 - acc: 0.9637 - val_loss: 0.0398 - val_acc: 0.9642\n",
      "Epoch 191/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0454 - acc: 0.9597 - val_loss: 0.0372 - val_acc: 0.9702\n",
      "Epoch 192/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0445 - acc: 0.9623 - val_loss: 0.0392 - val_acc: 0.9650\n",
      "Epoch 193/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0447 - acc: 0.9621 - val_loss: 0.0388 - val_acc: 0.9633\n",
      "Epoch 194/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0446 - acc: 0.9617 - val_loss: 0.0380 - val_acc: 0.9685\n",
      "Epoch 195/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0436 - acc: 0.9635 - val_loss: 0.0372 - val_acc: 0.9693\n",
      "Epoch 196/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0437 - acc: 0.9632 - val_loss: 0.0384 - val_acc: 0.9650\n",
      "Epoch 197/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0430 - acc: 0.9650 - val_loss: 0.0393 - val_acc: 0.9633\n",
      "Epoch 198/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0429 - acc: 0.9653 - val_loss: 0.0382 - val_acc: 0.9642\n",
      "Epoch 199/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0424 - acc: 0.9657 - val_loss: 0.0415 - val_acc: 0.9608\n",
      "Epoch 200/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0438 - acc: 0.9636 - val_loss: 0.0383 - val_acc: 0.9650\n",
      "Epoch 201/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0433 - acc: 0.9637 - val_loss: 0.0353 - val_acc: 0.9744\n",
      "Epoch 202/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0425 - acc: 0.9668 - val_loss: 0.0353 - val_acc: 0.9676\n",
      "Epoch 203/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0430 - acc: 0.9639 - val_loss: 0.0355 - val_acc: 0.9719\n",
      "Epoch 204/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0422 - acc: 0.9643 - val_loss: 0.0349 - val_acc: 0.9693\n",
      "Epoch 205/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0423 - acc: 0.9657 - val_loss: 0.0348 - val_acc: 0.9693\n",
      "Epoch 206/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0419 - acc: 0.9665 - val_loss: 0.0454 - val_acc: 0.9548\n",
      "Epoch 207/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0415 - acc: 0.9657 - val_loss: 0.0346 - val_acc: 0.9685\n",
      "Epoch 208/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0421 - acc: 0.9660 - val_loss: 0.0355 - val_acc: 0.9625\n",
      "Epoch 209/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0414 - acc: 0.9641 - val_loss: 0.0370 - val_acc: 0.9591\n",
      "Epoch 210/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0405 - acc: 0.9689 - val_loss: 0.0360 - val_acc: 0.9693\n",
      "Epoch 211/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0414 - acc: 0.9674 - val_loss: 0.0325 - val_acc: 0.9702\n",
      "Epoch 212/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0406 - acc: 0.9669 - val_loss: 0.0344 - val_acc: 0.9702\n",
      "Epoch 213/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0410 - acc: 0.9681 - val_loss: 0.0335 - val_acc: 0.9668\n",
      "Epoch 214/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0405 - acc: 0.9690 - val_loss: 0.0360 - val_acc: 0.9693\n",
      "Epoch 215/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0405 - acc: 0.9674 - val_loss: 0.0326 - val_acc: 0.9710\n",
      "Epoch 216/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0395 - acc: 0.9692 - val_loss: 0.0346 - val_acc: 0.9693\n",
      "Epoch 217/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0401 - acc: 0.9687 - val_loss: 0.0353 - val_acc: 0.9727\n",
      "Epoch 218/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0417 - acc: 0.9655 - val_loss: 0.0347 - val_acc: 0.9625\n",
      "Epoch 219/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0401 - acc: 0.9671 - val_loss: 0.0331 - val_acc: 0.9710\n",
      "Epoch 220/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0400 - acc: 0.9688 - val_loss: 0.0318 - val_acc: 0.9736\n",
      "Epoch 221/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0393 - acc: 0.9680 - val_loss: 0.0336 - val_acc: 0.9727\n",
      "Epoch 222/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0388 - acc: 0.9697 - val_loss: 0.0316 - val_acc: 0.9710\n",
      "Epoch 223/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0397 - acc: 0.9681 - val_loss: 0.0356 - val_acc: 0.9642\n",
      "Epoch 224/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0391 - acc: 0.9690 - val_loss: 0.0340 - val_acc: 0.9676\n",
      "Epoch 225/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0384 - acc: 0.9690 - val_loss: 0.0314 - val_acc: 0.9736\n",
      "Epoch 226/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0386 - acc: 0.9700 - val_loss: 0.0341 - val_acc: 0.9727\n",
      "Epoch 227/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0384 - acc: 0.9696 - val_loss: 0.0310 - val_acc: 0.9676\n",
      "Epoch 228/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0389 - acc: 0.9685 - val_loss: 0.0347 - val_acc: 0.9693\n",
      "Epoch 229/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0378 - acc: 0.9710 - val_loss: 0.0324 - val_acc: 0.9693\n",
      "Epoch 230/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0381 - acc: 0.9708 - val_loss: 0.0324 - val_acc: 0.9736\n",
      "Epoch 231/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0387 - acc: 0.9695 - val_loss: 0.0318 - val_acc: 0.9761\n",
      "Epoch 232/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0379 - acc: 0.9705 - val_loss: 0.0342 - val_acc: 0.9719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/500\n",
      "22272/22272 [==============================] - 15s 680us/step - loss: 0.0375 - acc: 0.9712 - val_loss: 0.0293 - val_acc: 0.9744\n",
      "Epoch 234/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0379 - acc: 0.9704 - val_loss: 0.0313 - val_acc: 0.9761\n",
      "Epoch 235/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0375 - acc: 0.9718 - val_loss: 0.0317 - val_acc: 0.9727\n",
      "Epoch 236/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0378 - acc: 0.9704 - val_loss: 0.0306 - val_acc: 0.9736\n",
      "Epoch 237/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0371 - acc: 0.9723 - val_loss: 0.0320 - val_acc: 0.9719\n",
      "Epoch 238/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0373 - acc: 0.9693 - val_loss: 0.0295 - val_acc: 0.9795\n",
      "Epoch 239/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0371 - acc: 0.9710 - val_loss: 0.0300 - val_acc: 0.9736\n",
      "Epoch 240/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0358 - acc: 0.9736 - val_loss: 0.0289 - val_acc: 0.9727\n",
      "Epoch 241/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0364 - acc: 0.9721 - val_loss: 0.0318 - val_acc: 0.9710\n",
      "Epoch 242/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0368 - acc: 0.9719 - val_loss: 0.0302 - val_acc: 0.9761\n",
      "Epoch 243/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0360 - acc: 0.9710 - val_loss: 0.0312 - val_acc: 0.9719\n",
      "Epoch 244/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0360 - acc: 0.9715 - val_loss: 0.0286 - val_acc: 0.9727\n",
      "Epoch 245/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0359 - acc: 0.9729 - val_loss: 0.0324 - val_acc: 0.9710\n",
      "Epoch 246/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0357 - acc: 0.9723 - val_loss: 0.0309 - val_acc: 0.9753\n",
      "Epoch 247/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0361 - acc: 0.9713 - val_loss: 0.0316 - val_acc: 0.9719\n",
      "Epoch 248/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0359 - acc: 0.9716 - val_loss: 0.0291 - val_acc: 0.9753\n",
      "Epoch 249/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0362 - acc: 0.9723 - val_loss: 0.0305 - val_acc: 0.9719\n",
      "Epoch 250/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0355 - acc: 0.9732 - val_loss: 0.0289 - val_acc: 0.9804\n",
      "Epoch 251/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0353 - acc: 0.9745 - val_loss: 0.0299 - val_acc: 0.9719\n",
      "Epoch 252/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0352 - acc: 0.9722 - val_loss: 0.0279 - val_acc: 0.9770\n",
      "Epoch 253/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0352 - acc: 0.9740 - val_loss: 0.0290 - val_acc: 0.9787\n",
      "Epoch 254/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0356 - acc: 0.9740 - val_loss: 0.0294 - val_acc: 0.9778\n",
      "Epoch 255/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0350 - acc: 0.9734 - val_loss: 0.0281 - val_acc: 0.9744\n",
      "Epoch 256/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0346 - acc: 0.9719 - val_loss: 0.0274 - val_acc: 0.9761\n",
      "Epoch 257/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0349 - acc: 0.9736 - val_loss: 0.0305 - val_acc: 0.9744\n",
      "Epoch 258/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0350 - acc: 0.9729 - val_loss: 0.0296 - val_acc: 0.9770\n",
      "Epoch 259/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0347 - acc: 0.9746 - val_loss: 0.0305 - val_acc: 0.9744\n",
      "Epoch 260/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0342 - acc: 0.9753 - val_loss: 0.0290 - val_acc: 0.9736\n",
      "Epoch 261/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0344 - acc: 0.9739 - val_loss: 0.0278 - val_acc: 0.9778\n",
      "Epoch 262/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0337 - acc: 0.9755 - val_loss: 0.0279 - val_acc: 0.9787\n",
      "Epoch 263/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0339 - acc: 0.9749 - val_loss: 0.0294 - val_acc: 0.9761\n",
      "Epoch 264/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0347 - acc: 0.9739 - val_loss: 0.0284 - val_acc: 0.9753\n",
      "Epoch 265/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0340 - acc: 0.9747 - val_loss: 0.0250 - val_acc: 0.9804\n",
      "Epoch 266/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0343 - acc: 0.9745 - val_loss: 0.0274 - val_acc: 0.9829\n",
      "Epoch 267/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0342 - acc: 0.9754 - val_loss: 0.0270 - val_acc: 0.9804\n",
      "Epoch 268/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0339 - acc: 0.9750 - val_loss: 0.0266 - val_acc: 0.9829\n",
      "Epoch 269/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0334 - acc: 0.9749 - val_loss: 0.0263 - val_acc: 0.9804\n",
      "Epoch 270/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0335 - acc: 0.9760 - val_loss: 0.0282 - val_acc: 0.9753\n",
      "Epoch 271/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0336 - acc: 0.9745 - val_loss: 0.0263 - val_acc: 0.9804\n",
      "Epoch 272/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0335 - acc: 0.9748 - val_loss: 0.0279 - val_acc: 0.9761\n",
      "Epoch 273/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0341 - acc: 0.9736 - val_loss: 0.0288 - val_acc: 0.9744\n",
      "Epoch 274/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0332 - acc: 0.9753 - val_loss: 0.0261 - val_acc: 0.9753\n",
      "Epoch 275/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0329 - acc: 0.9752 - val_loss: 0.0272 - val_acc: 0.9804\n",
      "Epoch 276/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0334 - acc: 0.9764 - val_loss: 0.0269 - val_acc: 0.9778\n",
      "Epoch 277/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0329 - acc: 0.9747 - val_loss: 0.0252 - val_acc: 0.9812\n",
      "Epoch 278/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0324 - acc: 0.9749 - val_loss: 0.0258 - val_acc: 0.9753\n",
      "Epoch 279/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0328 - acc: 0.9736 - val_loss: 0.0278 - val_acc: 0.9787\n",
      "Epoch 280/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0335 - acc: 0.9757 - val_loss: 0.0268 - val_acc: 0.9761\n",
      "Epoch 281/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0326 - acc: 0.9759 - val_loss: 0.0289 - val_acc: 0.9761\n",
      "Epoch 282/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0327 - acc: 0.9757 - val_loss: 0.0280 - val_acc: 0.9787\n",
      "Epoch 283/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0323 - acc: 0.9770 - val_loss: 0.0256 - val_acc: 0.9804\n",
      "Epoch 284/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0323 - acc: 0.9763 - val_loss: 0.0261 - val_acc: 0.9736\n",
      "Epoch 285/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0323 - acc: 0.9766 - val_loss: 0.0246 - val_acc: 0.9812\n",
      "Epoch 286/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0322 - acc: 0.9774 - val_loss: 0.0284 - val_acc: 0.9787\n",
      "Epoch 287/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0315 - acc: 0.9773 - val_loss: 0.0260 - val_acc: 0.9770\n",
      "Epoch 288/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0315 - acc: 0.9772 - val_loss: 0.0261 - val_acc: 0.9753\n",
      "Epoch 289/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0318 - acc: 0.9770 - val_loss: 0.0258 - val_acc: 0.9753\n",
      "Epoch 290/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0320 - acc: 0.9759 - val_loss: 0.0241 - val_acc: 0.9787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0312 - acc: 0.9780 - val_loss: 0.0232 - val_acc: 0.9804\n",
      "Epoch 292/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0320 - acc: 0.9760 - val_loss: 0.0247 - val_acc: 0.9753\n",
      "Epoch 293/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0318 - acc: 0.9776 - val_loss: 0.0276 - val_acc: 0.9736\n",
      "Epoch 294/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0306 - acc: 0.9786 - val_loss: 0.0238 - val_acc: 0.9795\n",
      "Epoch 295/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0318 - acc: 0.9765 - val_loss: 0.0250 - val_acc: 0.9812\n",
      "Epoch 296/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0318 - acc: 0.9760 - val_loss: 0.0259 - val_acc: 0.9770\n",
      "Epoch 297/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0318 - acc: 0.9763 - val_loss: 0.0281 - val_acc: 0.9770\n",
      "Epoch 298/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0309 - acc: 0.9773 - val_loss: 0.0241 - val_acc: 0.9804\n",
      "Epoch 299/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0312 - acc: 0.9768 - val_loss: 0.0262 - val_acc: 0.9821\n",
      "Epoch 300/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0308 - acc: 0.9776 - val_loss: 0.0236 - val_acc: 0.9812\n",
      "Epoch 301/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0311 - acc: 0.9773 - val_loss: 0.0252 - val_acc: 0.9804\n",
      "Epoch 302/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0306 - acc: 0.9775 - val_loss: 0.0228 - val_acc: 0.9812\n",
      "Epoch 303/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0305 - acc: 0.9795 - val_loss: 0.0243 - val_acc: 0.9778\n",
      "Epoch 304/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0308 - acc: 0.9772 - val_loss: 0.0246 - val_acc: 0.9795\n",
      "Epoch 305/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0304 - acc: 0.9777 - val_loss: 0.0237 - val_acc: 0.9821\n",
      "Epoch 306/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0300 - acc: 0.9795 - val_loss: 0.0242 - val_acc: 0.9778\n",
      "Epoch 307/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0302 - acc: 0.9774 - val_loss: 0.0232 - val_acc: 0.9770\n",
      "Epoch 308/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0301 - acc: 0.9790 - val_loss: 0.0232 - val_acc: 0.9821\n",
      "Epoch 309/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0298 - acc: 0.9798 - val_loss: 0.0233 - val_acc: 0.9829\n",
      "Epoch 310/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0294 - acc: 0.9796 - val_loss: 0.0224 - val_acc: 0.9812\n",
      "Epoch 311/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0293 - acc: 0.9784 - val_loss: 0.0214 - val_acc: 0.9804\n",
      "Epoch 312/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0299 - acc: 0.9785 - val_loss: 0.0240 - val_acc: 0.9795\n",
      "Epoch 313/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0297 - acc: 0.9780 - val_loss: 0.0235 - val_acc: 0.9795\n",
      "Epoch 314/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0295 - acc: 0.9786 - val_loss: 0.0224 - val_acc: 0.9838\n",
      "Epoch 315/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0293 - acc: 0.9796 - val_loss: 0.0242 - val_acc: 0.9812\n",
      "Epoch 316/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0298 - acc: 0.9777 - val_loss: 0.0238 - val_acc: 0.9804\n",
      "Epoch 317/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0303 - acc: 0.9777 - val_loss: 0.0226 - val_acc: 0.9812\n",
      "Epoch 318/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0293 - acc: 0.9796 - val_loss: 0.0222 - val_acc: 0.9829\n",
      "Epoch 319/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0287 - acc: 0.9796 - val_loss: 0.0214 - val_acc: 0.9812\n",
      "Epoch 320/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0293 - acc: 0.9778 - val_loss: 0.0234 - val_acc: 0.9829\n",
      "Epoch 321/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0297 - acc: 0.9789 - val_loss: 0.0226 - val_acc: 0.9821\n",
      "Epoch 322/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0291 - acc: 0.9782 - val_loss: 0.0220 - val_acc: 0.9838\n",
      "Epoch 323/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0295 - acc: 0.9788 - val_loss: 0.0228 - val_acc: 0.9787\n",
      "Epoch 324/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0291 - acc: 0.9804 - val_loss: 0.0219 - val_acc: 0.9753\n",
      "Epoch 325/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0288 - acc: 0.9792 - val_loss: 0.0231 - val_acc: 0.9812\n",
      "Epoch 326/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0290 - acc: 0.9786 - val_loss: 0.0233 - val_acc: 0.9829\n",
      "Epoch 327/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0288 - acc: 0.9799 - val_loss: 0.0241 - val_acc: 0.9753\n",
      "Epoch 328/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0289 - acc: 0.9802 - val_loss: 0.0225 - val_acc: 0.9804\n",
      "Epoch 329/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0293 - acc: 0.9781 - val_loss: 0.0227 - val_acc: 0.9804\n",
      "Epoch 330/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0285 - acc: 0.9806 - val_loss: 0.0230 - val_acc: 0.9795\n",
      "Epoch 331/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0287 - acc: 0.9803 - val_loss: 0.0211 - val_acc: 0.9838\n",
      "Epoch 332/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0284 - acc: 0.9798 - val_loss: 0.0224 - val_acc: 0.9821\n",
      "Epoch 333/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0284 - acc: 0.9787 - val_loss: 0.0215 - val_acc: 0.9787\n",
      "Epoch 334/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0285 - acc: 0.9793 - val_loss: 0.0221 - val_acc: 0.9804\n",
      "Epoch 335/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0288 - acc: 0.9796 - val_loss: 0.0232 - val_acc: 0.9778\n",
      "Epoch 336/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0286 - acc: 0.9797 - val_loss: 0.0231 - val_acc: 0.9821\n",
      "Epoch 337/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0280 - acc: 0.9812 - val_loss: 0.0214 - val_acc: 0.9778\n",
      "Epoch 338/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0285 - acc: 0.9805 - val_loss: 0.0227 - val_acc: 0.9778\n",
      "Epoch 339/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0287 - acc: 0.9789 - val_loss: 0.0219 - val_acc: 0.9795\n",
      "Epoch 340/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0285 - acc: 0.9798 - val_loss: 0.0208 - val_acc: 0.9804\n",
      "Epoch 341/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0277 - acc: 0.9801 - val_loss: 0.0226 - val_acc: 0.9804\n",
      "Epoch 342/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0277 - acc: 0.9799 - val_loss: 0.0213 - val_acc: 0.9804\n",
      "Epoch 343/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0277 - acc: 0.9811 - val_loss: 0.0210 - val_acc: 0.9804\n",
      "Epoch 344/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0278 - acc: 0.9801 - val_loss: 0.0230 - val_acc: 0.9787\n",
      "Epoch 345/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0280 - acc: 0.9804 - val_loss: 0.0252 - val_acc: 0.9761\n",
      "Epoch 346/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0279 - acc: 0.9803 - val_loss: 0.0210 - val_acc: 0.9804\n",
      "Epoch 347/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0278 - acc: 0.9824 - val_loss: 0.0208 - val_acc: 0.9812\n",
      "Epoch 348/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0275 - acc: 0.9804 - val_loss: 0.0190 - val_acc: 0.9838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0278 - acc: 0.9807 - val_loss: 0.0205 - val_acc: 0.9821\n",
      "Epoch 350/500\n",
      "22272/22272 [==============================] - 15s 680us/step - loss: 0.0275 - acc: 0.9800 - val_loss: 0.0210 - val_acc: 0.9838\n",
      "Epoch 351/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0274 - acc: 0.9801 - val_loss: 0.0205 - val_acc: 0.9821\n",
      "Epoch 352/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0278 - acc: 0.9804 - val_loss: 0.0208 - val_acc: 0.9770\n",
      "Epoch 353/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0277 - acc: 0.9801 - val_loss: 0.0210 - val_acc: 0.9778\n",
      "Epoch 354/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0277 - acc: 0.9807 - val_loss: 0.0202 - val_acc: 0.9812\n",
      "Epoch 355/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0271 - acc: 0.9808 - val_loss: 0.0199 - val_acc: 0.9821\n",
      "Epoch 356/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0274 - acc: 0.9806 - val_loss: 0.0198 - val_acc: 0.9804\n",
      "Epoch 357/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0271 - acc: 0.9816 - val_loss: 0.0204 - val_acc: 0.9838\n",
      "Epoch 358/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0267 - acc: 0.9819 - val_loss: 0.0199 - val_acc: 0.9829\n",
      "Epoch 359/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0270 - acc: 0.9815 - val_loss: 0.0198 - val_acc: 0.9787\n",
      "Epoch 360/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0268 - acc: 0.9808 - val_loss: 0.0222 - val_acc: 0.9821\n",
      "Epoch 361/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0271 - acc: 0.9820 - val_loss: 0.0212 - val_acc: 0.9804\n",
      "Epoch 362/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0273 - acc: 0.9824 - val_loss: 0.0205 - val_acc: 0.9821\n",
      "Epoch 363/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0267 - acc: 0.9801 - val_loss: 0.0195 - val_acc: 0.9821\n",
      "Epoch 364/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0266 - acc: 0.9814 - val_loss: 0.0186 - val_acc: 0.9829\n",
      "Epoch 365/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0267 - acc: 0.9815 - val_loss: 0.0205 - val_acc: 0.9787\n",
      "Epoch 366/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0266 - acc: 0.9811 - val_loss: 0.0179 - val_acc: 0.9847\n",
      "Epoch 367/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0269 - acc: 0.9814 - val_loss: 0.0232 - val_acc: 0.9761\n",
      "Epoch 368/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0271 - acc: 0.9802 - val_loss: 0.0205 - val_acc: 0.9821\n",
      "Epoch 369/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0264 - acc: 0.9815 - val_loss: 0.0195 - val_acc: 0.9847\n",
      "Epoch 370/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0261 - acc: 0.9811 - val_loss: 0.0205 - val_acc: 0.9847\n",
      "Epoch 371/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0261 - acc: 0.9812 - val_loss: 0.0199 - val_acc: 0.9829\n",
      "Epoch 372/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0264 - acc: 0.9816 - val_loss: 0.0209 - val_acc: 0.9838\n",
      "Epoch 373/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0259 - acc: 0.9813 - val_loss: 0.0198 - val_acc: 0.9812\n",
      "Epoch 374/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0263 - acc: 0.9810 - val_loss: 0.0215 - val_acc: 0.9804\n",
      "Epoch 375/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0264 - acc: 0.9809 - val_loss: 0.0213 - val_acc: 0.9795\n",
      "Epoch 376/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0267 - acc: 0.9805 - val_loss: 0.0200 - val_acc: 0.9804\n",
      "Epoch 377/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0263 - acc: 0.9809 - val_loss: 0.0200 - val_acc: 0.9795\n",
      "Epoch 378/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0262 - acc: 0.9826 - val_loss: 0.0190 - val_acc: 0.9821\n",
      "Epoch 379/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0261 - acc: 0.9815 - val_loss: 0.0200 - val_acc: 0.9812\n",
      "Epoch 380/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0270 - acc: 0.9809 - val_loss: 0.0185 - val_acc: 0.9795\n",
      "Epoch 381/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0257 - acc: 0.9827 - val_loss: 0.0193 - val_acc: 0.9838\n",
      "Epoch 382/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0262 - acc: 0.9817 - val_loss: 0.0210 - val_acc: 0.9821\n",
      "Epoch 383/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0258 - acc: 0.9827 - val_loss: 0.0219 - val_acc: 0.9821\n",
      "Epoch 384/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0256 - acc: 0.9818 - val_loss: 0.0189 - val_acc: 0.9838\n",
      "Epoch 385/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0255 - acc: 0.9833 - val_loss: 0.0188 - val_acc: 0.9847\n",
      "Epoch 386/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0256 - acc: 0.9820 - val_loss: 0.0193 - val_acc: 0.9847\n",
      "Epoch 387/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0255 - acc: 0.9828 - val_loss: 0.0190 - val_acc: 0.9812\n",
      "Epoch 388/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0256 - acc: 0.9822 - val_loss: 0.0185 - val_acc: 0.9821\n",
      "Epoch 389/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0255 - acc: 0.9827 - val_loss: 0.0201 - val_acc: 0.9761\n",
      "Epoch 390/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0255 - acc: 0.9821 - val_loss: 0.0200 - val_acc: 0.9804\n",
      "Epoch 391/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0253 - acc: 0.9842 - val_loss: 0.0207 - val_acc: 0.9804\n",
      "Epoch 392/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0256 - acc: 0.9819 - val_loss: 0.0183 - val_acc: 0.9778\n",
      "Epoch 393/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0252 - acc: 0.9829 - val_loss: 0.0185 - val_acc: 0.9821\n",
      "Epoch 394/500\n",
      "22272/22272 [==============================] - 16s 721us/step - loss: 0.0257 - acc: 0.9838 - val_loss: 0.0188 - val_acc: 0.9829\n",
      "Epoch 395/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0254 - acc: 0.9823 - val_loss: 0.0181 - val_acc: 0.9838\n",
      "Epoch 396/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0254 - acc: 0.9829 - val_loss: 0.0174 - val_acc: 0.9812\n",
      "Epoch 397/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0249 - acc: 0.9851 - val_loss: 0.0205 - val_acc: 0.9821\n",
      "Epoch 398/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0253 - acc: 0.9823 - val_loss: 0.0196 - val_acc: 0.9829\n",
      "Epoch 399/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0248 - acc: 0.9823 - val_loss: 0.0209 - val_acc: 0.9804\n",
      "Epoch 400/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0249 - acc: 0.9828 - val_loss: 0.0184 - val_acc: 0.9812\n",
      "Epoch 401/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0251 - acc: 0.9833 - val_loss: 0.0219 - val_acc: 0.9787\n",
      "Epoch 402/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0256 - acc: 0.9820 - val_loss: 0.0178 - val_acc: 0.9864\n",
      "Epoch 403/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0249 - acc: 0.9823 - val_loss: 0.0185 - val_acc: 0.9847\n",
      "Epoch 404/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0251 - acc: 0.9828 - val_loss: 0.0188 - val_acc: 0.9829\n",
      "Epoch 405/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0253 - acc: 0.9807 - val_loss: 0.0191 - val_acc: 0.9821\n",
      "Epoch 406/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0249 - acc: 0.9833 - val_loss: 0.0192 - val_acc: 0.9812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 407/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0255 - acc: 0.9821 - val_loss: 0.0175 - val_acc: 0.9829\n",
      "Epoch 408/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0249 - acc: 0.9824 - val_loss: 0.0193 - val_acc: 0.9795\n",
      "Epoch 409/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0253 - acc: 0.9820 - val_loss: 0.0186 - val_acc: 0.9795\n",
      "Epoch 410/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0248 - acc: 0.9835 - val_loss: 0.0189 - val_acc: 0.9829\n",
      "Epoch 411/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0244 - acc: 0.9833 - val_loss: 0.0203 - val_acc: 0.9838\n",
      "Epoch 412/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0250 - acc: 0.9824 - val_loss: 0.0208 - val_acc: 0.9838\n",
      "Epoch 413/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0249 - acc: 0.9825 - val_loss: 0.0201 - val_acc: 0.9829\n",
      "Epoch 414/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0248 - acc: 0.9830 - val_loss: 0.0186 - val_acc: 0.9829\n",
      "Epoch 415/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0248 - acc: 0.9833 - val_loss: 0.0183 - val_acc: 0.9829\n",
      "Epoch 416/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0252 - acc: 0.9829 - val_loss: 0.0173 - val_acc: 0.9855\n",
      "Epoch 417/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0243 - acc: 0.9834 - val_loss: 0.0175 - val_acc: 0.9821\n",
      "Epoch 418/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0245 - acc: 0.9823 - val_loss: 0.0182 - val_acc: 0.9804\n",
      "Epoch 419/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0247 - acc: 0.9827 - val_loss: 0.0191 - val_acc: 0.9847\n",
      "Epoch 420/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0248 - acc: 0.9822 - val_loss: 0.0198 - val_acc: 0.9744\n",
      "Epoch 421/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0250 - acc: 0.9830 - val_loss: 0.0177 - val_acc: 0.9795\n",
      "Epoch 422/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0237 - acc: 0.9832 - val_loss: 0.0176 - val_acc: 0.9829\n",
      "Epoch 423/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0239 - acc: 0.9834 - val_loss: 0.0169 - val_acc: 0.9847\n",
      "Epoch 424/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0241 - acc: 0.9838 - val_loss: 0.0210 - val_acc: 0.9829\n",
      "Epoch 425/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0242 - acc: 0.9833 - val_loss: 0.0175 - val_acc: 0.9847\n",
      "Epoch 426/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0245 - acc: 0.9823 - val_loss: 0.0176 - val_acc: 0.9829\n",
      "Epoch 427/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0240 - acc: 0.9846 - val_loss: 0.0161 - val_acc: 0.9847\n",
      "Epoch 428/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0241 - acc: 0.9837 - val_loss: 0.0189 - val_acc: 0.9812\n",
      "Epoch 429/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0238 - acc: 0.9842 - val_loss: 0.0176 - val_acc: 0.9838\n",
      "Epoch 430/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0240 - acc: 0.9845 - val_loss: 0.0182 - val_acc: 0.9847\n",
      "Epoch 431/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0236 - acc: 0.9834 - val_loss: 0.0197 - val_acc: 0.9812\n",
      "Epoch 432/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0242 - acc: 0.9842 - val_loss: 0.0181 - val_acc: 0.9847\n",
      "Epoch 433/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0239 - acc: 0.9845 - val_loss: 0.0186 - val_acc: 0.9787\n",
      "Epoch 434/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0244 - acc: 0.9828 - val_loss: 0.0177 - val_acc: 0.9855\n",
      "Epoch 435/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0236 - acc: 0.9829 - val_loss: 0.0180 - val_acc: 0.9864\n",
      "Epoch 436/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0243 - acc: 0.9833 - val_loss: 0.0189 - val_acc: 0.9829\n",
      "Epoch 437/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0239 - acc: 0.9832 - val_loss: 0.0166 - val_acc: 0.9829\n",
      "Epoch 438/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0241 - acc: 0.9823 - val_loss: 0.0179 - val_acc: 0.9847\n",
      "Epoch 439/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0235 - acc: 0.9840 - val_loss: 0.0160 - val_acc: 0.9838\n",
      "Epoch 440/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0241 - acc: 0.9828 - val_loss: 0.0174 - val_acc: 0.9821\n",
      "Epoch 441/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0238 - acc: 0.9825 - val_loss: 0.0177 - val_acc: 0.9838\n",
      "Epoch 442/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0233 - acc: 0.9841 - val_loss: 0.0167 - val_acc: 0.9829\n",
      "Epoch 443/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0238 - acc: 0.9837 - val_loss: 0.0189 - val_acc: 0.9795\n",
      "Epoch 444/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0240 - acc: 0.9832 - val_loss: 0.0174 - val_acc: 0.9821\n",
      "Epoch 445/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0233 - acc: 0.9839 - val_loss: 0.0185 - val_acc: 0.9821\n",
      "Epoch 446/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0236 - acc: 0.9838 - val_loss: 0.0208 - val_acc: 0.9812\n",
      "Epoch 447/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0239 - acc: 0.9828 - val_loss: 0.0182 - val_acc: 0.9855\n",
      "Epoch 448/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0233 - acc: 0.9852 - val_loss: 0.0186 - val_acc: 0.9855\n",
      "Epoch 449/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0235 - acc: 0.9853 - val_loss: 0.0182 - val_acc: 0.9872\n",
      "Epoch 450/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0236 - acc: 0.9833 - val_loss: 0.0178 - val_acc: 0.9847\n",
      "Epoch 451/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0236 - acc: 0.9835 - val_loss: 0.0198 - val_acc: 0.9795\n",
      "Epoch 452/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0236 - acc: 0.9835 - val_loss: 0.0190 - val_acc: 0.9838\n",
      "Epoch 453/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0234 - acc: 0.9837 - val_loss: 0.0173 - val_acc: 0.9838\n",
      "Epoch 454/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0231 - acc: 0.9850 - val_loss: 0.0160 - val_acc: 0.9804\n",
      "Epoch 455/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0230 - acc: 0.9845 - val_loss: 0.0186 - val_acc: 0.9838\n",
      "Epoch 456/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0234 - acc: 0.9841 - val_loss: 0.0168 - val_acc: 0.9855\n",
      "Epoch 457/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0233 - acc: 0.9837 - val_loss: 0.0178 - val_acc: 0.9821\n",
      "Epoch 458/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0232 - acc: 0.9842 - val_loss: 0.0179 - val_acc: 0.9821\n",
      "Epoch 459/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0235 - acc: 0.9836 - val_loss: 0.0178 - val_acc: 0.9804\n",
      "Epoch 460/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0234 - acc: 0.9833 - val_loss: 0.0165 - val_acc: 0.9847\n",
      "Epoch 461/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0230 - acc: 0.9840 - val_loss: 0.0184 - val_acc: 0.9787\n",
      "Epoch 462/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0232 - acc: 0.9849 - val_loss: 0.0166 - val_acc: 0.9889\n",
      "Epoch 463/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0231 - acc: 0.9842 - val_loss: 0.0189 - val_acc: 0.9847\n",
      "Epoch 464/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0235 - acc: 0.9839 - val_loss: 0.0167 - val_acc: 0.9838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0230 - acc: 0.9857 - val_loss: 0.0164 - val_acc: 0.9829\n",
      "Epoch 466/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0227 - acc: 0.9844 - val_loss: 0.0160 - val_acc: 0.9864\n",
      "Epoch 467/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0230 - acc: 0.9847 - val_loss: 0.0170 - val_acc: 0.9812\n",
      "Epoch 468/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0228 - acc: 0.9848 - val_loss: 0.0163 - val_acc: 0.9847\n",
      "Epoch 469/500\n",
      "22272/22272 [==============================] - 15s 680us/step - loss: 0.0230 - acc: 0.9853 - val_loss: 0.0167 - val_acc: 0.9864\n",
      "Epoch 470/500\n",
      "22272/22272 [==============================] - 15s 682us/step - loss: 0.0231 - acc: 0.9838 - val_loss: 0.0163 - val_acc: 0.9821\n",
      "Epoch 471/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0227 - acc: 0.9846 - val_loss: 0.0177 - val_acc: 0.9812\n",
      "Epoch 472/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0224 - acc: 0.9845 - val_loss: 0.0156 - val_acc: 0.9864\n",
      "Epoch 473/500\n",
      "22272/22272 [==============================] - 15s 683us/step - loss: 0.0224 - acc: 0.9857 - val_loss: 0.0161 - val_acc: 0.9855\n",
      "Epoch 474/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0228 - acc: 0.9830 - val_loss: 0.0182 - val_acc: 0.9821\n",
      "Epoch 475/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0230 - acc: 0.9841 - val_loss: 0.0194 - val_acc: 0.9812\n",
      "Epoch 476/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0230 - acc: 0.9850 - val_loss: 0.0167 - val_acc: 0.9855\n",
      "Epoch 477/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0228 - acc: 0.9855 - val_loss: 0.0173 - val_acc: 0.9881\n",
      "Epoch 478/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0220 - acc: 0.9855 - val_loss: 0.0156 - val_acc: 0.9898\n",
      "Epoch 479/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0224 - acc: 0.9846 - val_loss: 0.0167 - val_acc: 0.9864\n",
      "Epoch 480/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0222 - acc: 0.9856 - val_loss: 0.0163 - val_acc: 0.9847\n",
      "Epoch 481/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0222 - acc: 0.9845 - val_loss: 0.0178 - val_acc: 0.9821\n",
      "Epoch 482/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0227 - acc: 0.9844 - val_loss: 0.0162 - val_acc: 0.9838\n",
      "Epoch 483/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0226 - acc: 0.9850 - val_loss: 0.0170 - val_acc: 0.9838\n",
      "Epoch 484/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0224 - acc: 0.9853 - val_loss: 0.0163 - val_acc: 0.9864\n",
      "Epoch 485/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0226 - acc: 0.9833 - val_loss: 0.0160 - val_acc: 0.9881\n",
      "Epoch 486/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0226 - acc: 0.9839 - val_loss: 0.0169 - val_acc: 0.9864\n",
      "Epoch 487/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0222 - acc: 0.9867 - val_loss: 0.0170 - val_acc: 0.9838\n",
      "Epoch 488/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0224 - acc: 0.9858 - val_loss: 0.0169 - val_acc: 0.9812\n",
      "Epoch 489/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0223 - acc: 0.9848 - val_loss: 0.0182 - val_acc: 0.9838\n",
      "Epoch 490/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0225 - acc: 0.9841 - val_loss: 0.0162 - val_acc: 0.9812\n",
      "Epoch 491/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0225 - acc: 0.9839 - val_loss: 0.0176 - val_acc: 0.9872\n",
      "Epoch 492/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0223 - acc: 0.9852 - val_loss: 0.0156 - val_acc: 0.9864\n",
      "Epoch 493/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0222 - acc: 0.9853 - val_loss: 0.0183 - val_acc: 0.9847\n",
      "Epoch 494/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0227 - acc: 0.9846 - val_loss: 0.0174 - val_acc: 0.9804\n",
      "Epoch 495/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0225 - acc: 0.9854 - val_loss: 0.0166 - val_acc: 0.9864\n",
      "Epoch 496/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0220 - acc: 0.9850 - val_loss: 0.0170 - val_acc: 0.9812\n",
      "Epoch 497/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0219 - acc: 0.9852 - val_loss: 0.0180 - val_acc: 0.9821\n",
      "Epoch 498/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0220 - acc: 0.9848 - val_loss: 0.0163 - val_acc: 0.9821\n",
      "Epoch 499/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0218 - acc: 0.9855 - val_loss: 0.0161 - val_acc: 0.9855\n",
      "Epoch 500/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0223 - acc: 0.9851 - val_loss: 0.0164 - val_acc: 0.9864\n"
     ]
    }
   ],
   "source": [
    "##Fitting the GRU to the Training set\n",
    "hist4=model4.fit(X_train, y_train, batch_size=300, epochs=500, validation_split=0.05, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22272 samples, validate on 1173 samples\n",
      "Epoch 1/500\n",
      "22272/22272 [==============================] - 20s 890us/step - loss: 0.2397 - acc: 0.7434 - val_loss: 0.2234 - val_acc: 0.7298\n",
      "Epoch 2/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.2128 - acc: 0.7503 - val_loss: 0.2147 - val_acc: 0.7263\n",
      "Epoch 3/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.2037 - acc: 0.7454 - val_loss: 0.2059 - val_acc: 0.7340\n",
      "Epoch 4/500\n",
      "22272/22272 [==============================] - 16s 714us/step - loss: 0.2000 - acc: 0.7506 - val_loss: 0.2045 - val_acc: 0.7315\n",
      "Epoch 5/500\n",
      "22272/22272 [==============================] - 16s 711us/step - loss: 0.1967 - acc: 0.7528 - val_loss: 0.2035 - val_acc: 0.7425\n",
      "Epoch 6/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1955 - acc: 0.7534 - val_loss: 0.1972 - val_acc: 0.7460\n",
      "Epoch 7/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.1934 - acc: 0.7526 - val_loss: 0.2041 - val_acc: 0.7425\n",
      "Epoch 8/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1895 - acc: 0.7539 - val_loss: 0.1943 - val_acc: 0.7417\n",
      "Epoch 9/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1848 - acc: 0.7564 - val_loss: 0.1935 - val_acc: 0.7374\n",
      "Epoch 10/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.1798 - acc: 0.7611 - val_loss: 0.1847 - val_acc: 0.7553\n",
      "Epoch 11/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1767 - acc: 0.7688 - val_loss: 0.1791 - val_acc: 0.7579\n",
      "Epoch 12/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.1738 - acc: 0.7743 - val_loss: 0.1806 - val_acc: 0.7587\n",
      "Epoch 13/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1713 - acc: 0.7800 - val_loss: 0.1775 - val_acc: 0.7596\n",
      "Epoch 14/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.1700 - acc: 0.7823 - val_loss: 0.1739 - val_acc: 0.7681\n",
      "Epoch 15/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.1703 - acc: 0.7833 - val_loss: 0.1785 - val_acc: 0.7724\n",
      "Epoch 16/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.1684 - acc: 0.7868 - val_loss: 0.1718 - val_acc: 0.7673\n",
      "Epoch 17/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.1644 - acc: 0.7874 - val_loss: 0.1739 - val_acc: 0.7630\n",
      "Epoch 18/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.1640 - acc: 0.7873 - val_loss: 0.1709 - val_acc: 0.7715\n",
      "Epoch 19/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.1605 - acc: 0.7947 - val_loss: 0.1798 - val_acc: 0.7698\n",
      "Epoch 20/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.1613 - acc: 0.7932 - val_loss: 0.1666 - val_acc: 0.7579\n",
      "Epoch 21/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1588 - acc: 0.7937 - val_loss: 0.1642 - val_acc: 0.7801\n",
      "Epoch 22/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.1572 - acc: 0.7959 - val_loss: 0.1692 - val_acc: 0.7596\n",
      "Epoch 23/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.1557 - acc: 0.8008 - val_loss: 0.1655 - val_acc: 0.7886\n",
      "Epoch 24/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.1545 - acc: 0.8014 - val_loss: 0.1687 - val_acc: 0.7801\n",
      "Epoch 25/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.1555 - acc: 0.8002 - val_loss: 0.1637 - val_acc: 0.7911\n",
      "Epoch 26/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.1528 - acc: 0.8032 - val_loss: 0.1591 - val_acc: 0.7826\n",
      "Epoch 27/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.1511 - acc: 0.8050 - val_loss: 0.1554 - val_acc: 0.7894\n",
      "Epoch 28/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.1500 - acc: 0.8077 - val_loss: 0.1553 - val_acc: 0.7928\n",
      "Epoch 29/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.1472 - acc: 0.8114 - val_loss: 0.1548 - val_acc: 0.7835\n",
      "Epoch 30/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.1476 - acc: 0.8110 - val_loss: 0.1595 - val_acc: 0.7954\n",
      "Epoch 31/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.1461 - acc: 0.8127 - val_loss: 0.1592 - val_acc: 0.7818\n",
      "Epoch 32/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1452 - acc: 0.8162 - val_loss: 0.1499 - val_acc: 0.8082\n",
      "Epoch 33/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.1432 - acc: 0.8170 - val_loss: 0.1465 - val_acc: 0.8022\n",
      "Epoch 34/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.1436 - acc: 0.8150 - val_loss: 0.1527 - val_acc: 0.7894\n",
      "Epoch 35/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.1416 - acc: 0.8209 - val_loss: 0.1477 - val_acc: 0.7962\n",
      "Epoch 36/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1426 - acc: 0.8174 - val_loss: 0.1481 - val_acc: 0.8056\n",
      "Epoch 37/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.1399 - acc: 0.8212 - val_loss: 0.1435 - val_acc: 0.8252\n",
      "Epoch 38/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.1391 - acc: 0.8265 - val_loss: 0.1423 - val_acc: 0.8082\n",
      "Epoch 39/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1395 - acc: 0.8238 - val_loss: 0.1432 - val_acc: 0.8227\n",
      "Epoch 40/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1371 - acc: 0.8252 - val_loss: 0.1415 - val_acc: 0.8142\n",
      "Epoch 41/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.1361 - acc: 0.8306 - val_loss: 0.1395 - val_acc: 0.8278\n",
      "Epoch 42/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.1346 - acc: 0.8319 - val_loss: 0.1420 - val_acc: 0.8159\n",
      "Epoch 43/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.1347 - acc: 0.8295 - val_loss: 0.1435 - val_acc: 0.8184\n",
      "Epoch 44/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.1340 - acc: 0.8320 - val_loss: 0.1365 - val_acc: 0.8269\n",
      "Epoch 45/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.1345 - acc: 0.8330 - val_loss: 0.1393 - val_acc: 0.8201\n",
      "Epoch 46/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.1331 - acc: 0.8345 - val_loss: 0.1391 - val_acc: 0.8252\n",
      "Epoch 47/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.1317 - acc: 0.8376 - val_loss: 0.1362 - val_acc: 0.8218\n",
      "Epoch 48/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.1306 - acc: 0.8364 - val_loss: 0.1376 - val_acc: 0.8210\n",
      "Epoch 49/500\n",
      "22272/22272 [==============================] - 16s 715us/step - loss: 0.1284 - acc: 0.8399 - val_loss: 0.1306 - val_acc: 0.8346\n",
      "Epoch 50/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.1265 - acc: 0.8431 - val_loss: 0.1326 - val_acc: 0.8363\n",
      "Epoch 51/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.1284 - acc: 0.8417 - val_loss: 0.1300 - val_acc: 0.8355\n",
      "Epoch 52/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.1247 - acc: 0.8465 - val_loss: 0.1276 - val_acc: 0.8397\n",
      "Epoch 53/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.1256 - acc: 0.8427 - val_loss: 0.1361 - val_acc: 0.8227\n",
      "Epoch 54/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.1240 - acc: 0.8446 - val_loss: 0.1294 - val_acc: 0.8431\n",
      "Epoch 55/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.1230 - acc: 0.8477 - val_loss: 0.1265 - val_acc: 0.8491\n",
      "Epoch 56/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.1224 - acc: 0.8483 - val_loss: 0.1223 - val_acc: 0.8397\n",
      "Epoch 57/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.1195 - acc: 0.8539 - val_loss: 0.1263 - val_acc: 0.8303\n",
      "Epoch 58/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.1186 - acc: 0.8552 - val_loss: 0.1229 - val_acc: 0.8406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.1182 - acc: 0.8542 - val_loss: 0.1346 - val_acc: 0.8372\n",
      "Epoch 60/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.1166 - acc: 0.8576 - val_loss: 0.1200 - val_acc: 0.8474\n",
      "Epoch 61/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.1182 - acc: 0.8566 - val_loss: 0.1259 - val_acc: 0.8414\n",
      "Epoch 62/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.1165 - acc: 0.8598 - val_loss: 0.1199 - val_acc: 0.8465\n",
      "Epoch 63/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.1144 - acc: 0.8588 - val_loss: 0.1123 - val_acc: 0.8576\n",
      "Epoch 64/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.1136 - acc: 0.8622 - val_loss: 0.1132 - val_acc: 0.8500\n",
      "Epoch 65/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.1128 - acc: 0.8632 - val_loss: 0.1118 - val_acc: 0.8653\n",
      "Epoch 66/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.1112 - acc: 0.8650 - val_loss: 0.1133 - val_acc: 0.8559\n",
      "Epoch 67/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.1085 - acc: 0.8698 - val_loss: 0.1102 - val_acc: 0.8670\n",
      "Epoch 68/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.1085 - acc: 0.8682 - val_loss: 0.1086 - val_acc: 0.8593\n",
      "Epoch 69/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.1076 - acc: 0.8724 - val_loss: 0.1079 - val_acc: 0.8610\n",
      "Epoch 70/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.1067 - acc: 0.8723 - val_loss: 0.1043 - val_acc: 0.8679\n",
      "Epoch 71/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.1044 - acc: 0.8769 - val_loss: 0.1072 - val_acc: 0.8687\n",
      "Epoch 72/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.1048 - acc: 0.8749 - val_loss: 0.1034 - val_acc: 0.8755\n",
      "Epoch 73/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.1039 - acc: 0.8761 - val_loss: 0.1038 - val_acc: 0.8696\n",
      "Epoch 74/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1028 - acc: 0.8782 - val_loss: 0.1059 - val_acc: 0.8636\n",
      "Epoch 75/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.1012 - acc: 0.8792 - val_loss: 0.1030 - val_acc: 0.8747\n",
      "Epoch 76/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.1009 - acc: 0.8832 - val_loss: 0.1008 - val_acc: 0.8764\n",
      "Epoch 77/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1000 - acc: 0.8803 - val_loss: 0.0995 - val_acc: 0.8849\n",
      "Epoch 78/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0993 - acc: 0.8838 - val_loss: 0.1017 - val_acc: 0.8781\n",
      "Epoch 79/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0982 - acc: 0.8838 - val_loss: 0.0988 - val_acc: 0.8883\n",
      "Epoch 80/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0970 - acc: 0.8868 - val_loss: 0.0967 - val_acc: 0.8841\n",
      "Epoch 81/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0959 - acc: 0.8877 - val_loss: 0.0947 - val_acc: 0.8875\n",
      "Epoch 82/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0961 - acc: 0.8876 - val_loss: 0.0992 - val_acc: 0.8892\n",
      "Epoch 83/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0949 - acc: 0.8904 - val_loss: 0.1010 - val_acc: 0.8832\n",
      "Epoch 84/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0940 - acc: 0.8933 - val_loss: 0.0930 - val_acc: 0.8832\n",
      "Epoch 85/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0927 - acc: 0.8903 - val_loss: 0.0946 - val_acc: 0.8892\n",
      "Epoch 86/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0918 - acc: 0.8939 - val_loss: 0.0908 - val_acc: 0.8875\n",
      "Epoch 87/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0920 - acc: 0.8937 - val_loss: 0.0858 - val_acc: 0.8917\n",
      "Epoch 88/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0913 - acc: 0.8922 - val_loss: 0.0900 - val_acc: 0.8900\n",
      "Epoch 89/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0907 - acc: 0.8970 - val_loss: 0.0841 - val_acc: 0.9079\n",
      "Epoch 90/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0886 - acc: 0.8986 - val_loss: 0.0882 - val_acc: 0.8986\n",
      "Epoch 91/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0888 - acc: 0.8995 - val_loss: 0.0860 - val_acc: 0.9011\n",
      "Epoch 92/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0873 - acc: 0.9012 - val_loss: 0.0834 - val_acc: 0.9054\n",
      "Epoch 93/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0862 - acc: 0.9013 - val_loss: 0.0870 - val_acc: 0.9037\n",
      "Epoch 94/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0859 - acc: 0.9043 - val_loss: 0.0824 - val_acc: 0.8883\n",
      "Epoch 95/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0847 - acc: 0.9037 - val_loss: 0.0843 - val_acc: 0.9045\n",
      "Epoch 96/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0843 - acc: 0.9062 - val_loss: 0.0803 - val_acc: 0.9045\n",
      "Epoch 97/500\n",
      "22272/22272 [==============================] - 16s 718us/step - loss: 0.0844 - acc: 0.9066 - val_loss: 0.0824 - val_acc: 0.9088\n",
      "Epoch 98/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0829 - acc: 0.9056 - val_loss: 0.0765 - val_acc: 0.9139\n",
      "Epoch 99/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0836 - acc: 0.9056 - val_loss: 0.0759 - val_acc: 0.9147\n",
      "Epoch 100/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0819 - acc: 0.9083 - val_loss: 0.0833 - val_acc: 0.8986\n",
      "Epoch 101/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0795 - acc: 0.9118 - val_loss: 0.0749 - val_acc: 0.9207\n",
      "Epoch 102/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0804 - acc: 0.9106 - val_loss: 0.0822 - val_acc: 0.9122\n",
      "Epoch 103/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0794 - acc: 0.9120 - val_loss: 0.0766 - val_acc: 0.9199\n",
      "Epoch 104/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0788 - acc: 0.9120 - val_loss: 0.0720 - val_acc: 0.9207\n",
      "Epoch 105/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0779 - acc: 0.9118 - val_loss: 0.0720 - val_acc: 0.9207\n",
      "Epoch 106/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0780 - acc: 0.9148 - val_loss: 0.0707 - val_acc: 0.9250\n",
      "Epoch 107/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0772 - acc: 0.9162 - val_loss: 0.0741 - val_acc: 0.9199\n",
      "Epoch 108/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0759 - acc: 0.9167 - val_loss: 0.0721 - val_acc: 0.9190\n",
      "Epoch 109/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0757 - acc: 0.9177 - val_loss: 0.0709 - val_acc: 0.9207\n",
      "Epoch 110/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0754 - acc: 0.9173 - val_loss: 0.0757 - val_acc: 0.9165\n",
      "Epoch 111/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0759 - acc: 0.9181 - val_loss: 0.0719 - val_acc: 0.9250\n",
      "Epoch 112/500\n",
      "22272/22272 [==============================] - 16s 711us/step - loss: 0.0739 - acc: 0.9194 - val_loss: 0.0697 - val_acc: 0.9258\n",
      "Epoch 113/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0728 - acc: 0.9211 - val_loss: 0.0687 - val_acc: 0.9258\n",
      "Epoch 114/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0737 - acc: 0.9225 - val_loss: 0.0669 - val_acc: 0.9309\n",
      "Epoch 115/500\n",
      "22272/22272 [==============================] - 16s 710us/step - loss: 0.0731 - acc: 0.9206 - val_loss: 0.0703 - val_acc: 0.9216\n",
      "Epoch 116/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0729 - acc: 0.9195 - val_loss: 0.0704 - val_acc: 0.9241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0715 - acc: 0.9234 - val_loss: 0.0691 - val_acc: 0.9267\n",
      "Epoch 118/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0708 - acc: 0.9246 - val_loss: 0.0710 - val_acc: 0.9284\n",
      "Epoch 119/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0692 - acc: 0.9299 - val_loss: 0.0674 - val_acc: 0.9335\n",
      "Epoch 120/500\n",
      "22272/22272 [==============================] - 16s 710us/step - loss: 0.0696 - acc: 0.9282 - val_loss: 0.0644 - val_acc: 0.9352\n",
      "Epoch 121/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0689 - acc: 0.9267 - val_loss: 0.0624 - val_acc: 0.9361\n",
      "Epoch 122/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0688 - acc: 0.9279 - val_loss: 0.0684 - val_acc: 0.9361\n",
      "Epoch 123/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0691 - acc: 0.9270 - val_loss: 0.0646 - val_acc: 0.9369\n",
      "Epoch 124/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0666 - acc: 0.9312 - val_loss: 0.0664 - val_acc: 0.9241\n",
      "Epoch 125/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0673 - acc: 0.9280 - val_loss: 0.0625 - val_acc: 0.9344\n",
      "Epoch 126/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0673 - acc: 0.9312 - val_loss: 0.0623 - val_acc: 0.9352\n",
      "Epoch 127/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0661 - acc: 0.9297 - val_loss: 0.0620 - val_acc: 0.9344\n",
      "Epoch 128/500\n",
      "22272/22272 [==============================] - 16s 727us/step - loss: 0.0662 - acc: 0.9320 - val_loss: 0.0627 - val_acc: 0.9327\n",
      "Epoch 129/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0658 - acc: 0.9326 - val_loss: 0.0603 - val_acc: 0.9395\n",
      "Epoch 130/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0647 - acc: 0.9322 - val_loss: 0.0648 - val_acc: 0.9284\n",
      "Epoch 131/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0640 - acc: 0.9339 - val_loss: 0.0636 - val_acc: 0.9352\n",
      "Epoch 132/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0647 - acc: 0.9347 - val_loss: 0.0609 - val_acc: 0.9301\n",
      "Epoch 133/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0629 - acc: 0.9344 - val_loss: 0.0589 - val_acc: 0.9412\n",
      "Epoch 134/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0634 - acc: 0.9366 - val_loss: 0.0613 - val_acc: 0.9454\n",
      "Epoch 135/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0633 - acc: 0.9357 - val_loss: 0.0576 - val_acc: 0.9369\n",
      "Epoch 136/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0618 - acc: 0.9377 - val_loss: 0.0568 - val_acc: 0.9446\n",
      "Epoch 137/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0612 - acc: 0.9401 - val_loss: 0.0584 - val_acc: 0.9361\n",
      "Epoch 138/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0613 - acc: 0.9384 - val_loss: 0.0596 - val_acc: 0.9403\n",
      "Epoch 139/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0610 - acc: 0.9393 - val_loss: 0.0562 - val_acc: 0.9429\n",
      "Epoch 140/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0613 - acc: 0.9384 - val_loss: 0.0561 - val_acc: 0.9420\n",
      "Epoch 141/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0603 - acc: 0.9421 - val_loss: 0.0571 - val_acc: 0.9395\n",
      "Epoch 142/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0592 - acc: 0.9415 - val_loss: 0.0554 - val_acc: 0.9437\n",
      "Epoch 143/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0591 - acc: 0.9407 - val_loss: 0.0531 - val_acc: 0.9531\n",
      "Epoch 144/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0590 - acc: 0.9425 - val_loss: 0.0555 - val_acc: 0.9446\n",
      "Epoch 145/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0590 - acc: 0.9418 - val_loss: 0.0570 - val_acc: 0.9335\n",
      "Epoch 146/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0582 - acc: 0.9416 - val_loss: 0.0595 - val_acc: 0.9378\n",
      "Epoch 147/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0587 - acc: 0.9434 - val_loss: 0.0548 - val_acc: 0.9403\n",
      "Epoch 148/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0579 - acc: 0.9443 - val_loss: 0.0559 - val_acc: 0.9412\n",
      "Epoch 149/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0575 - acc: 0.9452 - val_loss: 0.0540 - val_acc: 0.9506\n",
      "Epoch 150/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0570 - acc: 0.9462 - val_loss: 0.0520 - val_acc: 0.9471\n",
      "Epoch 151/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0561 - acc: 0.9461 - val_loss: 0.0543 - val_acc: 0.9378\n",
      "Epoch 152/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0555 - acc: 0.9485 - val_loss: 0.0547 - val_acc: 0.9471\n",
      "Epoch 153/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0558 - acc: 0.9470 - val_loss: 0.0508 - val_acc: 0.9506\n",
      "Epoch 154/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0554 - acc: 0.9480 - val_loss: 0.0568 - val_acc: 0.9395\n",
      "Epoch 155/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0554 - acc: 0.9465 - val_loss: 0.0494 - val_acc: 0.9506\n",
      "Epoch 156/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0557 - acc: 0.9469 - val_loss: 0.0513 - val_acc: 0.9471\n",
      "Epoch 157/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0542 - acc: 0.9498 - val_loss: 0.0490 - val_acc: 0.9523\n",
      "Epoch 158/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0541 - acc: 0.9487 - val_loss: 0.0515 - val_acc: 0.9480\n",
      "Epoch 159/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0540 - acc: 0.9498 - val_loss: 0.0503 - val_acc: 0.9531\n",
      "Epoch 160/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0538 - acc: 0.9524 - val_loss: 0.0500 - val_acc: 0.9480\n",
      "Epoch 161/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0529 - acc: 0.9505 - val_loss: 0.0518 - val_acc: 0.9557\n",
      "Epoch 162/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0532 - acc: 0.9495 - val_loss: 0.0495 - val_acc: 0.9514\n",
      "Epoch 163/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0521 - acc: 0.9497 - val_loss: 0.0490 - val_acc: 0.9548\n",
      "Epoch 164/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0522 - acc: 0.9519 - val_loss: 0.0490 - val_acc: 0.9531\n",
      "Epoch 165/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0515 - acc: 0.9529 - val_loss: 0.0469 - val_acc: 0.9531\n",
      "Epoch 166/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0508 - acc: 0.9535 - val_loss: 0.0463 - val_acc: 0.9599\n",
      "Epoch 167/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0505 - acc: 0.9548 - val_loss: 0.0495 - val_acc: 0.9497\n",
      "Epoch 168/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.0519 - acc: 0.9520 - val_loss: 0.0469 - val_acc: 0.9514\n",
      "Epoch 169/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0509 - acc: 0.9523 - val_loss: 0.0458 - val_acc: 0.9574\n",
      "Epoch 170/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0491 - acc: 0.9569 - val_loss: 0.0456 - val_acc: 0.9531\n",
      "Epoch 171/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0496 - acc: 0.9550 - val_loss: 0.0469 - val_acc: 0.9582\n",
      "Epoch 172/500\n",
      "22272/22272 [==============================] - 16s 713us/step - loss: 0.0501 - acc: 0.9547 - val_loss: 0.0455 - val_acc: 0.9591\n",
      "Epoch 173/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0488 - acc: 0.9560 - val_loss: 0.0447 - val_acc: 0.9633\n",
      "Epoch 174/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0493 - acc: 0.9579 - val_loss: 0.0479 - val_acc: 0.9582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0492 - acc: 0.9571 - val_loss: 0.0445 - val_acc: 0.9574\n",
      "Epoch 176/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0487 - acc: 0.9573 - val_loss: 0.0444 - val_acc: 0.9582\n",
      "Epoch 177/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0478 - acc: 0.9588 - val_loss: 0.0443 - val_acc: 0.9582\n",
      "Epoch 178/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0478 - acc: 0.9582 - val_loss: 0.0439 - val_acc: 0.9642\n",
      "Epoch 179/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0478 - acc: 0.9576 - val_loss: 0.0408 - val_acc: 0.9676\n",
      "Epoch 180/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0480 - acc: 0.9572 - val_loss: 0.0554 - val_acc: 0.9420\n",
      "Epoch 181/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0481 - acc: 0.9572 - val_loss: 0.0458 - val_acc: 0.9642\n",
      "Epoch 182/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0468 - acc: 0.9609 - val_loss: 0.0463 - val_acc: 0.9514\n",
      "Epoch 183/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0483 - acc: 0.9569 - val_loss: 0.0427 - val_acc: 0.9582\n",
      "Epoch 184/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0461 - acc: 0.9600 - val_loss: 0.0432 - val_acc: 0.9557\n",
      "Epoch 185/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0473 - acc: 0.9588 - val_loss: 0.0407 - val_acc: 0.9659\n",
      "Epoch 186/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0466 - acc: 0.9591 - val_loss: 0.0415 - val_acc: 0.9633\n",
      "Epoch 187/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0461 - acc: 0.9594 - val_loss: 0.0418 - val_acc: 0.9633\n",
      "Epoch 188/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0454 - acc: 0.9612 - val_loss: 0.0404 - val_acc: 0.9642\n",
      "Epoch 189/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0455 - acc: 0.9628 - val_loss: 0.0420 - val_acc: 0.9574\n",
      "Epoch 190/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0449 - acc: 0.9627 - val_loss: 0.0439 - val_acc: 0.9591\n",
      "Epoch 191/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.0456 - acc: 0.9600 - val_loss: 0.0420 - val_acc: 0.9591\n",
      "Epoch 192/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0447 - acc: 0.9616 - val_loss: 0.0426 - val_acc: 0.9540\n",
      "Epoch 193/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0445 - acc: 0.9624 - val_loss: 0.0401 - val_acc: 0.9582\n",
      "Epoch 194/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0446 - acc: 0.9613 - val_loss: 0.0412 - val_acc: 0.9548\n",
      "Epoch 195/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.0445 - acc: 0.9626 - val_loss: 0.0392 - val_acc: 0.9668\n",
      "Epoch 196/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0437 - acc: 0.9646 - val_loss: 0.0424 - val_acc: 0.9633\n",
      "Epoch 197/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0445 - acc: 0.9628 - val_loss: 0.0414 - val_acc: 0.9591\n",
      "Epoch 198/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0432 - acc: 0.9634 - val_loss: 0.0398 - val_acc: 0.9616\n",
      "Epoch 199/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0436 - acc: 0.9625 - val_loss: 0.0384 - val_acc: 0.9633\n",
      "Epoch 200/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0440 - acc: 0.9634 - val_loss: 0.0376 - val_acc: 0.9668\n",
      "Epoch 201/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0430 - acc: 0.9669 - val_loss: 0.0384 - val_acc: 0.9668\n",
      "Epoch 202/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0417 - acc: 0.9668 - val_loss: 0.0365 - val_acc: 0.9693\n",
      "Epoch 203/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0425 - acc: 0.9652 - val_loss: 0.0384 - val_acc: 0.9685\n",
      "Epoch 204/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0425 - acc: 0.9643 - val_loss: 0.0368 - val_acc: 0.9719\n",
      "Epoch 205/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0421 - acc: 0.9643 - val_loss: 0.0392 - val_acc: 0.9668\n",
      "Epoch 206/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0424 - acc: 0.9643 - val_loss: 0.0370 - val_acc: 0.9702\n",
      "Epoch 207/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0429 - acc: 0.9654 - val_loss: 0.0365 - val_acc: 0.9685\n",
      "Epoch 208/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0415 - acc: 0.9649 - val_loss: 0.0372 - val_acc: 0.9608\n",
      "Epoch 209/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0418 - acc: 0.9637 - val_loss: 0.0389 - val_acc: 0.9616\n",
      "Epoch 210/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0416 - acc: 0.9660 - val_loss: 0.0412 - val_acc: 0.9574\n",
      "Epoch 211/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0411 - acc: 0.9672 - val_loss: 0.0353 - val_acc: 0.9727\n",
      "Epoch 212/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0406 - acc: 0.9683 - val_loss: 0.0370 - val_acc: 0.9736\n",
      "Epoch 213/500\n",
      "22272/22272 [==============================] - 16s 713us/step - loss: 0.0414 - acc: 0.9670 - val_loss: 0.0371 - val_acc: 0.9676\n",
      "Epoch 214/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0410 - acc: 0.9677 - val_loss: 0.0347 - val_acc: 0.9719\n",
      "Epoch 215/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0410 - acc: 0.9662 - val_loss: 0.0367 - val_acc: 0.9642\n",
      "Epoch 216/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0404 - acc: 0.9677 - val_loss: 0.0332 - val_acc: 0.9710\n",
      "Epoch 217/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0398 - acc: 0.9687 - val_loss: 0.0347 - val_acc: 0.9719\n",
      "Epoch 218/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0400 - acc: 0.9705 - val_loss: 0.0341 - val_acc: 0.9753\n",
      "Epoch 219/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0401 - acc: 0.9685 - val_loss: 0.0317 - val_acc: 0.9744\n",
      "Epoch 220/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0402 - acc: 0.9670 - val_loss: 0.0328 - val_acc: 0.9736\n",
      "Epoch 221/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0398 - acc: 0.9693 - val_loss: 0.0347 - val_acc: 0.9702\n",
      "Epoch 222/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0398 - acc: 0.9687 - val_loss: 0.0375 - val_acc: 0.9727\n",
      "Epoch 223/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0400 - acc: 0.9680 - val_loss: 0.0363 - val_acc: 0.9702\n",
      "Epoch 224/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0393 - acc: 0.9674 - val_loss: 0.0323 - val_acc: 0.9727\n",
      "Epoch 225/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0389 - acc: 0.9683 - val_loss: 0.0349 - val_acc: 0.9676\n",
      "Epoch 226/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0390 - acc: 0.9713 - val_loss: 0.0337 - val_acc: 0.9702\n",
      "Epoch 227/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0386 - acc: 0.9695 - val_loss: 0.0330 - val_acc: 0.9719\n",
      "Epoch 228/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.0383 - acc: 0.9699 - val_loss: 0.0335 - val_acc: 0.9676\n",
      "Epoch 229/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0382 - acc: 0.9700 - val_loss: 0.0354 - val_acc: 0.9676\n",
      "Epoch 230/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0383 - acc: 0.9683 - val_loss: 0.0360 - val_acc: 0.9693\n",
      "Epoch 231/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0384 - acc: 0.9704 - val_loss: 0.0336 - val_acc: 0.9710\n",
      "Epoch 232/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0376 - acc: 0.9713 - val_loss: 0.0327 - val_acc: 0.9736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0381 - acc: 0.9700 - val_loss: 0.0327 - val_acc: 0.9668\n",
      "Epoch 234/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0378 - acc: 0.9697 - val_loss: 0.0366 - val_acc: 0.9668\n",
      "Epoch 235/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0384 - acc: 0.9701 - val_loss: 0.0348 - val_acc: 0.9770\n",
      "Epoch 236/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0374 - acc: 0.9715 - val_loss: 0.0329 - val_acc: 0.9761\n",
      "Epoch 237/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0374 - acc: 0.9703 - val_loss: 0.0330 - val_acc: 0.9702\n",
      "Epoch 238/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0376 - acc: 0.9709 - val_loss: 0.0333 - val_acc: 0.9744\n",
      "Epoch 239/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0370 - acc: 0.9727 - val_loss: 0.0326 - val_acc: 0.9795\n",
      "Epoch 240/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0368 - acc: 0.9722 - val_loss: 0.0322 - val_acc: 0.9727\n",
      "Epoch 241/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0370 - acc: 0.9709 - val_loss: 0.0321 - val_acc: 0.9693\n",
      "Epoch 242/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0367 - acc: 0.9737 - val_loss: 0.0309 - val_acc: 0.9744\n",
      "Epoch 243/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0367 - acc: 0.9718 - val_loss: 0.0304 - val_acc: 0.9770\n",
      "Epoch 244/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0365 - acc: 0.9718 - val_loss: 0.0335 - val_acc: 0.9736\n",
      "Epoch 245/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0364 - acc: 0.9720 - val_loss: 0.0309 - val_acc: 0.9761\n",
      "Epoch 246/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0356 - acc: 0.9722 - val_loss: 0.0341 - val_acc: 0.9753\n",
      "Epoch 247/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0362 - acc: 0.9733 - val_loss: 0.0317 - val_acc: 0.9744\n",
      "Epoch 248/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0364 - acc: 0.9719 - val_loss: 0.0322 - val_acc: 0.9727\n",
      "Epoch 249/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0367 - acc: 0.9710 - val_loss: 0.0291 - val_acc: 0.9778\n",
      "Epoch 250/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0357 - acc: 0.9745 - val_loss: 0.0280 - val_acc: 0.9787\n",
      "Epoch 251/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0355 - acc: 0.9737 - val_loss: 0.0322 - val_acc: 0.9736\n",
      "Epoch 252/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0358 - acc: 0.9737 - val_loss: 0.0294 - val_acc: 0.9761\n",
      "Epoch 253/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0349 - acc: 0.9730 - val_loss: 0.0322 - val_acc: 0.9727\n",
      "Epoch 254/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0355 - acc: 0.9724 - val_loss: 0.0331 - val_acc: 0.9770\n",
      "Epoch 255/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0351 - acc: 0.9726 - val_loss: 0.0293 - val_acc: 0.9770\n",
      "Epoch 256/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0352 - acc: 0.9741 - val_loss: 0.0324 - val_acc: 0.9650\n",
      "Epoch 257/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0346 - acc: 0.9757 - val_loss: 0.0289 - val_acc: 0.9761\n",
      "Epoch 258/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0347 - acc: 0.9741 - val_loss: 0.0300 - val_acc: 0.9744\n",
      "Epoch 259/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0353 - acc: 0.9739 - val_loss: 0.0299 - val_acc: 0.9778\n",
      "Epoch 260/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0349 - acc: 0.9730 - val_loss: 0.0291 - val_acc: 0.9710\n",
      "Epoch 261/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0341 - acc: 0.9746 - val_loss: 0.0287 - val_acc: 0.9761\n",
      "Epoch 262/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0344 - acc: 0.9749 - val_loss: 0.0299 - val_acc: 0.9744\n",
      "Epoch 263/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0340 - acc: 0.9752 - val_loss: 0.0300 - val_acc: 0.9727\n",
      "Epoch 264/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0346 - acc: 0.9739 - val_loss: 0.0295 - val_acc: 0.9787\n",
      "Epoch 265/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0343 - acc: 0.9734 - val_loss: 0.0276 - val_acc: 0.9778\n",
      "Epoch 266/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0338 - acc: 0.9759 - val_loss: 0.0310 - val_acc: 0.9685\n",
      "Epoch 267/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0334 - acc: 0.9751 - val_loss: 0.0289 - val_acc: 0.9744\n",
      "Epoch 268/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0341 - acc: 0.9748 - val_loss: 0.0281 - val_acc: 0.9770\n",
      "Epoch 269/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0335 - acc: 0.9739 - val_loss: 0.0291 - val_acc: 0.9753\n",
      "Epoch 270/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0336 - acc: 0.9749 - val_loss: 0.0309 - val_acc: 0.9710\n",
      "Epoch 271/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0335 - acc: 0.9756 - val_loss: 0.0309 - val_acc: 0.9727\n",
      "Epoch 272/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0338 - acc: 0.9733 - val_loss: 0.0277 - val_acc: 0.9795\n",
      "Epoch 273/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0330 - acc: 0.9739 - val_loss: 0.0306 - val_acc: 0.9727\n",
      "Epoch 274/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0333 - acc: 0.9765 - val_loss: 0.0310 - val_acc: 0.9719\n",
      "Epoch 275/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0338 - acc: 0.9741 - val_loss: 0.0309 - val_acc: 0.9744\n",
      "Epoch 276/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0333 - acc: 0.9748 - val_loss: 0.0279 - val_acc: 0.9761\n",
      "Epoch 277/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0328 - acc: 0.9765 - val_loss: 0.0310 - val_acc: 0.9770\n",
      "Epoch 278/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0327 - acc: 0.9760 - val_loss: 0.0280 - val_acc: 0.9787\n",
      "Epoch 279/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0327 - acc: 0.9765 - val_loss: 0.0282 - val_acc: 0.9787\n",
      "Epoch 280/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0322 - acc: 0.9771 - val_loss: 0.0274 - val_acc: 0.9727\n",
      "Epoch 281/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0323 - acc: 0.9757 - val_loss: 0.0269 - val_acc: 0.9770\n",
      "Epoch 282/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0328 - acc: 0.9768 - val_loss: 0.0281 - val_acc: 0.9778\n",
      "Epoch 283/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.0325 - acc: 0.9754 - val_loss: 0.0284 - val_acc: 0.9770\n",
      "Epoch 284/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0321 - acc: 0.9768 - val_loss: 0.0272 - val_acc: 0.9770\n",
      "Epoch 285/500\n",
      "22272/22272 [==============================] - 16s 710us/step - loss: 0.0329 - acc: 0.9741 - val_loss: 0.0282 - val_acc: 0.9804\n",
      "Epoch 286/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0326 - acc: 0.9761 - val_loss: 0.0253 - val_acc: 0.9812\n",
      "Epoch 287/500\n",
      "22272/22272 [==============================] - 16s 714us/step - loss: 0.0320 - acc: 0.9780 - val_loss: 0.0261 - val_acc: 0.9804\n",
      "Epoch 288/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0324 - acc: 0.9766 - val_loss: 0.0271 - val_acc: 0.9804\n",
      "Epoch 289/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0316 - acc: 0.9758 - val_loss: 0.0289 - val_acc: 0.9821\n",
      "Epoch 290/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0319 - acc: 0.9776 - val_loss: 0.0269 - val_acc: 0.9795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0308 - acc: 0.9790 - val_loss: 0.0253 - val_acc: 0.9787\n",
      "Epoch 292/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0311 - acc: 0.9766 - val_loss: 0.0278 - val_acc: 0.9761\n",
      "Epoch 293/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0313 - acc: 0.9770 - val_loss: 0.0282 - val_acc: 0.9795\n",
      "Epoch 294/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0318 - acc: 0.9767 - val_loss: 0.0271 - val_acc: 0.9770\n",
      "Epoch 295/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0316 - acc: 0.9774 - val_loss: 0.0277 - val_acc: 0.9744\n",
      "Epoch 296/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0312 - acc: 0.9767 - val_loss: 0.0269 - val_acc: 0.9770\n",
      "Epoch 297/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0311 - acc: 0.9775 - val_loss: 0.0268 - val_acc: 0.9804\n",
      "Epoch 298/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0309 - acc: 0.9771 - val_loss: 0.0281 - val_acc: 0.9787\n",
      "Epoch 299/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0313 - acc: 0.9779 - val_loss: 0.0251 - val_acc: 0.9829\n",
      "Epoch 300/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0311 - acc: 0.9771 - val_loss: 0.0253 - val_acc: 0.9829\n",
      "Epoch 301/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0313 - acc: 0.9767 - val_loss: 0.0269 - val_acc: 0.9812\n",
      "Epoch 302/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0311 - acc: 0.9775 - val_loss: 0.0248 - val_acc: 0.9804\n",
      "Epoch 303/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0307 - acc: 0.9767 - val_loss: 0.0275 - val_acc: 0.9727\n",
      "Epoch 304/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0309 - acc: 0.9773 - val_loss: 0.0263 - val_acc: 0.9778\n",
      "Epoch 305/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0310 - acc: 0.9778 - val_loss: 0.0252 - val_acc: 0.9787\n",
      "Epoch 306/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0311 - acc: 0.9772 - val_loss: 0.0261 - val_acc: 0.9770\n",
      "Epoch 307/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0308 - acc: 0.9777 - val_loss: 0.0259 - val_acc: 0.9795\n",
      "Epoch 308/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0303 - acc: 0.9789 - val_loss: 0.0249 - val_acc: 0.9812\n",
      "Epoch 309/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0301 - acc: 0.9789 - val_loss: 0.0259 - val_acc: 0.9778\n",
      "Epoch 310/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0301 - acc: 0.9790 - val_loss: 0.0254 - val_acc: 0.9812\n",
      "Epoch 311/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0299 - acc: 0.9792 - val_loss: 0.0237 - val_acc: 0.9838\n",
      "Epoch 312/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0301 - acc: 0.9772 - val_loss: 0.0259 - val_acc: 0.9770\n",
      "Epoch 313/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0299 - acc: 0.9785 - val_loss: 0.0232 - val_acc: 0.9812\n",
      "Epoch 314/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0298 - acc: 0.9784 - val_loss: 0.0240 - val_acc: 0.9812\n",
      "Epoch 315/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0299 - acc: 0.9774 - val_loss: 0.0242 - val_acc: 0.9838\n",
      "Epoch 316/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0297 - acc: 0.9782 - val_loss: 0.0258 - val_acc: 0.9787\n",
      "Epoch 317/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0305 - acc: 0.9778 - val_loss: 0.0256 - val_acc: 0.9821\n",
      "Epoch 318/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0295 - acc: 0.9785 - val_loss: 0.0255 - val_acc: 0.9812\n",
      "Epoch 319/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0297 - acc: 0.9775 - val_loss: 0.0230 - val_acc: 0.9795\n",
      "Epoch 320/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0299 - acc: 0.9789 - val_loss: 0.0250 - val_acc: 0.9770\n",
      "Epoch 321/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0292 - acc: 0.9786 - val_loss: 0.0256 - val_acc: 0.9744\n",
      "Epoch 322/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0296 - acc: 0.9781 - val_loss: 0.0248 - val_acc: 0.9778\n",
      "Epoch 323/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0296 - acc: 0.9780 - val_loss: 0.0238 - val_acc: 0.9812\n",
      "Epoch 324/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0291 - acc: 0.9790 - val_loss: 0.0252 - val_acc: 0.9787\n",
      "Epoch 325/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0301 - acc: 0.9783 - val_loss: 0.0280 - val_acc: 0.9812\n",
      "Epoch 326/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0292 - acc: 0.9801 - val_loss: 0.0244 - val_acc: 0.9812\n",
      "Epoch 327/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0294 - acc: 0.9793 - val_loss: 0.0234 - val_acc: 0.9821\n",
      "Epoch 328/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0288 - acc: 0.9808 - val_loss: 0.0276 - val_acc: 0.9770\n",
      "Epoch 329/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0296 - acc: 0.9786 - val_loss: 0.0243 - val_acc: 0.9812\n",
      "Epoch 330/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0286 - acc: 0.9796 - val_loss: 0.0256 - val_acc: 0.9812\n",
      "Epoch 331/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0295 - acc: 0.9787 - val_loss: 0.0244 - val_acc: 0.9821\n",
      "Epoch 332/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0290 - acc: 0.9788 - val_loss: 0.0226 - val_acc: 0.9829\n",
      "Epoch 333/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0287 - acc: 0.9799 - val_loss: 0.0244 - val_acc: 0.9812\n",
      "Epoch 334/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0290 - acc: 0.9801 - val_loss: 0.0242 - val_acc: 0.9804\n",
      "Epoch 335/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0283 - acc: 0.9790 - val_loss: 0.0226 - val_acc: 0.9838\n",
      "Epoch 336/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0287 - acc: 0.9796 - val_loss: 0.0248 - val_acc: 0.9795\n",
      "Epoch 337/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0282 - acc: 0.9819 - val_loss: 0.0225 - val_acc: 0.9804\n",
      "Epoch 338/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0283 - acc: 0.9802 - val_loss: 0.0235 - val_acc: 0.9804\n",
      "Epoch 339/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0287 - acc: 0.9790 - val_loss: 0.0237 - val_acc: 0.9821\n",
      "Epoch 340/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0283 - acc: 0.9802 - val_loss: 0.0221 - val_acc: 0.9812\n",
      "Epoch 341/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0284 - acc: 0.9798 - val_loss: 0.0234 - val_acc: 0.9753\n",
      "Epoch 342/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0277 - acc: 0.9804 - val_loss: 0.0223 - val_acc: 0.9838\n",
      "Epoch 343/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0280 - acc: 0.9810 - val_loss: 0.0232 - val_acc: 0.9778\n",
      "Epoch 344/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0278 - acc: 0.9797 - val_loss: 0.0236 - val_acc: 0.9847\n",
      "Epoch 345/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0282 - acc: 0.9799 - val_loss: 0.0236 - val_acc: 0.9821\n",
      "Epoch 346/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0279 - acc: 0.9794 - val_loss: 0.0229 - val_acc: 0.9821\n",
      "Epoch 347/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0273 - acc: 0.9812 - val_loss: 0.0238 - val_acc: 0.9847\n",
      "Epoch 348/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0279 - acc: 0.9811 - val_loss: 0.0223 - val_acc: 0.9829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0278 - acc: 0.9808 - val_loss: 0.0211 - val_acc: 0.9847\n",
      "Epoch 350/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0274 - acc: 0.9813 - val_loss: 0.0231 - val_acc: 0.9812\n",
      "Epoch 351/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0278 - acc: 0.9813 - val_loss: 0.0222 - val_acc: 0.9812\n",
      "Epoch 352/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0275 - acc: 0.9811 - val_loss: 0.0226 - val_acc: 0.9812\n",
      "Epoch 353/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0269 - acc: 0.9823 - val_loss: 0.0227 - val_acc: 0.9821\n",
      "Epoch 354/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0276 - acc: 0.9806 - val_loss: 0.0238 - val_acc: 0.9787\n",
      "Epoch 355/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0271 - acc: 0.9820 - val_loss: 0.0214 - val_acc: 0.9821\n",
      "Epoch 356/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0272 - acc: 0.9816 - val_loss: 0.0229 - val_acc: 0.9778\n",
      "Epoch 357/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0273 - acc: 0.9808 - val_loss: 0.0225 - val_acc: 0.9787\n",
      "Epoch 358/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0274 - acc: 0.9805 - val_loss: 0.0249 - val_acc: 0.9804\n",
      "Epoch 359/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0276 - acc: 0.9797 - val_loss: 0.0241 - val_acc: 0.9812\n",
      "Epoch 360/500\n",
      "22272/22272 [==============================] - 16s 725us/step - loss: 0.0278 - acc: 0.9810 - val_loss: 0.0213 - val_acc: 0.9855\n",
      "Epoch 361/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0276 - acc: 0.9803 - val_loss: 0.0219 - val_acc: 0.9864\n",
      "Epoch 362/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0276 - acc: 0.9802 - val_loss: 0.0214 - val_acc: 0.9864\n",
      "Epoch 363/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0268 - acc: 0.9812 - val_loss: 0.0243 - val_acc: 0.9778\n",
      "Epoch 364/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0273 - acc: 0.9816 - val_loss: 0.0219 - val_acc: 0.9804\n",
      "Epoch 365/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0267 - acc: 0.9827 - val_loss: 0.0239 - val_acc: 0.9847\n",
      "Epoch 366/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0268 - acc: 0.9804 - val_loss: 0.0225 - val_acc: 0.9838\n",
      "Epoch 367/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0266 - acc: 0.9811 - val_loss: 0.0220 - val_acc: 0.9829\n",
      "Epoch 368/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0271 - acc: 0.9807 - val_loss: 0.0218 - val_acc: 0.9821\n",
      "Epoch 369/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0266 - acc: 0.9819 - val_loss: 0.0214 - val_acc: 0.9821\n",
      "Epoch 370/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0272 - acc: 0.9796 - val_loss: 0.0219 - val_acc: 0.9812\n",
      "Epoch 371/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0262 - acc: 0.9834 - val_loss: 0.0218 - val_acc: 0.9812\n",
      "Epoch 372/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0266 - acc: 0.9820 - val_loss: 0.0214 - val_acc: 0.9821\n",
      "Epoch 373/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0267 - acc: 0.9801 - val_loss: 0.0217 - val_acc: 0.9855\n",
      "Epoch 374/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0268 - acc: 0.9824 - val_loss: 0.0229 - val_acc: 0.9804\n",
      "Epoch 375/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0268 - acc: 0.9830 - val_loss: 0.0223 - val_acc: 0.9787\n",
      "Epoch 376/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0272 - acc: 0.9800 - val_loss: 0.0230 - val_acc: 0.9821\n",
      "Epoch 377/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0262 - acc: 0.9811 - val_loss: 0.0220 - val_acc: 0.9821\n",
      "Epoch 378/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0266 - acc: 0.9816 - val_loss: 0.0211 - val_acc: 0.9847\n",
      "Epoch 379/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0268 - acc: 0.9820 - val_loss: 0.0223 - val_acc: 0.9804\n",
      "Epoch 380/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0263 - acc: 0.9808 - val_loss: 0.0223 - val_acc: 0.9847\n",
      "Epoch 381/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0266 - acc: 0.9813 - val_loss: 0.0213 - val_acc: 0.9795\n",
      "Epoch 382/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0267 - acc: 0.9820 - val_loss: 0.0224 - val_acc: 0.9838\n",
      "Epoch 383/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0261 - acc: 0.9816 - val_loss: 0.0225 - val_acc: 0.9847\n",
      "Epoch 384/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0260 - acc: 0.9814 - val_loss: 0.0214 - val_acc: 0.9829\n",
      "Epoch 385/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0256 - acc: 0.9820 - val_loss: 0.0214 - val_acc: 0.9829\n",
      "Epoch 386/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0257 - acc: 0.9813 - val_loss: 0.0208 - val_acc: 0.9855\n",
      "Epoch 387/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0258 - acc: 0.9828 - val_loss: 0.0208 - val_acc: 0.9838\n",
      "Epoch 388/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0258 - acc: 0.9822 - val_loss: 0.0208 - val_acc: 0.9829\n",
      "Epoch 389/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0258 - acc: 0.9814 - val_loss: 0.0206 - val_acc: 0.9855\n",
      "Epoch 390/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0260 - acc: 0.9823 - val_loss: 0.0212 - val_acc: 0.9812\n",
      "Epoch 391/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0263 - acc: 0.9801 - val_loss: 0.0215 - val_acc: 0.9855\n",
      "Epoch 392/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0259 - acc: 0.9808 - val_loss: 0.0230 - val_acc: 0.9829\n",
      "Epoch 393/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0256 - acc: 0.9814 - val_loss: 0.0208 - val_acc: 0.9864\n",
      "Epoch 394/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0254 - acc: 0.9809 - val_loss: 0.0215 - val_acc: 0.9829\n",
      "Epoch 395/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0253 - acc: 0.9828 - val_loss: 0.0225 - val_acc: 0.9829\n",
      "Epoch 396/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0257 - acc: 0.9817 - val_loss: 0.0225 - val_acc: 0.9821\n",
      "Epoch 397/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0257 - acc: 0.9824 - val_loss: 0.0208 - val_acc: 0.9787\n",
      "Epoch 398/500\n",
      "22272/22272 [==============================] - 16s 722us/step - loss: 0.0260 - acc: 0.9809 - val_loss: 0.0222 - val_acc: 0.9864\n",
      "Epoch 399/500\n",
      "22272/22272 [==============================] - 16s 730us/step - loss: 0.0255 - acc: 0.9830 - val_loss: 0.0204 - val_acc: 0.9829\n",
      "Epoch 400/500\n",
      "22272/22272 [==============================] - 16s 714us/step - loss: 0.0253 - acc: 0.9824 - val_loss: 0.0216 - val_acc: 0.9787\n",
      "Epoch 401/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0256 - acc: 0.9814 - val_loss: 0.0193 - val_acc: 0.9881\n",
      "Epoch 402/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0255 - acc: 0.9829 - val_loss: 0.0207 - val_acc: 0.9838\n",
      "Epoch 403/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0250 - acc: 0.9837 - val_loss: 0.0221 - val_acc: 0.9864\n",
      "Epoch 404/500\n",
      "22272/22272 [==============================] - 16s 717us/step - loss: 0.0252 - acc: 0.9836 - val_loss: 0.0232 - val_acc: 0.9821\n",
      "Epoch 405/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0253 - acc: 0.9833 - val_loss: 0.0190 - val_acc: 0.9872\n",
      "Epoch 406/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0250 - acc: 0.9831 - val_loss: 0.0228 - val_acc: 0.9761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 407/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.0250 - acc: 0.9833 - val_loss: 0.0221 - val_acc: 0.9847\n",
      "Epoch 408/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0250 - acc: 0.9828 - val_loss: 0.0216 - val_acc: 0.9812\n",
      "Epoch 409/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0254 - acc: 0.9825 - val_loss: 0.0200 - val_acc: 0.9898\n",
      "Epoch 410/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0254 - acc: 0.9818 - val_loss: 0.0198 - val_acc: 0.9829\n",
      "Epoch 411/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0251 - acc: 0.9822 - val_loss: 0.0217 - val_acc: 0.9872\n",
      "Epoch 412/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0254 - acc: 0.9822 - val_loss: 0.0214 - val_acc: 0.9872\n",
      "Epoch 413/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0255 - acc: 0.9824 - val_loss: 0.0203 - val_acc: 0.9881\n",
      "Epoch 414/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0248 - acc: 0.9832 - val_loss: 0.0218 - val_acc: 0.9855\n",
      "Epoch 415/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0248 - acc: 0.9846 - val_loss: 0.0201 - val_acc: 0.9812\n",
      "Epoch 416/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0250 - acc: 0.9828 - val_loss: 0.0208 - val_acc: 0.9821\n",
      "Epoch 417/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0250 - acc: 0.9813 - val_loss: 0.0200 - val_acc: 0.9829\n",
      "Epoch 418/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0250 - acc: 0.9826 - val_loss: 0.0193 - val_acc: 0.9855\n",
      "Epoch 419/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0250 - acc: 0.9818 - val_loss: 0.0203 - val_acc: 0.9855\n",
      "Epoch 420/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0244 - acc: 0.9836 - val_loss: 0.0193 - val_acc: 0.9821\n",
      "Epoch 421/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0248 - acc: 0.9828 - val_loss: 0.0202 - val_acc: 0.9847\n",
      "Epoch 422/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0246 - acc: 0.9835 - val_loss: 0.0201 - val_acc: 0.9847\n",
      "Epoch 423/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0248 - acc: 0.9846 - val_loss: 0.0194 - val_acc: 0.9847\n",
      "Epoch 424/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0243 - acc: 0.9841 - val_loss: 0.0198 - val_acc: 0.9881\n",
      "Epoch 425/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0245 - acc: 0.9837 - val_loss: 0.0222 - val_acc: 0.9778\n",
      "Epoch 426/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0243 - acc: 0.9840 - val_loss: 0.0197 - val_acc: 0.9855\n",
      "Epoch 427/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0245 - acc: 0.9827 - val_loss: 0.0195 - val_acc: 0.9872\n",
      "Epoch 428/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0249 - acc: 0.9825 - val_loss: 0.0204 - val_acc: 0.9821\n",
      "Epoch 429/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0243 - acc: 0.9852 - val_loss: 0.0195 - val_acc: 0.9898\n",
      "Epoch 430/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0245 - acc: 0.9816 - val_loss: 0.0191 - val_acc: 0.9889\n",
      "Epoch 431/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0240 - acc: 0.9838 - val_loss: 0.0203 - val_acc: 0.9872\n",
      "Epoch 432/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0239 - acc: 0.9851 - val_loss: 0.0203 - val_acc: 0.9821\n",
      "Epoch 433/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0240 - acc: 0.9845 - val_loss: 0.0191 - val_acc: 0.9881\n",
      "Epoch 434/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0238 - acc: 0.9834 - val_loss: 0.0188 - val_acc: 0.9881\n",
      "Epoch 435/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0241 - acc: 0.9842 - val_loss: 0.0197 - val_acc: 0.9847\n",
      "Epoch 436/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0245 - acc: 0.9819 - val_loss: 0.0194 - val_acc: 0.9855\n",
      "Epoch 437/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0239 - acc: 0.9832 - val_loss: 0.0187 - val_acc: 0.9881\n",
      "Epoch 438/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0242 - acc: 0.9835 - val_loss: 0.0211 - val_acc: 0.9812\n",
      "Epoch 439/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0240 - acc: 0.9842 - val_loss: 0.0202 - val_acc: 0.9889\n",
      "Epoch 440/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0236 - acc: 0.9830 - val_loss: 0.0190 - val_acc: 0.9864\n",
      "Epoch 441/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0239 - acc: 0.9821 - val_loss: 0.0199 - val_acc: 0.9787\n",
      "Epoch 442/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0241 - acc: 0.9824 - val_loss: 0.0209 - val_acc: 0.9872\n",
      "Epoch 443/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0247 - acc: 0.9832 - val_loss: 0.0213 - val_acc: 0.9838\n",
      "Epoch 444/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0242 - acc: 0.9823 - val_loss: 0.0183 - val_acc: 0.9838\n",
      "Epoch 445/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0237 - acc: 0.9839 - val_loss: 0.0195 - val_acc: 0.9872\n",
      "Epoch 446/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0241 - acc: 0.9833 - val_loss: 0.0183 - val_acc: 0.9881\n",
      "Epoch 447/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0237 - acc: 0.9836 - val_loss: 0.0196 - val_acc: 0.9881\n",
      "Epoch 448/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0233 - acc: 0.9839 - val_loss: 0.0193 - val_acc: 0.9864\n",
      "Epoch 449/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0234 - acc: 0.9846 - val_loss: 0.0187 - val_acc: 0.9881\n",
      "Epoch 450/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0240 - acc: 0.9834 - val_loss: 0.0206 - val_acc: 0.9855\n",
      "Epoch 451/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0236 - acc: 0.9842 - val_loss: 0.0198 - val_acc: 0.9889\n",
      "Epoch 452/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.0241 - acc: 0.9827 - val_loss: 0.0204 - val_acc: 0.9847\n",
      "Epoch 453/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0238 - acc: 0.9846 - val_loss: 0.0214 - val_acc: 0.9847\n",
      "Epoch 454/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0235 - acc: 0.9833 - val_loss: 0.0187 - val_acc: 0.9872\n",
      "Epoch 455/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0235 - acc: 0.9844 - val_loss: 0.0213 - val_acc: 0.9855\n",
      "Epoch 456/500\n",
      "22272/22272 [==============================] - 16s 714us/step - loss: 0.0234 - acc: 0.9840 - val_loss: 0.0212 - val_acc: 0.9838\n",
      "Epoch 457/500\n",
      "22272/22272 [==============================] - 20s 920us/step - loss: 0.0234 - acc: 0.9842 - val_loss: 0.0188 - val_acc: 0.9855\n",
      "Epoch 458/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0235 - acc: 0.9836 - val_loss: 0.0189 - val_acc: 0.9881\n",
      "Epoch 459/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0227 - acc: 0.9842 - val_loss: 0.0193 - val_acc: 0.9855\n",
      "Epoch 460/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0230 - acc: 0.9839 - val_loss: 0.0188 - val_acc: 0.9881\n",
      "Epoch 461/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0234 - acc: 0.9830 - val_loss: 0.0178 - val_acc: 0.9855\n",
      "Epoch 462/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0234 - acc: 0.9847 - val_loss: 0.0204 - val_acc: 0.9847\n",
      "Epoch 463/500\n",
      "22272/22272 [==============================] - 26s 1ms/step - loss: 0.0240 - acc: 0.9846 - val_loss: 0.0196 - val_acc: 0.9838\n",
      "Epoch 464/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0232 - acc: 0.9851 - val_loss: 0.0191 - val_acc: 0.9864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0230 - acc: 0.9847 - val_loss: 0.0196 - val_acc: 0.9881\n",
      "Epoch 466/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0232 - acc: 0.9838 - val_loss: 0.0181 - val_acc: 0.9855\n",
      "Epoch 467/500\n",
      "22272/22272 [==============================] - 26s 1ms/step - loss: 0.0234 - acc: 0.9842 - val_loss: 0.0205 - val_acc: 0.9804\n",
      "Epoch 468/500\n",
      "22272/22272 [==============================] - 26s 1ms/step - loss: 0.0235 - acc: 0.9826 - val_loss: 0.0210 - val_acc: 0.9812\n",
      "Epoch 469/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0230 - acc: 0.9847 - val_loss: 0.0193 - val_acc: 0.9872\n",
      "Epoch 470/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0226 - acc: 0.9855 - val_loss: 0.0186 - val_acc: 0.9906\n",
      "Epoch 471/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0235 - acc: 0.9833 - val_loss: 0.0198 - val_acc: 0.9864\n",
      "Epoch 472/500\n",
      "22272/22272 [==============================] - 26s 1ms/step - loss: 0.0233 - acc: 0.9843 - val_loss: 0.0217 - val_acc: 0.9761\n",
      "Epoch 473/500\n",
      "22272/22272 [==============================] - 26s 1ms/step - loss: 0.0231 - acc: 0.9847 - val_loss: 0.0199 - val_acc: 0.9804\n",
      "Epoch 474/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0230 - acc: 0.9839 - val_loss: 0.0207 - val_acc: 0.9838\n",
      "Epoch 475/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0238 - acc: 0.9836 - val_loss: 0.0199 - val_acc: 0.9855\n",
      "Epoch 476/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0229 - acc: 0.9837 - val_loss: 0.0195 - val_acc: 0.9829\n",
      "Epoch 477/500\n",
      "22272/22272 [==============================] - 26s 1ms/step - loss: 0.0231 - acc: 0.9849 - val_loss: 0.0195 - val_acc: 0.9855\n",
      "Epoch 478/500\n",
      "22272/22272 [==============================] - 26s 1ms/step - loss: 0.0226 - acc: 0.9853 - val_loss: 0.0201 - val_acc: 0.9889\n",
      "Epoch 479/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0229 - acc: 0.9839 - val_loss: 0.0185 - val_acc: 0.9881\n",
      "Epoch 480/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0229 - acc: 0.9836 - val_loss: 0.0179 - val_acc: 0.9881\n",
      "Epoch 481/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0232 - acc: 0.9837 - val_loss: 0.0191 - val_acc: 0.9864\n",
      "Epoch 482/500\n",
      "22272/22272 [==============================] - 26s 1ms/step - loss: 0.0225 - acc: 0.9842 - val_loss: 0.0176 - val_acc: 0.9898\n",
      "Epoch 483/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0225 - acc: 0.9853 - val_loss: 0.0178 - val_acc: 0.9906\n",
      "Epoch 484/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0229 - acc: 0.9842 - val_loss: 0.0175 - val_acc: 0.9872\n",
      "Epoch 485/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0227 - acc: 0.9845 - val_loss: 0.0179 - val_acc: 0.9881\n",
      "Epoch 486/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0230 - acc: 0.9825 - val_loss: 0.0178 - val_acc: 0.9872\n",
      "Epoch 487/500\n",
      "22272/22272 [==============================] - 26s 1ms/step - loss: 0.0226 - acc: 0.9845 - val_loss: 0.0179 - val_acc: 0.9872\n",
      "Epoch 488/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0226 - acc: 0.9846 - val_loss: 0.0170 - val_acc: 0.9881\n",
      "Epoch 489/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0228 - acc: 0.9849 - val_loss: 0.0191 - val_acc: 0.9795\n",
      "Epoch 490/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0224 - acc: 0.9840 - val_loss: 0.0179 - val_acc: 0.9847\n",
      "Epoch 491/500\n",
      "22272/22272 [==============================] - 26s 1ms/step - loss: 0.0233 - acc: 0.9854 - val_loss: 0.0182 - val_acc: 0.9847\n",
      "Epoch 492/500\n",
      "22272/22272 [==============================] - 26s 1ms/step - loss: 0.0226 - acc: 0.9859 - val_loss: 0.0198 - val_acc: 0.9872\n",
      "Epoch 493/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0230 - acc: 0.9840 - val_loss: 0.0204 - val_acc: 0.9855\n",
      "Epoch 494/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0226 - acc: 0.9845 - val_loss: 0.0168 - val_acc: 0.9804\n",
      "Epoch 495/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0228 - acc: 0.9831 - val_loss: 0.0196 - val_acc: 0.9855\n",
      "Epoch 496/500\n",
      "22272/22272 [==============================] - 26s 1ms/step - loss: 0.0226 - acc: 0.9850 - val_loss: 0.0191 - val_acc: 0.9898\n",
      "Epoch 497/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0231 - acc: 0.9853 - val_loss: 0.0186 - val_acc: 0.9821\n",
      "Epoch 498/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0227 - acc: 0.9844 - val_loss: 0.0184 - val_acc: 0.9864\n",
      "Epoch 499/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0226 - acc: 0.9851 - val_loss: 0.0202 - val_acc: 0.9804\n",
      "Epoch 500/500\n",
      "22272/22272 [==============================] - 25s 1ms/step - loss: 0.0226 - acc: 0.9850 - val_loss: 0.0201 - val_acc: 0.9812\n"
     ]
    }
   ],
   "source": [
    "##Fitting the GRU to the Training set\n",
    "hist5=model5.fit(X_train, y_train, batch_size=300, epochs=500, validation_split=0.05, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22272 samples, validate on 1173 samples\n",
      "Epoch 1/500\n",
      "22272/22272 [==============================] - 33s 1ms/step - loss: 0.2408 - acc: 0.7491 - val_loss: 0.2221 - val_acc: 0.7289\n",
      "Epoch 2/500\n",
      "22272/22272 [==============================] - 20s 917us/step - loss: 0.2105 - acc: 0.7486 - val_loss: 0.2084 - val_acc: 0.7255\n",
      "Epoch 3/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.2019 - acc: 0.7512 - val_loss: 0.2043 - val_acc: 0.7366\n",
      "Epoch 4/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.1990 - acc: 0.7562 - val_loss: 0.2018 - val_acc: 0.7349\n",
      "Epoch 5/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.1968 - acc: 0.7560 - val_loss: 0.2032 - val_acc: 0.7460\n",
      "Epoch 6/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.1946 - acc: 0.7538 - val_loss: 0.1980 - val_acc: 0.7425\n",
      "Epoch 7/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.1923 - acc: 0.7516 - val_loss: 0.1948 - val_acc: 0.7280\n",
      "Epoch 8/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.1901 - acc: 0.7525 - val_loss: 0.1999 - val_acc: 0.7425\n",
      "Epoch 9/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.1860 - acc: 0.7584 - val_loss: 0.1876 - val_acc: 0.7511\n",
      "Epoch 10/500\n",
      "22272/22272 [==============================] - 17s 749us/step - loss: 0.1796 - acc: 0.7636 - val_loss: 0.1827 - val_acc: 0.7434\n",
      "Epoch 11/500\n",
      "22272/22272 [==============================] - 17s 784us/step - loss: 0.1785 - acc: 0.7675 - val_loss: 0.1855 - val_acc: 0.7434\n",
      "Epoch 12/500\n",
      "22272/22272 [==============================] - 17s 785us/step - loss: 0.1747 - acc: 0.7714 - val_loss: 0.1860 - val_acc: 0.7630\n",
      "Epoch 13/500\n",
      "22272/22272 [==============================] - 17s 751us/step - loss: 0.1735 - acc: 0.7772 - val_loss: 0.1810 - val_acc: 0.7545\n",
      "Epoch 14/500\n",
      "22272/22272 [==============================] - 17s 767us/step - loss: 0.1707 - acc: 0.7822 - val_loss: 0.1755 - val_acc: 0.7681\n",
      "Epoch 15/500\n",
      "22272/22272 [==============================] - 17s 767us/step - loss: 0.1687 - acc: 0.7852 - val_loss: 0.1719 - val_acc: 0.7749\n",
      "Epoch 16/500\n",
      "22272/22272 [==============================] - 16s 719us/step - loss: 0.1665 - acc: 0.7880 - val_loss: 0.1725 - val_acc: 0.7425\n",
      "Epoch 17/500\n",
      "22272/22272 [==============================] - 16s 721us/step - loss: 0.1659 - acc: 0.7890 - val_loss: 0.1703 - val_acc: 0.7579\n",
      "Epoch 18/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.1632 - acc: 0.7909 - val_loss: 0.1691 - val_acc: 0.7792\n",
      "Epoch 19/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.1608 - acc: 0.7942 - val_loss: 0.1735 - val_acc: 0.7690\n",
      "Epoch 20/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.1623 - acc: 0.7914 - val_loss: 0.1648 - val_acc: 0.7835\n",
      "Epoch 21/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.1584 - acc: 0.7976 - val_loss: 0.1724 - val_acc: 0.7775\n",
      "Epoch 22/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.1568 - acc: 0.7999 - val_loss: 0.1709 - val_acc: 0.7741\n",
      "Epoch 23/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.1559 - acc: 0.7983 - val_loss: 0.1607 - val_acc: 0.7792\n",
      "Epoch 24/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.1530 - acc: 0.8013 - val_loss: 0.1650 - val_acc: 0.7835\n",
      "Epoch 25/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.1544 - acc: 0.8003 - val_loss: 0.1790 - val_acc: 0.7826\n",
      "Epoch 26/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.1529 - acc: 0.8053 - val_loss: 0.1605 - val_acc: 0.7826\n",
      "Epoch 27/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.1505 - acc: 0.8045 - val_loss: 0.1567 - val_acc: 0.7877\n",
      "Epoch 28/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.1498 - acc: 0.8074 - val_loss: 0.1526 - val_acc: 0.7903\n",
      "Epoch 29/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.1479 - acc: 0.8090 - val_loss: 0.1591 - val_acc: 0.7997\n",
      "Epoch 30/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.1467 - acc: 0.8142 - val_loss: 0.1552 - val_acc: 0.7903\n",
      "Epoch 31/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.1462 - acc: 0.8142 - val_loss: 0.1522 - val_acc: 0.7945\n",
      "Epoch 32/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.1462 - acc: 0.8126 - val_loss: 0.1530 - val_acc: 0.7988\n",
      "Epoch 33/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.1433 - acc: 0.8175 - val_loss: 0.1494 - val_acc: 0.8124\n",
      "Epoch 34/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.1422 - acc: 0.8186 - val_loss: 0.1498 - val_acc: 0.7988\n",
      "Epoch 35/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.1412 - acc: 0.8217 - val_loss: 0.1480 - val_acc: 0.8082\n",
      "Epoch 36/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.1398 - acc: 0.8222 - val_loss: 0.1452 - val_acc: 0.8048\n",
      "Epoch 37/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.1402 - acc: 0.8228 - val_loss: 0.1489 - val_acc: 0.7971\n",
      "Epoch 38/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.1394 - acc: 0.8222 - val_loss: 0.1458 - val_acc: 0.8065\n",
      "Epoch 39/500\n",
      "22272/22272 [==============================] - 16s 715us/step - loss: 0.1376 - acc: 0.8256 - val_loss: 0.1516 - val_acc: 0.8048\n",
      "Epoch 40/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.1385 - acc: 0.8243 - val_loss: 0.1437 - val_acc: 0.8142\n",
      "Epoch 41/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.1372 - acc: 0.8267 - val_loss: 0.1476 - val_acc: 0.8167\n",
      "Epoch 42/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.1358 - acc: 0.8295 - val_loss: 0.1446 - val_acc: 0.8210\n",
      "Epoch 43/500\n",
      "22272/22272 [==============================] - 16s 716us/step - loss: 0.1341 - acc: 0.8290 - val_loss: 0.1436 - val_acc: 0.8329\n",
      "Epoch 44/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.1342 - acc: 0.8312 - val_loss: 0.1410 - val_acc: 0.8073\n",
      "Epoch 45/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.1331 - acc: 0.8346 - val_loss: 0.1368 - val_acc: 0.8244\n",
      "Epoch 46/500\n",
      "22272/22272 [==============================] - 16s 720us/step - loss: 0.1321 - acc: 0.8352 - val_loss: 0.1355 - val_acc: 0.8321\n",
      "Epoch 47/500\n",
      "22272/22272 [==============================] - 16s 710us/step - loss: 0.1310 - acc: 0.8371 - val_loss: 0.1318 - val_acc: 0.8312\n",
      "Epoch 48/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.1317 - acc: 0.8375 - val_loss: 0.1373 - val_acc: 0.8346\n",
      "Epoch 49/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.1292 - acc: 0.8378 - val_loss: 0.1372 - val_acc: 0.8286\n",
      "Epoch 50/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.1268 - acc: 0.8431 - val_loss: 0.1330 - val_acc: 0.8346\n",
      "Epoch 51/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.1284 - acc: 0.8417 - val_loss: 0.1314 - val_acc: 0.8457\n",
      "Epoch 52/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.1269 - acc: 0.8433 - val_loss: 0.1347 - val_acc: 0.8380\n",
      "Epoch 53/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.1259 - acc: 0.8440 - val_loss: 0.1291 - val_acc: 0.8448\n",
      "Epoch 54/500\n",
      "22272/22272 [==============================] - 16s 714us/step - loss: 0.1244 - acc: 0.8479 - val_loss: 0.1288 - val_acc: 0.8338\n",
      "Epoch 55/500\n",
      "22272/22272 [==============================] - 16s 710us/step - loss: 0.1224 - acc: 0.8493 - val_loss: 0.1280 - val_acc: 0.8414\n",
      "Epoch 56/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.1222 - acc: 0.8476 - val_loss: 0.1264 - val_acc: 0.8440\n",
      "Epoch 57/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.1202 - acc: 0.8522 - val_loss: 0.1224 - val_acc: 0.8389\n",
      "Epoch 58/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.1199 - acc: 0.8510 - val_loss: 0.1197 - val_acc: 0.8559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.1184 - acc: 0.8539 - val_loss: 0.1196 - val_acc: 0.8517\n",
      "Epoch 60/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.1167 - acc: 0.8573 - val_loss: 0.1177 - val_acc: 0.8491\n",
      "Epoch 61/500\n",
      "22272/22272 [==============================] - 16s 727us/step - loss: 0.1174 - acc: 0.8563 - val_loss: 0.1186 - val_acc: 0.8559\n",
      "Epoch 62/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.1154 - acc: 0.8609 - val_loss: 0.1190 - val_acc: 0.8465\n",
      "Epoch 63/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.1129 - acc: 0.8640 - val_loss: 0.1155 - val_acc: 0.8593\n",
      "Epoch 64/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.1125 - acc: 0.8645 - val_loss: 0.1173 - val_acc: 0.8457\n",
      "Epoch 65/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.1115 - acc: 0.8646 - val_loss: 0.1114 - val_acc: 0.8619\n",
      "Epoch 66/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.1110 - acc: 0.8651 - val_loss: 0.1111 - val_acc: 0.8645\n",
      "Epoch 67/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.1099 - acc: 0.8676 - val_loss: 0.1094 - val_acc: 0.8653\n",
      "Epoch 68/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1095 - acc: 0.8675 - val_loss: 0.1095 - val_acc: 0.8491\n",
      "Epoch 69/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.1081 - acc: 0.8684 - val_loss: 0.1131 - val_acc: 0.8636\n",
      "Epoch 70/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.1054 - acc: 0.8745 - val_loss: 0.1052 - val_acc: 0.8542\n",
      "Epoch 71/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.1056 - acc: 0.8728 - val_loss: 0.1053 - val_acc: 0.8619\n",
      "Epoch 72/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.1048 - acc: 0.8740 - val_loss: 0.1050 - val_acc: 0.8721\n",
      "Epoch 73/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.1042 - acc: 0.8756 - val_loss: 0.1045 - val_acc: 0.8653\n",
      "Epoch 74/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.1019 - acc: 0.8786 - val_loss: 0.1026 - val_acc: 0.8798\n",
      "Epoch 75/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.1016 - acc: 0.8792 - val_loss: 0.1020 - val_acc: 0.8798\n",
      "Epoch 76/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.1011 - acc: 0.8819 - val_loss: 0.0992 - val_acc: 0.8832\n",
      "Epoch 77/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.1002 - acc: 0.8802 - val_loss: 0.1024 - val_acc: 0.8781\n",
      "Epoch 78/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0986 - acc: 0.8849 - val_loss: 0.0971 - val_acc: 0.8892\n",
      "Epoch 79/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0990 - acc: 0.8833 - val_loss: 0.0975 - val_acc: 0.8917\n",
      "Epoch 80/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0959 - acc: 0.8896 - val_loss: 0.0953 - val_acc: 0.8900\n",
      "Epoch 81/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0953 - acc: 0.8895 - val_loss: 0.0928 - val_acc: 0.8832\n",
      "Epoch 82/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0942 - acc: 0.8895 - val_loss: 0.0902 - val_acc: 0.8951\n",
      "Epoch 83/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0937 - acc: 0.8917 - val_loss: 0.0980 - val_acc: 0.8713\n",
      "Epoch 84/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0935 - acc: 0.8919 - val_loss: 0.0971 - val_acc: 0.8815\n",
      "Epoch 85/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0924 - acc: 0.8940 - val_loss: 0.0955 - val_acc: 0.8849\n",
      "Epoch 86/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0919 - acc: 0.8943 - val_loss: 0.0877 - val_acc: 0.8849\n",
      "Epoch 87/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0914 - acc: 0.8944 - val_loss: 0.0909 - val_acc: 0.8866\n",
      "Epoch 88/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0902 - acc: 0.8970 - val_loss: 0.0880 - val_acc: 0.8943\n",
      "Epoch 89/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0893 - acc: 0.8980 - val_loss: 0.0850 - val_acc: 0.9037\n",
      "Epoch 90/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0875 - acc: 0.8966 - val_loss: 0.0833 - val_acc: 0.8960\n",
      "Epoch 91/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0877 - acc: 0.9005 - val_loss: 0.0834 - val_acc: 0.9105\n",
      "Epoch 92/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0866 - acc: 0.9013 - val_loss: 0.0849 - val_acc: 0.8986\n",
      "Epoch 93/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0857 - acc: 0.9032 - val_loss: 0.0838 - val_acc: 0.8986\n",
      "Epoch 94/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0856 - acc: 0.9019 - val_loss: 0.0793 - val_acc: 0.9173\n",
      "Epoch 95/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0852 - acc: 0.9039 - val_loss: 0.0873 - val_acc: 0.9003\n",
      "Epoch 96/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0844 - acc: 0.9055 - val_loss: 0.0888 - val_acc: 0.8960\n",
      "Epoch 97/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0838 - acc: 0.9051 - val_loss: 0.0815 - val_acc: 0.9062\n",
      "Epoch 98/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.0822 - acc: 0.9071 - val_loss: 0.0770 - val_acc: 0.9130\n",
      "Epoch 99/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.0815 - acc: 0.9068 - val_loss: 0.0774 - val_acc: 0.9096\n",
      "Epoch 100/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0812 - acc: 0.9069 - val_loss: 0.0776 - val_acc: 0.9182\n",
      "Epoch 101/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0799 - acc: 0.9127 - val_loss: 0.0800 - val_acc: 0.8968\n",
      "Epoch 102/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.0796 - acc: 0.9102 - val_loss: 0.0791 - val_acc: 0.9165\n",
      "Epoch 103/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0785 - acc: 0.9135 - val_loss: 0.0834 - val_acc: 0.9130\n",
      "Epoch 104/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0788 - acc: 0.9149 - val_loss: 0.0749 - val_acc: 0.9147\n",
      "Epoch 105/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0774 - acc: 0.9124 - val_loss: 0.0730 - val_acc: 0.9216\n",
      "Epoch 106/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0763 - acc: 0.9167 - val_loss: 0.0772 - val_acc: 0.9147\n",
      "Epoch 107/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0760 - acc: 0.9178 - val_loss: 0.0721 - val_acc: 0.9207\n",
      "Epoch 108/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0756 - acc: 0.9199 - val_loss: 0.0719 - val_acc: 0.9199\n",
      "Epoch 109/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0748 - acc: 0.9187 - val_loss: 0.0741 - val_acc: 0.9139\n",
      "Epoch 110/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0747 - acc: 0.9198 - val_loss: 0.0687 - val_acc: 0.9216\n",
      "Epoch 111/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0747 - acc: 0.9191 - val_loss: 0.0711 - val_acc: 0.9207\n",
      "Epoch 112/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0737 - acc: 0.9208 - val_loss: 0.0682 - val_acc: 0.9301\n",
      "Epoch 113/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0733 - acc: 0.9187 - val_loss: 0.0696 - val_acc: 0.9233\n",
      "Epoch 114/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0732 - acc: 0.9205 - val_loss: 0.0677 - val_acc: 0.9318\n",
      "Epoch 115/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0719 - acc: 0.9230 - val_loss: 0.0699 - val_acc: 0.9258\n",
      "Epoch 116/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0715 - acc: 0.9227 - val_loss: 0.0675 - val_acc: 0.9301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0714 - acc: 0.9237 - val_loss: 0.0719 - val_acc: 0.9224\n",
      "Epoch 118/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0701 - acc: 0.9250 - val_loss: 0.0669 - val_acc: 0.9258\n",
      "Epoch 119/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0686 - acc: 0.9286 - val_loss: 0.0665 - val_acc: 0.9250\n",
      "Epoch 120/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0690 - acc: 0.9251 - val_loss: 0.0646 - val_acc: 0.9335\n",
      "Epoch 121/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0695 - acc: 0.9253 - val_loss: 0.0690 - val_acc: 0.9301\n",
      "Epoch 122/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0675 - acc: 0.9292 - val_loss: 0.0712 - val_acc: 0.9224\n",
      "Epoch 123/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0683 - acc: 0.9302 - val_loss: 0.0627 - val_acc: 0.9395\n",
      "Epoch 124/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0677 - acc: 0.9285 - val_loss: 0.0624 - val_acc: 0.9369\n",
      "Epoch 125/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0675 - acc: 0.9303 - val_loss: 0.0646 - val_acc: 0.9309\n",
      "Epoch 126/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0668 - acc: 0.9320 - val_loss: 0.0639 - val_acc: 0.9361\n",
      "Epoch 127/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0665 - acc: 0.9310 - val_loss: 0.0618 - val_acc: 0.9335\n",
      "Epoch 128/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0657 - acc: 0.9334 - val_loss: 0.0631 - val_acc: 0.9378\n",
      "Epoch 129/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0641 - acc: 0.9347 - val_loss: 0.0633 - val_acc: 0.9335\n",
      "Epoch 130/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0652 - acc: 0.9328 - val_loss: 0.0647 - val_acc: 0.9352\n",
      "Epoch 131/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0649 - acc: 0.9330 - val_loss: 0.0600 - val_acc: 0.9412\n",
      "Epoch 132/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0633 - acc: 0.9361 - val_loss: 0.0600 - val_acc: 0.9403\n",
      "Epoch 133/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0622 - acc: 0.9388 - val_loss: 0.0636 - val_acc: 0.9361\n",
      "Epoch 134/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0622 - acc: 0.9373 - val_loss: 0.0576 - val_acc: 0.9386\n",
      "Epoch 135/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0615 - acc: 0.9393 - val_loss: 0.0545 - val_acc: 0.9480\n",
      "Epoch 136/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0614 - acc: 0.9388 - val_loss: 0.0536 - val_acc: 0.9488\n",
      "Epoch 137/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0616 - acc: 0.9384 - val_loss: 0.0577 - val_acc: 0.9403\n",
      "Epoch 138/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0599 - acc: 0.9403 - val_loss: 0.0574 - val_acc: 0.9454\n",
      "Epoch 139/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0615 - acc: 0.9385 - val_loss: 0.0547 - val_acc: 0.9488\n",
      "Epoch 140/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0602 - acc: 0.9418 - val_loss: 0.0579 - val_acc: 0.9454\n",
      "Epoch 141/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0594 - acc: 0.9410 - val_loss: 0.0619 - val_acc: 0.9369\n",
      "Epoch 142/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0596 - acc: 0.9406 - val_loss: 0.0584 - val_acc: 0.9437\n",
      "Epoch 143/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0586 - acc: 0.9425 - val_loss: 0.0546 - val_acc: 0.9471\n",
      "Epoch 144/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0590 - acc: 0.9429 - val_loss: 0.0554 - val_acc: 0.9506\n",
      "Epoch 145/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0577 - acc: 0.9448 - val_loss: 0.0524 - val_acc: 0.9471\n",
      "Epoch 146/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0578 - acc: 0.9456 - val_loss: 0.0545 - val_acc: 0.9497\n",
      "Epoch 147/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0580 - acc: 0.9439 - val_loss: 0.0564 - val_acc: 0.9471\n",
      "Epoch 148/500\n",
      "22272/22272 [==============================] - 16s 711us/step - loss: 0.0577 - acc: 0.9439 - val_loss: 0.0513 - val_acc: 0.9540\n",
      "Epoch 149/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0570 - acc: 0.9445 - val_loss: 0.0537 - val_acc: 0.9420\n",
      "Epoch 150/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0563 - acc: 0.9478 - val_loss: 0.0512 - val_acc: 0.9540\n",
      "Epoch 151/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0565 - acc: 0.9465 - val_loss: 0.0494 - val_acc: 0.9557\n",
      "Epoch 152/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0552 - acc: 0.9495 - val_loss: 0.0520 - val_acc: 0.9488\n",
      "Epoch 153/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0551 - acc: 0.9471 - val_loss: 0.0498 - val_acc: 0.9523\n",
      "Epoch 154/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0545 - acc: 0.9481 - val_loss: 0.0520 - val_acc: 0.9446\n",
      "Epoch 155/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0548 - acc: 0.9485 - val_loss: 0.0489 - val_acc: 0.9531\n",
      "Epoch 156/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.0545 - acc: 0.9472 - val_loss: 0.0473 - val_acc: 0.9540\n",
      "Epoch 157/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0538 - acc: 0.9500 - val_loss: 0.0486 - val_acc: 0.9557\n",
      "Epoch 158/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0539 - acc: 0.9498 - val_loss: 0.0576 - val_acc: 0.9420\n",
      "Epoch 159/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0538 - acc: 0.9498 - val_loss: 0.0481 - val_acc: 0.9599\n",
      "Epoch 160/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0528 - acc: 0.9539 - val_loss: 0.0494 - val_acc: 0.9514\n",
      "Epoch 161/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0517 - acc: 0.9535 - val_loss: 0.0481 - val_acc: 0.9548\n",
      "Epoch 162/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0533 - acc: 0.9512 - val_loss: 0.0460 - val_acc: 0.9557\n",
      "Epoch 163/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0521 - acc: 0.9546 - val_loss: 0.0471 - val_acc: 0.9582\n",
      "Epoch 164/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0520 - acc: 0.9528 - val_loss: 0.0468 - val_acc: 0.9548\n",
      "Epoch 165/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0525 - acc: 0.9508 - val_loss: 0.0469 - val_acc: 0.9557\n",
      "Epoch 166/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0505 - acc: 0.9541 - val_loss: 0.0457 - val_acc: 0.9574\n",
      "Epoch 167/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.0514 - acc: 0.9549 - val_loss: 0.0476 - val_acc: 0.9565\n",
      "Epoch 168/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.0503 - acc: 0.9555 - val_loss: 0.0456 - val_acc: 0.9540\n",
      "Epoch 169/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0503 - acc: 0.9543 - val_loss: 0.0470 - val_acc: 0.9523\n",
      "Epoch 170/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0504 - acc: 0.9543 - val_loss: 0.0468 - val_acc: 0.9565\n",
      "Epoch 171/500\n",
      "22272/22272 [==============================] - 16s 714us/step - loss: 0.0499 - acc: 0.9560 - val_loss: 0.0461 - val_acc: 0.9548\n",
      "Epoch 172/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0495 - acc: 0.9563 - val_loss: 0.0482 - val_acc: 0.9557\n",
      "Epoch 173/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0497 - acc: 0.9551 - val_loss: 0.0432 - val_acc: 0.9650\n",
      "Epoch 174/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0496 - acc: 0.9550 - val_loss: 0.0427 - val_acc: 0.9591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0482 - acc: 0.9589 - val_loss: 0.0455 - val_acc: 0.9548\n",
      "Epoch 176/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0484 - acc: 0.9575 - val_loss: 0.0477 - val_acc: 0.9531\n",
      "Epoch 177/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0491 - acc: 0.9555 - val_loss: 0.0436 - val_acc: 0.9591\n",
      "Epoch 178/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0491 - acc: 0.9577 - val_loss: 0.0479 - val_acc: 0.9497\n",
      "Epoch 179/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0479 - acc: 0.9582 - val_loss: 0.0470 - val_acc: 0.9565\n",
      "Epoch 180/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0471 - acc: 0.9578 - val_loss: 0.0428 - val_acc: 0.9582\n",
      "Epoch 181/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0471 - acc: 0.9592 - val_loss: 0.0446 - val_acc: 0.9557\n",
      "Epoch 182/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0472 - acc: 0.9593 - val_loss: 0.0421 - val_acc: 0.9650\n",
      "Epoch 183/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0465 - acc: 0.9608 - val_loss: 0.0411 - val_acc: 0.9616\n",
      "Epoch 184/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0464 - acc: 0.9601 - val_loss: 0.0419 - val_acc: 0.9599\n",
      "Epoch 185/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0457 - acc: 0.9614 - val_loss: 0.0415 - val_acc: 0.9599\n",
      "Epoch 186/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0458 - acc: 0.9613 - val_loss: 0.0431 - val_acc: 0.9565\n",
      "Epoch 187/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0454 - acc: 0.9613 - val_loss: 0.0397 - val_acc: 0.9633\n",
      "Epoch 188/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0460 - acc: 0.9604 - val_loss: 0.0424 - val_acc: 0.9599\n",
      "Epoch 189/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0449 - acc: 0.9642 - val_loss: 0.0392 - val_acc: 0.9616\n",
      "Epoch 190/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0440 - acc: 0.9651 - val_loss: 0.0390 - val_acc: 0.9642\n",
      "Epoch 191/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0446 - acc: 0.9639 - val_loss: 0.0414 - val_acc: 0.9591\n",
      "Epoch 192/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0448 - acc: 0.9638 - val_loss: 0.0384 - val_acc: 0.9676\n",
      "Epoch 193/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0439 - acc: 0.9638 - val_loss: 0.0398 - val_acc: 0.9608\n",
      "Epoch 194/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0440 - acc: 0.9633 - val_loss: 0.0389 - val_acc: 0.9702\n",
      "Epoch 195/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0432 - acc: 0.9645 - val_loss: 0.0406 - val_acc: 0.9625\n",
      "Epoch 196/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0438 - acc: 0.9652 - val_loss: 0.0394 - val_acc: 0.9650\n",
      "Epoch 197/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0428 - acc: 0.9656 - val_loss: 0.0387 - val_acc: 0.9642\n",
      "Epoch 198/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0439 - acc: 0.9636 - val_loss: 0.0394 - val_acc: 0.9650\n",
      "Epoch 199/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0438 - acc: 0.9631 - val_loss: 0.0395 - val_acc: 0.9668\n",
      "Epoch 200/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0433 - acc: 0.9639 - val_loss: 0.0363 - val_acc: 0.9659\n",
      "Epoch 201/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0429 - acc: 0.9644 - val_loss: 0.0367 - val_acc: 0.9736\n",
      "Epoch 202/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0422 - acc: 0.9659 - val_loss: 0.0364 - val_acc: 0.9736\n",
      "Epoch 203/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0421 - acc: 0.9665 - val_loss: 0.0361 - val_acc: 0.9702\n",
      "Epoch 204/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0426 - acc: 0.9647 - val_loss: 0.0362 - val_acc: 0.9650\n",
      "Epoch 205/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0421 - acc: 0.9679 - val_loss: 0.0389 - val_acc: 0.9608\n",
      "Epoch 206/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0410 - acc: 0.9673 - val_loss: 0.0373 - val_acc: 0.9702\n",
      "Epoch 207/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0413 - acc: 0.9671 - val_loss: 0.0398 - val_acc: 0.9608\n",
      "Epoch 208/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0409 - acc: 0.9658 - val_loss: 0.0363 - val_acc: 0.9685\n",
      "Epoch 209/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0419 - acc: 0.9668 - val_loss: 0.0364 - val_acc: 0.9702\n",
      "Epoch 210/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0409 - acc: 0.9672 - val_loss: 0.0382 - val_acc: 0.9710\n",
      "Epoch 211/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.0409 - acc: 0.9679 - val_loss: 0.0337 - val_acc: 0.9702\n",
      "Epoch 212/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0406 - acc: 0.9658 - val_loss: 0.0355 - val_acc: 0.9642\n",
      "Epoch 213/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0407 - acc: 0.9682 - val_loss: 0.0365 - val_acc: 0.9702\n",
      "Epoch 214/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.0403 - acc: 0.9694 - val_loss: 0.0390 - val_acc: 0.9642\n",
      "Epoch 215/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0408 - acc: 0.9687 - val_loss: 0.0390 - val_acc: 0.9676\n",
      "Epoch 216/500\n",
      "22272/22272 [==============================] - 16s 718us/step - loss: 0.0404 - acc: 0.9682 - val_loss: 0.0341 - val_acc: 0.9719\n",
      "Epoch 217/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0396 - acc: 0.9694 - val_loss: 0.0360 - val_acc: 0.9668\n",
      "Epoch 218/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.0390 - acc: 0.9697 - val_loss: 0.0350 - val_acc: 0.9668\n",
      "Epoch 219/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.0402 - acc: 0.9670 - val_loss: 0.0334 - val_acc: 0.9702\n",
      "Epoch 220/500\n",
      "22272/22272 [==============================] - 16s 714us/step - loss: 0.0391 - acc: 0.9699 - val_loss: 0.0334 - val_acc: 0.9727\n",
      "Epoch 221/500\n",
      "22272/22272 [==============================] - 16s 711us/step - loss: 0.0386 - acc: 0.9714 - val_loss: 0.0355 - val_acc: 0.9693\n",
      "Epoch 222/500\n",
      "22272/22272 [==============================] - 16s 721us/step - loss: 0.0388 - acc: 0.9692 - val_loss: 0.0354 - val_acc: 0.9702\n",
      "Epoch 223/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0387 - acc: 0.9709 - val_loss: 0.0330 - val_acc: 0.9685\n",
      "Epoch 224/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0393 - acc: 0.9690 - val_loss: 0.0356 - val_acc: 0.9710\n",
      "Epoch 225/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.0384 - acc: 0.9701 - val_loss: 0.0323 - val_acc: 0.9753\n",
      "Epoch 226/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0385 - acc: 0.9694 - val_loss: 0.0330 - val_acc: 0.9719\n",
      "Epoch 227/500\n",
      "22272/22272 [==============================] - 16s 718us/step - loss: 0.0380 - acc: 0.9711 - val_loss: 0.0320 - val_acc: 0.9770\n",
      "Epoch 228/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0379 - acc: 0.9698 - val_loss: 0.0368 - val_acc: 0.9685\n",
      "Epoch 229/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0387 - acc: 0.9677 - val_loss: 0.0332 - val_acc: 0.9727\n",
      "Epoch 230/500\n",
      "22272/22272 [==============================] - 16s 717us/step - loss: 0.0381 - acc: 0.9703 - val_loss: 0.0321 - val_acc: 0.9744\n",
      "Epoch 231/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.0375 - acc: 0.9727 - val_loss: 0.0312 - val_acc: 0.9770\n",
      "Epoch 232/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.0379 - acc: 0.9701 - val_loss: 0.0382 - val_acc: 0.9668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0374 - acc: 0.9728 - val_loss: 0.0323 - val_acc: 0.9753\n",
      "Epoch 234/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0367 - acc: 0.9728 - val_loss: 0.0318 - val_acc: 0.9761\n",
      "Epoch 235/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0374 - acc: 0.9712 - val_loss: 0.0332 - val_acc: 0.9719\n",
      "Epoch 236/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0376 - acc: 0.9732 - val_loss: 0.0362 - val_acc: 0.9574\n",
      "Epoch 237/500\n",
      "22272/22272 [==============================] - 16s 715us/step - loss: 0.0369 - acc: 0.9715 - val_loss: 0.0327 - val_acc: 0.9693\n",
      "Epoch 238/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0367 - acc: 0.9722 - val_loss: 0.0318 - val_acc: 0.9710\n",
      "Epoch 239/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0358 - acc: 0.9732 - val_loss: 0.0305 - val_acc: 0.9710\n",
      "Epoch 240/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0361 - acc: 0.9738 - val_loss: 0.0328 - val_acc: 0.9736\n",
      "Epoch 241/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0363 - acc: 0.9723 - val_loss: 0.0303 - val_acc: 0.9744\n",
      "Epoch 242/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0357 - acc: 0.9719 - val_loss: 0.0336 - val_acc: 0.9753\n",
      "Epoch 243/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0365 - acc: 0.9733 - val_loss: 0.0354 - val_acc: 0.9676\n",
      "Epoch 244/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0358 - acc: 0.9740 - val_loss: 0.0307 - val_acc: 0.9787\n",
      "Epoch 245/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0360 - acc: 0.9734 - val_loss: 0.0305 - val_acc: 0.9787\n",
      "Epoch 246/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0361 - acc: 0.9723 - val_loss: 0.0310 - val_acc: 0.9744\n",
      "Epoch 247/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0355 - acc: 0.9722 - val_loss: 0.0281 - val_acc: 0.9787\n",
      "Epoch 248/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0353 - acc: 0.9734 - val_loss: 0.0305 - val_acc: 0.9719\n",
      "Epoch 249/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0349 - acc: 0.9737 - val_loss: 0.0333 - val_acc: 0.9753\n",
      "Epoch 250/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0356 - acc: 0.9734 - val_loss: 0.0323 - val_acc: 0.9727\n",
      "Epoch 251/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0353 - acc: 0.9727 - val_loss: 0.0293 - val_acc: 0.9795\n",
      "Epoch 252/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0351 - acc: 0.9737 - val_loss: 0.0314 - val_acc: 0.9761\n",
      "Epoch 253/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0347 - acc: 0.9743 - val_loss: 0.0290 - val_acc: 0.9795\n",
      "Epoch 254/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0350 - acc: 0.9741 - val_loss: 0.0341 - val_acc: 0.9650\n",
      "Epoch 255/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0344 - acc: 0.9733 - val_loss: 0.0305 - val_acc: 0.9719\n",
      "Epoch 256/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0341 - acc: 0.9754 - val_loss: 0.0329 - val_acc: 0.9693\n",
      "Epoch 257/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0344 - acc: 0.9735 - val_loss: 0.0293 - val_acc: 0.9770\n",
      "Epoch 258/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0341 - acc: 0.9759 - val_loss: 0.0308 - val_acc: 0.9719\n",
      "Epoch 259/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0337 - acc: 0.9765 - val_loss: 0.0291 - val_acc: 0.9795\n",
      "Epoch 260/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0340 - acc: 0.9751 - val_loss: 0.0304 - val_acc: 0.9744\n",
      "Epoch 261/500\n",
      "22272/22272 [==============================] - 16s 710us/step - loss: 0.0338 - acc: 0.9754 - val_loss: 0.0297 - val_acc: 0.9753\n",
      "Epoch 262/500\n",
      "22272/22272 [==============================] - 16s 727us/step - loss: 0.0339 - acc: 0.9759 - val_loss: 0.0303 - val_acc: 0.9744\n",
      "Epoch 263/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0334 - acc: 0.9755 - val_loss: 0.0283 - val_acc: 0.9761\n",
      "Epoch 264/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0334 - acc: 0.9762 - val_loss: 0.0287 - val_acc: 0.9778\n",
      "Epoch 265/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0341 - acc: 0.9745 - val_loss: 0.0290 - val_acc: 0.9761\n",
      "Epoch 266/500\n",
      "22272/22272 [==============================] - 16s 710us/step - loss: 0.0336 - acc: 0.9754 - val_loss: 0.0285 - val_acc: 0.9761\n",
      "Epoch 267/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0336 - acc: 0.9751 - val_loss: 0.0296 - val_acc: 0.9753\n",
      "Epoch 268/500\n",
      "22272/22272 [==============================] - 16s 710us/step - loss: 0.0332 - acc: 0.9750 - val_loss: 0.0285 - val_acc: 0.9778\n",
      "Epoch 269/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.0328 - acc: 0.9757 - val_loss: 0.0303 - val_acc: 0.9753\n",
      "Epoch 270/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.0336 - acc: 0.9743 - val_loss: 0.0298 - val_acc: 0.9753\n",
      "Epoch 271/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0329 - acc: 0.9749 - val_loss: 0.0289 - val_acc: 0.9736\n",
      "Epoch 272/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0336 - acc: 0.9744 - val_loss: 0.0286 - val_acc: 0.9753\n",
      "Epoch 273/500\n",
      "22272/22272 [==============================] - 16s 710us/step - loss: 0.0333 - acc: 0.9755 - val_loss: 0.0253 - val_acc: 0.9778\n",
      "Epoch 274/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0326 - acc: 0.9761 - val_loss: 0.0280 - val_acc: 0.9804\n",
      "Epoch 275/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0324 - acc: 0.9765 - val_loss: 0.0269 - val_acc: 0.9770\n",
      "Epoch 276/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0327 - acc: 0.9756 - val_loss: 0.0270 - val_acc: 0.9761\n",
      "Epoch 277/500\n",
      "22272/22272 [==============================] - 16s 717us/step - loss: 0.0325 - acc: 0.9770 - val_loss: 0.0277 - val_acc: 0.9812\n",
      "Epoch 278/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0319 - acc: 0.9774 - val_loss: 0.0287 - val_acc: 0.9795\n",
      "Epoch 279/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0327 - acc: 0.9767 - val_loss: 0.0257 - val_acc: 0.9761\n",
      "Epoch 280/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0325 - acc: 0.9759 - val_loss: 0.0275 - val_acc: 0.9787\n",
      "Epoch 281/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0325 - acc: 0.9765 - val_loss: 0.0312 - val_acc: 0.9744\n",
      "Epoch 282/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0325 - acc: 0.9765 - val_loss: 0.0257 - val_acc: 0.9804\n",
      "Epoch 283/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.0318 - acc: 0.9772 - val_loss: 0.0271 - val_acc: 0.9778\n",
      "Epoch 284/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.0317 - acc: 0.9767 - val_loss: 0.0262 - val_acc: 0.9795\n",
      "Epoch 285/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0313 - acc: 0.9769 - val_loss: 0.0273 - val_acc: 0.9804\n",
      "Epoch 286/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.0320 - acc: 0.9760 - val_loss: 0.0266 - val_acc: 0.9795\n",
      "Epoch 287/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.0319 - acc: 0.9754 - val_loss: 0.0281 - val_acc: 0.9761\n",
      "Epoch 288/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0316 - acc: 0.9771 - val_loss: 0.0255 - val_acc: 0.9804\n",
      "Epoch 289/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0314 - acc: 0.9775 - val_loss: 0.0281 - val_acc: 0.9719\n",
      "Epoch 290/500\n",
      "22272/22272 [==============================] - 16s 710us/step - loss: 0.0309 - acc: 0.9772 - val_loss: 0.0263 - val_acc: 0.9829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/500\n",
      "22272/22272 [==============================] - 16s 736us/step - loss: 0.0312 - acc: 0.9783 - val_loss: 0.0275 - val_acc: 0.9795\n",
      "Epoch 292/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0309 - acc: 0.9777 - val_loss: 0.0264 - val_acc: 0.9829\n",
      "Epoch 293/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0311 - acc: 0.9766 - val_loss: 0.0265 - val_acc: 0.9804\n",
      "Epoch 294/500\n",
      "22272/22272 [==============================] - 15s 689us/step - loss: 0.0306 - acc: 0.9804 - val_loss: 0.0256 - val_acc: 0.9821\n",
      "Epoch 295/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0312 - acc: 0.9764 - val_loss: 0.0248 - val_acc: 0.9812\n",
      "Epoch 296/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0307 - acc: 0.9767 - val_loss: 0.0260 - val_acc: 0.9804\n",
      "Epoch 297/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0306 - acc: 0.9798 - val_loss: 0.0268 - val_acc: 0.9787\n",
      "Epoch 298/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0301 - acc: 0.9790 - val_loss: 0.0247 - val_acc: 0.9821\n",
      "Epoch 299/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0302 - acc: 0.9787 - val_loss: 0.0274 - val_acc: 0.9753\n",
      "Epoch 300/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0307 - acc: 0.9770 - val_loss: 0.0248 - val_acc: 0.9787\n",
      "Epoch 301/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0299 - acc: 0.9796 - val_loss: 0.0261 - val_acc: 0.9761\n",
      "Epoch 302/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0305 - acc: 0.9778 - val_loss: 0.0261 - val_acc: 0.9770\n",
      "Epoch 303/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0302 - acc: 0.9766 - val_loss: 0.0261 - val_acc: 0.9710\n",
      "Epoch 304/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0304 - acc: 0.9781 - val_loss: 0.0262 - val_acc: 0.9795\n",
      "Epoch 305/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0304 - acc: 0.9781 - val_loss: 0.0276 - val_acc: 0.9804\n",
      "Epoch 306/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0301 - acc: 0.9797 - val_loss: 0.0255 - val_acc: 0.9838\n",
      "Epoch 307/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.0299 - acc: 0.9782 - val_loss: 0.0266 - val_acc: 0.9770\n",
      "Epoch 308/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0297 - acc: 0.9791 - val_loss: 0.0274 - val_acc: 0.9770\n",
      "Epoch 309/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0302 - acc: 0.9778 - val_loss: 0.0252 - val_acc: 0.9812\n",
      "Epoch 310/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0299 - acc: 0.9792 - val_loss: 0.0264 - val_acc: 0.9795\n",
      "Epoch 311/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0297 - acc: 0.9803 - val_loss: 0.0252 - val_acc: 0.9795\n",
      "Epoch 312/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0302 - acc: 0.9780 - val_loss: 0.0265 - val_acc: 0.9804\n",
      "Epoch 313/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0295 - acc: 0.9789 - val_loss: 0.0248 - val_acc: 0.9821\n",
      "Epoch 314/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0297 - acc: 0.9793 - val_loss: 0.0237 - val_acc: 0.9795\n",
      "Epoch 315/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0296 - acc: 0.9777 - val_loss: 0.0266 - val_acc: 0.9778\n",
      "Epoch 316/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0293 - acc: 0.9807 - val_loss: 0.0244 - val_acc: 0.9804\n",
      "Epoch 317/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0292 - acc: 0.9797 - val_loss: 0.0265 - val_acc: 0.9770\n",
      "Epoch 318/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0296 - acc: 0.9786 - val_loss: 0.0274 - val_acc: 0.9770\n",
      "Epoch 319/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0297 - acc: 0.9784 - val_loss: 0.0260 - val_acc: 0.9795\n",
      "Epoch 320/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0291 - acc: 0.9793 - val_loss: 0.0249 - val_acc: 0.9787\n",
      "Epoch 321/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0290 - acc: 0.9804 - val_loss: 0.0239 - val_acc: 0.9847\n",
      "Epoch 322/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0291 - acc: 0.9799 - val_loss: 0.0248 - val_acc: 0.9829\n",
      "Epoch 323/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0293 - acc: 0.9801 - val_loss: 0.0251 - val_acc: 0.9787\n",
      "Epoch 324/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0285 - acc: 0.9799 - val_loss: 0.0261 - val_acc: 0.9838\n",
      "Epoch 325/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0293 - acc: 0.9797 - val_loss: 0.0234 - val_acc: 0.9838\n",
      "Epoch 326/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0288 - acc: 0.9811 - val_loss: 0.0259 - val_acc: 0.9778\n",
      "Epoch 327/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0284 - acc: 0.9800 - val_loss: 0.0224 - val_acc: 0.9804\n",
      "Epoch 328/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0290 - acc: 0.9801 - val_loss: 0.0234 - val_acc: 0.9821\n",
      "Epoch 329/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.0287 - acc: 0.9799 - val_loss: 0.0238 - val_acc: 0.9829\n",
      "Epoch 330/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0285 - acc: 0.9804 - val_loss: 0.0248 - val_acc: 0.9812\n",
      "Epoch 331/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0285 - acc: 0.9803 - val_loss: 0.0237 - val_acc: 0.9829\n",
      "Epoch 332/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.0281 - acc: 0.9809 - val_loss: 0.0258 - val_acc: 0.9855\n",
      "Epoch 333/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0284 - acc: 0.9806 - val_loss: 0.0258 - val_acc: 0.9812\n",
      "Epoch 334/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0287 - acc: 0.9803 - val_loss: 0.0243 - val_acc: 0.9778\n",
      "Epoch 335/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0279 - acc: 0.9802 - val_loss: 0.0249 - val_acc: 0.9804\n",
      "Epoch 336/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.0286 - acc: 0.9803 - val_loss: 0.0237 - val_acc: 0.9812\n",
      "Epoch 337/500\n",
      "22272/22272 [==============================] - 16s 711us/step - loss: 0.0289 - acc: 0.9803 - val_loss: 0.0232 - val_acc: 0.9812\n",
      "Epoch 338/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0278 - acc: 0.9807 - val_loss: 0.0236 - val_acc: 0.9804\n",
      "Epoch 339/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0284 - acc: 0.9797 - val_loss: 0.0253 - val_acc: 0.9778\n",
      "Epoch 340/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0283 - acc: 0.9811 - val_loss: 0.0243 - val_acc: 0.9787\n",
      "Epoch 341/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0277 - acc: 0.9817 - val_loss: 0.0225 - val_acc: 0.9847\n",
      "Epoch 342/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0275 - acc: 0.9800 - val_loss: 0.0220 - val_acc: 0.9847\n",
      "Epoch 343/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0275 - acc: 0.9801 - val_loss: 0.0239 - val_acc: 0.9778\n",
      "Epoch 344/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.0275 - acc: 0.9808 - val_loss: 0.0236 - val_acc: 0.9821\n",
      "Epoch 345/500\n",
      "22272/22272 [==============================] - 16s 711us/step - loss: 0.0272 - acc: 0.9805 - val_loss: 0.0226 - val_acc: 0.9821\n",
      "Epoch 346/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0275 - acc: 0.9805 - val_loss: 0.0241 - val_acc: 0.9812\n",
      "Epoch 347/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0280 - acc: 0.9795 - val_loss: 0.0226 - val_acc: 0.9821\n",
      "Epoch 348/500\n",
      "22272/22272 [==============================] - 16s 715us/step - loss: 0.0273 - acc: 0.9817 - val_loss: 0.0232 - val_acc: 0.9804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0278 - acc: 0.9803 - val_loss: 0.0243 - val_acc: 0.9812\n",
      "Epoch 350/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0271 - acc: 0.9813 - val_loss: 0.0238 - val_acc: 0.9847\n",
      "Epoch 351/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0272 - acc: 0.9812 - val_loss: 0.0230 - val_acc: 0.9812\n",
      "Epoch 352/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.0274 - acc: 0.9816 - val_loss: 0.0226 - val_acc: 0.9812\n",
      "Epoch 353/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0273 - acc: 0.9812 - val_loss: 0.0235 - val_acc: 0.9804\n",
      "Epoch 354/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0271 - acc: 0.9812 - val_loss: 0.0222 - val_acc: 0.9829\n",
      "Epoch 355/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0277 - acc: 0.9810 - val_loss: 0.0257 - val_acc: 0.9770\n",
      "Epoch 356/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0271 - acc: 0.9810 - val_loss: 0.0226 - val_acc: 0.9821\n",
      "Epoch 357/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0271 - acc: 0.9799 - val_loss: 0.0219 - val_acc: 0.9847\n",
      "Epoch 358/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0274 - acc: 0.9799 - val_loss: 0.0230 - val_acc: 0.9821\n",
      "Epoch 359/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0271 - acc: 0.9811 - val_loss: 0.0231 - val_acc: 0.9812\n",
      "Epoch 360/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0265 - acc: 0.9814 - val_loss: 0.0224 - val_acc: 0.9855\n",
      "Epoch 361/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0273 - acc: 0.9818 - val_loss: 0.0240 - val_acc: 0.9795\n",
      "Epoch 362/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0270 - acc: 0.9815 - val_loss: 0.0226 - val_acc: 0.9821\n",
      "Epoch 363/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0265 - acc: 0.9821 - val_loss: 0.0205 - val_acc: 0.9881\n",
      "Epoch 364/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0268 - acc: 0.9804 - val_loss: 0.0207 - val_acc: 0.9838\n",
      "Epoch 365/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0262 - acc: 0.9823 - val_loss: 0.0220 - val_acc: 0.9821\n",
      "Epoch 366/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0262 - acc: 0.9820 - val_loss: 0.0227 - val_acc: 0.9838\n",
      "Epoch 367/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0262 - acc: 0.9803 - val_loss: 0.0223 - val_acc: 0.9821\n",
      "Epoch 368/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0264 - acc: 0.9819 - val_loss: 0.0216 - val_acc: 0.9829\n",
      "Epoch 369/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0266 - acc: 0.9811 - val_loss: 0.0218 - val_acc: 0.9812\n",
      "Epoch 370/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0267 - acc: 0.9813 - val_loss: 0.0215 - val_acc: 0.9795\n",
      "Epoch 371/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0266 - acc: 0.9803 - val_loss: 0.0214 - val_acc: 0.9829\n",
      "Epoch 372/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0259 - acc: 0.9817 - val_loss: 0.0206 - val_acc: 0.9864\n",
      "Epoch 373/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0261 - acc: 0.9811 - val_loss: 0.0216 - val_acc: 0.9821\n",
      "Epoch 374/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.0258 - acc: 0.9830 - val_loss: 0.0223 - val_acc: 0.9847\n",
      "Epoch 375/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0258 - acc: 0.9825 - val_loss: 0.0242 - val_acc: 0.9829\n",
      "Epoch 376/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0262 - acc: 0.9820 - val_loss: 0.0218 - val_acc: 0.9881\n",
      "Epoch 377/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0263 - acc: 0.9827 - val_loss: 0.0219 - val_acc: 0.9855\n",
      "Epoch 378/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0264 - acc: 0.9823 - val_loss: 0.0210 - val_acc: 0.9855\n",
      "Epoch 379/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0262 - acc: 0.9813 - val_loss: 0.0207 - val_acc: 0.9778\n",
      "Epoch 380/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0257 - acc: 0.9830 - val_loss: 0.0207 - val_acc: 0.9855\n",
      "Epoch 381/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0257 - acc: 0.9813 - val_loss: 0.0237 - val_acc: 0.9812\n",
      "Epoch 382/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0260 - acc: 0.9828 - val_loss: 0.0217 - val_acc: 0.9821\n",
      "Epoch 383/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0263 - acc: 0.9818 - val_loss: 0.0214 - val_acc: 0.9847\n",
      "Epoch 384/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0261 - acc: 0.9823 - val_loss: 0.0203 - val_acc: 0.9847\n",
      "Epoch 385/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0254 - acc: 0.9828 - val_loss: 0.0206 - val_acc: 0.9821\n",
      "Epoch 386/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0254 - acc: 0.9820 - val_loss: 0.0228 - val_acc: 0.9821\n",
      "Epoch 387/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.0256 - acc: 0.9824 - val_loss: 0.0214 - val_acc: 0.9829\n",
      "Epoch 388/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0254 - acc: 0.9822 - val_loss: 0.0209 - val_acc: 0.9821\n",
      "Epoch 389/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0261 - acc: 0.9820 - val_loss: 0.0204 - val_acc: 0.9864\n",
      "Epoch 390/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0254 - acc: 0.9833 - val_loss: 0.0207 - val_acc: 0.9812\n",
      "Epoch 391/500\n",
      "22272/22272 [==============================] - 16s 717us/step - loss: 0.0255 - acc: 0.9819 - val_loss: 0.0231 - val_acc: 0.9804\n",
      "Epoch 392/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0251 - acc: 0.9819 - val_loss: 0.0217 - val_acc: 0.9795\n",
      "Epoch 393/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0252 - acc: 0.9827 - val_loss: 0.0217 - val_acc: 0.9829\n",
      "Epoch 394/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0256 - acc: 0.9822 - val_loss: 0.0202 - val_acc: 0.9847\n",
      "Epoch 395/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0264 - acc: 0.9807 - val_loss: 0.0203 - val_acc: 0.9812\n",
      "Epoch 396/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0255 - acc: 0.9822 - val_loss: 0.0196 - val_acc: 0.9855\n",
      "Epoch 397/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0250 - acc: 0.9832 - val_loss: 0.0222 - val_acc: 0.9838\n",
      "Epoch 398/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0255 - acc: 0.9825 - val_loss: 0.0217 - val_acc: 0.9829\n",
      "Epoch 399/500\n",
      "22272/22272 [==============================] - 16s 711us/step - loss: 0.0251 - acc: 0.9837 - val_loss: 0.0227 - val_acc: 0.9847\n",
      "Epoch 400/500\n",
      "22272/22272 [==============================] - 16s 710us/step - loss: 0.0252 - acc: 0.9820 - val_loss: 0.0197 - val_acc: 0.9821\n",
      "Epoch 401/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0253 - acc: 0.9834 - val_loss: 0.0205 - val_acc: 0.9838\n",
      "Epoch 402/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0250 - acc: 0.9824 - val_loss: 0.0214 - val_acc: 0.9812\n",
      "Epoch 403/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0250 - acc: 0.9831 - val_loss: 0.0208 - val_acc: 0.9821\n",
      "Epoch 404/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0248 - acc: 0.9826 - val_loss: 0.0199 - val_acc: 0.9829\n",
      "Epoch 405/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0250 - acc: 0.9834 - val_loss: 0.0216 - val_acc: 0.9804\n",
      "Epoch 406/500\n",
      "22272/22272 [==============================] - 16s 710us/step - loss: 0.0250 - acc: 0.9813 - val_loss: 0.0201 - val_acc: 0.9821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 407/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0252 - acc: 0.9819 - val_loss: 0.0212 - val_acc: 0.9795\n",
      "Epoch 408/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0247 - acc: 0.9823 - val_loss: 0.0212 - val_acc: 0.9821\n",
      "Epoch 409/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0246 - acc: 0.9833 - val_loss: 0.0198 - val_acc: 0.9838\n",
      "Epoch 410/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0250 - acc: 0.9822 - val_loss: 0.0225 - val_acc: 0.9821\n",
      "Epoch 411/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0248 - acc: 0.9826 - val_loss: 0.0202 - val_acc: 0.9855\n",
      "Epoch 412/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0249 - acc: 0.9842 - val_loss: 0.0208 - val_acc: 0.9804\n",
      "Epoch 413/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0243 - acc: 0.9842 - val_loss: 0.0192 - val_acc: 0.9855\n",
      "Epoch 414/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0246 - acc: 0.9825 - val_loss: 0.0201 - val_acc: 0.9838\n",
      "Epoch 415/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0243 - acc: 0.9840 - val_loss: 0.0198 - val_acc: 0.9889\n",
      "Epoch 416/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0246 - acc: 0.9833 - val_loss: 0.0216 - val_acc: 0.9821\n",
      "Epoch 417/500\n",
      "22272/22272 [==============================] - 16s 696us/step - loss: 0.0242 - acc: 0.9827 - val_loss: 0.0193 - val_acc: 0.9847\n",
      "Epoch 418/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0243 - acc: 0.9841 - val_loss: 0.0198 - val_acc: 0.9881\n",
      "Epoch 419/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0244 - acc: 0.9837 - val_loss: 0.0184 - val_acc: 0.9829\n",
      "Epoch 420/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0239 - acc: 0.9838 - val_loss: 0.0192 - val_acc: 0.9881\n",
      "Epoch 421/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0243 - acc: 0.9834 - val_loss: 0.0194 - val_acc: 0.9881\n",
      "Epoch 422/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0244 - acc: 0.9821 - val_loss: 0.0197 - val_acc: 0.9855\n",
      "Epoch 423/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0244 - acc: 0.9831 - val_loss: 0.0202 - val_acc: 0.9787\n",
      "Epoch 424/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0242 - acc: 0.9842 - val_loss: 0.0203 - val_acc: 0.9829\n",
      "Epoch 425/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0240 - acc: 0.9825 - val_loss: 0.0198 - val_acc: 0.9864\n",
      "Epoch 426/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0243 - acc: 0.9822 - val_loss: 0.0202 - val_acc: 0.9829\n",
      "Epoch 427/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0241 - acc: 0.9831 - val_loss: 0.0198 - val_acc: 0.9864\n",
      "Epoch 428/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0242 - acc: 0.9843 - val_loss: 0.0199 - val_acc: 0.9881\n",
      "Epoch 429/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0239 - acc: 0.9840 - val_loss: 0.0192 - val_acc: 0.9838\n",
      "Epoch 430/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0243 - acc: 0.9837 - val_loss: 0.0198 - val_acc: 0.9838\n",
      "Epoch 431/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0239 - acc: 0.9850 - val_loss: 0.0206 - val_acc: 0.9881\n",
      "Epoch 432/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0241 - acc: 0.9846 - val_loss: 0.0223 - val_acc: 0.9795\n",
      "Epoch 433/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0241 - acc: 0.9846 - val_loss: 0.0182 - val_acc: 0.9855\n",
      "Epoch 434/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0236 - acc: 0.9844 - val_loss: 0.0206 - val_acc: 0.9872\n",
      "Epoch 435/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0236 - acc: 0.9828 - val_loss: 0.0201 - val_acc: 0.9829\n",
      "Epoch 436/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0239 - acc: 0.9819 - val_loss: 0.0190 - val_acc: 0.9847\n",
      "Epoch 437/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0236 - acc: 0.9832 - val_loss: 0.0192 - val_acc: 0.9864\n",
      "Epoch 438/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.0238 - acc: 0.9844 - val_loss: 0.0215 - val_acc: 0.9829\n",
      "Epoch 439/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0236 - acc: 0.9831 - val_loss: 0.0210 - val_acc: 0.9821\n",
      "Epoch 440/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0238 - acc: 0.9834 - val_loss: 0.0196 - val_acc: 0.9847\n",
      "Epoch 441/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0237 - acc: 0.9833 - val_loss: 0.0192 - val_acc: 0.9795\n",
      "Epoch 442/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0237 - acc: 0.9841 - val_loss: 0.0181 - val_acc: 0.9847\n",
      "Epoch 443/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0234 - acc: 0.9837 - val_loss: 0.0205 - val_acc: 0.9855\n",
      "Epoch 444/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0237 - acc: 0.9832 - val_loss: 0.0209 - val_acc: 0.9847\n",
      "Epoch 445/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0240 - acc: 0.9840 - val_loss: 0.0193 - val_acc: 0.9812\n",
      "Epoch 446/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0238 - acc: 0.9843 - val_loss: 0.0189 - val_acc: 0.9838\n",
      "Epoch 447/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0239 - acc: 0.9828 - val_loss: 0.0201 - val_acc: 0.9787\n",
      "Epoch 448/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0238 - acc: 0.9847 - val_loss: 0.0186 - val_acc: 0.9829\n",
      "Epoch 449/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0232 - acc: 0.9845 - val_loss: 0.0188 - val_acc: 0.9829\n",
      "Epoch 450/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0234 - acc: 0.9833 - val_loss: 0.0184 - val_acc: 0.9829\n",
      "Epoch 451/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0235 - acc: 0.9836 - val_loss: 0.0177 - val_acc: 0.9889\n",
      "Epoch 452/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0236 - acc: 0.9837 - val_loss: 0.0187 - val_acc: 0.9889\n",
      "Epoch 453/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0231 - acc: 0.9847 - val_loss: 0.0181 - val_acc: 0.9855\n",
      "Epoch 454/500\n",
      "22272/22272 [==============================] - 16s 716us/step - loss: 0.0236 - acc: 0.9841 - val_loss: 0.0174 - val_acc: 0.9881\n",
      "Epoch 455/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0232 - acc: 0.9842 - val_loss: 0.0185 - val_acc: 0.9855\n",
      "Epoch 456/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0231 - acc: 0.9851 - val_loss: 0.0203 - val_acc: 0.9795\n",
      "Epoch 457/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0235 - acc: 0.9841 - val_loss: 0.0186 - val_acc: 0.9821\n",
      "Epoch 458/500\n",
      "22272/22272 [==============================] - 16s 715us/step - loss: 0.0236 - acc: 0.9828 - val_loss: 0.0196 - val_acc: 0.9821\n",
      "Epoch 459/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0230 - acc: 0.9837 - val_loss: 0.0199 - val_acc: 0.9838\n",
      "Epoch 460/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0234 - acc: 0.9850 - val_loss: 0.0208 - val_acc: 0.9821\n",
      "Epoch 461/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0227 - acc: 0.9854 - val_loss: 0.0178 - val_acc: 0.9855\n",
      "Epoch 462/500\n",
      "22272/22272 [==============================] - 16s 703us/step - loss: 0.0234 - acc: 0.9832 - val_loss: 0.0189 - val_acc: 0.9847\n",
      "Epoch 463/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0229 - acc: 0.9850 - val_loss: 0.0185 - val_acc: 0.9855\n",
      "Epoch 464/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0235 - acc: 0.9826 - val_loss: 0.0167 - val_acc: 0.9889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0231 - acc: 0.9837 - val_loss: 0.0183 - val_acc: 0.9855\n",
      "Epoch 466/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0227 - acc: 0.9841 - val_loss: 0.0178 - val_acc: 0.9872\n",
      "Epoch 467/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0228 - acc: 0.9854 - val_loss: 0.0178 - val_acc: 0.9847\n",
      "Epoch 468/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0231 - acc: 0.9841 - val_loss: 0.0185 - val_acc: 0.9812\n",
      "Epoch 469/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0227 - acc: 0.9840 - val_loss: 0.0175 - val_acc: 0.9872\n",
      "Epoch 470/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0228 - acc: 0.9836 - val_loss: 0.0196 - val_acc: 0.9838\n",
      "Epoch 471/500\n",
      "22272/22272 [==============================] - 15s 692us/step - loss: 0.0228 - acc: 0.9845 - val_loss: 0.0183 - val_acc: 0.9864\n",
      "Epoch 472/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0224 - acc: 0.9851 - val_loss: 0.0181 - val_acc: 0.9847\n",
      "Epoch 473/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0228 - acc: 0.9843 - val_loss: 0.0183 - val_acc: 0.9847\n",
      "Epoch 474/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0225 - acc: 0.9849 - val_loss: 0.0186 - val_acc: 0.9872\n",
      "Epoch 475/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0230 - acc: 0.9851 - val_loss: 0.0195 - val_acc: 0.9838\n",
      "Epoch 476/500\n",
      "22272/22272 [==============================] - 16s 709us/step - loss: 0.0228 - acc: 0.9846 - val_loss: 0.0197 - val_acc: 0.9829\n",
      "Epoch 477/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0226 - acc: 0.9864 - val_loss: 0.0183 - val_acc: 0.9838\n",
      "Epoch 478/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0226 - acc: 0.9844 - val_loss: 0.0171 - val_acc: 0.9872\n",
      "Epoch 479/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0226 - acc: 0.9847 - val_loss: 0.0165 - val_acc: 0.9889\n",
      "Epoch 480/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0225 - acc: 0.9859 - val_loss: 0.0185 - val_acc: 0.9881\n",
      "Epoch 481/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0228 - acc: 0.9843 - val_loss: 0.0187 - val_acc: 0.9778\n",
      "Epoch 482/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0227 - acc: 0.9850 - val_loss: 0.0172 - val_acc: 0.9881\n",
      "Epoch 483/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0224 - acc: 0.9859 - val_loss: 0.0185 - val_acc: 0.9872\n",
      "Epoch 484/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0224 - acc: 0.9855 - val_loss: 0.0188 - val_acc: 0.9829\n",
      "Epoch 485/500\n",
      "22272/22272 [==============================] - 16s 710us/step - loss: 0.0225 - acc: 0.9841 - val_loss: 0.0174 - val_acc: 0.9872\n",
      "Epoch 486/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0222 - acc: 0.9856 - val_loss: 0.0180 - val_acc: 0.9864\n",
      "Epoch 487/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0225 - acc: 0.9855 - val_loss: 0.0176 - val_acc: 0.9838\n",
      "Epoch 488/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0222 - acc: 0.9848 - val_loss: 0.0177 - val_acc: 0.9855\n",
      "Epoch 489/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0224 - acc: 0.9852 - val_loss: 0.0182 - val_acc: 0.9847\n",
      "Epoch 490/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0225 - acc: 0.9855 - val_loss: 0.0200 - val_acc: 0.9838\n",
      "Epoch 491/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0225 - acc: 0.9842 - val_loss: 0.0185 - val_acc: 0.9872\n",
      "Epoch 492/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0224 - acc: 0.9859 - val_loss: 0.0161 - val_acc: 0.9881\n",
      "Epoch 493/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0225 - acc: 0.9846 - val_loss: 0.0173 - val_acc: 0.9855\n",
      "Epoch 494/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0225 - acc: 0.9855 - val_loss: 0.0179 - val_acc: 0.9864\n",
      "Epoch 495/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0224 - acc: 0.9850 - val_loss: 0.0177 - val_acc: 0.9855\n",
      "Epoch 496/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0227 - acc: 0.9836 - val_loss: 0.0186 - val_acc: 0.9821\n",
      "Epoch 497/500\n",
      "22272/22272 [==============================] - 16s 699us/step - loss: 0.0224 - acc: 0.9850 - val_loss: 0.0205 - val_acc: 0.9821\n",
      "Epoch 498/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0224 - acc: 0.9837 - val_loss: 0.0160 - val_acc: 0.9898\n",
      "Epoch 499/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0222 - acc: 0.9859 - val_loss: 0.0214 - val_acc: 0.9855\n",
      "Epoch 500/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.0222 - acc: 0.9854 - val_loss: 0.0180 - val_acc: 0.9812\n"
     ]
    }
   ],
   "source": [
    "##Fitting the GRU to the Training set\n",
    "hist6=model6.fit(X_train, y_train, batch_size=300, epochs=500, validation_split=0.05, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The computational time of GRU model is :- 0.9033246279148255\n"
     ]
    }
   ],
   "source": [
    "#Getting the predicted locations \n",
    "from timeit import default_timer as timer\n",
    "start3 = timer()\n",
    "pred_gru3=model3.predict(pred_x)\n",
    "duration3 = timer() - start3\n",
    "print('The computational time of GRU model is :-', duration3)\n",
    "\n",
    "predictions_trans2=y_scale.inverse_transform(pred_gru3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.15322191286857384\n",
      "MAE: 0.06183844484650376\n"
     ]
    }
   ],
   "source": [
    "#Performance Evaluations in 3 hiden layers\n",
    "print(\"RMSE:\", sqrt(mean_squared_error(pred_gru3,Yy_pred)))\n",
    "print(\"MAE:\", mean_absolute_error(pred_gru3,Yy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The computational time of GRU model is :- 0.8673835232496968\n"
     ]
    }
   ],
   "source": [
    "#Getting the predicted locations \n",
    "from timeit import default_timer as timer\n",
    "start4 = timer()\n",
    "pred_gru4=model4.predict(pred_x)\n",
    "duration4 = timer() - start4\n",
    "print('The computational time of GRU model is :-', duration4)\n",
    "\n",
    "predictions_trans4=y_scale.inverse_transform(pred_gru4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.15070809821703784\n",
      "MAE: 0.05681413348689622\n"
     ]
    }
   ],
   "source": [
    "#Performance Evaluations in 3 hiden layers\n",
    "print(\"RMSE:\", sqrt(mean_squared_error(pred_gru4,Yy_pred)))\n",
    "print(\"MAE:\", mean_absolute_error(pred_gru4,Yy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The computational time of GRU model is :- 0.8723938677786163\n"
     ]
    }
   ],
   "source": [
    "#Getting the predicted locations \n",
    "from timeit import default_timer as timer\n",
    "start5 = timer()\n",
    "pred_gru5=model5.predict(pred_x)\n",
    "duration5 = timer() - start5\n",
    "print('The computational time of GRU model is :-', duration5)\n",
    "\n",
    "predictions_trans5=y_scale.inverse_transform(pred_gru5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.13924962870200577\n",
      "MAE: 0.05403862938073929\n"
     ]
    }
   ],
   "source": [
    "#Performance Evaluations in 3 hiden layers\n",
    "print(\"RMSE:\", sqrt(mean_squared_error(pred_gru5,Yy_pred)))\n",
    "print(\"MAE:\", mean_absolute_error(pred_gru5,Yy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The computational time of GRU model is :- 0.8680606429607924\n"
     ]
    }
   ],
   "source": [
    "#Getting the predicted locations \n",
    "from timeit import default_timer as timer\n",
    "start6 = timer()\n",
    "pred_gru6=model6.predict(pred_x)\n",
    "duration6 = timer() - start6\n",
    "print('The computational time of GRU model is :-', duration6)\n",
    "\n",
    "predictions_trans6=y_scale.inverse_transform(pred_gru6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.14479509394354037\n",
      "MAE: 0.059873231547076675\n"
     ]
    }
   ],
   "source": [
    "#Performance Evaluations in 3 hiden layers\n",
    "print(\"RMSE:\", sqrt(mean_squared_error(pred_gru6,Yy_pred)))\n",
    "print(\"MAE:\", mean_absolute_error(pred_gru6,Yy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loss graph \n",
    "plt.figure(figsize=(7,6))\n",
    "#plt.plot(hist3.history['loss'], 'r-', linewidth=1)\n",
    "plt.plot(hist3.history['val_loss'],'B', linewidth=1)\n",
    "\n",
    "\n",
    "#plt.plot(hist4.history['loss'],'k', linewidth=1)\n",
    "plt.plot(hist4.history['val_loss'],'y', linewidth=1)\n",
    "\n",
    "\n",
    "#plt.plot(hist5.history['loss'],'m', linewidth=1)\n",
    "plt.plot(hist5.history['val_loss'],'g', linewidth=1)\n",
    "\n",
    "#plt.plot(hist6.history['loss'],'m', linewidth=1)\n",
    "plt.plot(hist6.history['val_loss'],'r', linewidth=1)\n",
    "\n",
    "plt.title(\"Optimizers in GRU network\")\n",
    "plt.ylabel('Optimizer Losses (%)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['H=2', 'H=3', 'H=4', 'H=5'], loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.savefig('MLP_LSTM_GRU_Trackloss.png', format='png', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Performance Graph\n",
    "plt.figure(figsize=(7,6))\n",
    "#plt.plot(hist1.history['acc'], 'r-', linewidth=1)\n",
    "plt.plot(hist3.history['val_acc'],'B', linewidth=1)\n",
    "\n",
    "\n",
    "#plt.plot(hist2.history['acc'],'k', linewidth=2)\n",
    "plt.plot(hist4.history['val_acc'],'y', linewidth=1)\n",
    "\n",
    "\n",
    "#plt.plot(hist3.history['acc'],'m', linewidth=2)\n",
    "plt.plot(hist5.history['val_acc'],'g', linewidth=1)\n",
    "\n",
    "plt.plot(hist6.history['val_acc'],'r', linewidth=1)\n",
    "\n",
    "\n",
    "plt.title(\"Loss of Training and Testing  Using DNN Algorithms\")\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['GRU_Adam_Validation', 'GRU_sgd_Validation', 'GRU_Rmsprop_Validation', 'GRU_adamax_Validation'], loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.savefig('MLP_LSTM_GRU_Track.png', format='png', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
