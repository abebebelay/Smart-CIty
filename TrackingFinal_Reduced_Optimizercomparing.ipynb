{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------mlp-LSTM- GRU- Tracking Models--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#From Pandas\n",
    "from pandas import read_csv\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "\n",
    "#From Keras\n",
    "from keras.models import Sequential, load_model \n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM,GRU\n",
    "\n",
    "\n",
    "#From sklearn \n",
    "from sklearn import preprocessing  \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error \n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "#Import Files\n",
    "import math\n",
    "import time\n",
    "import pandas as pd #define the data structures\n",
    "import matplotlib as plt #for visualization\n",
    "import numpy\n",
    "import numpy as np #for matrix multiplication\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go \n",
    "import seaborn as sns\n",
    "import plotly.offline as py\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler #for normalizing our data(scaling)\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.model_selection import train_test_split   \n",
    "\n",
    "\n",
    "from math import sqrt\n",
    "from numpy import concatenate \n",
    "from datetime import datetime\n",
    "from pandas.plotting import scatter_matrix\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from sklearn import preprocessing \n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.model_selection import train_test_split   \n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error \n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "\n",
    "\n",
    "#Import others\n",
    "py.init_notebook_mode(connected=True)\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------Read Data------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserCode</th>\n",
       "      <th>UAV1</th>\n",
       "      <th>UAV2</th>\n",
       "      <th>UAV3</th>\n",
       "      <th>AP1</th>\n",
       "      <th>AP2</th>\n",
       "      <th>AP3</th>\n",
       "      <th>AP4</th>\n",
       "      <th>AP5</th>\n",
       "      <th>AP6</th>\n",
       "      <th>...</th>\n",
       "      <th>AP29</th>\n",
       "      <th>AP30</th>\n",
       "      <th>CH1</th>\n",
       "      <th>CH2</th>\n",
       "      <th>CH3</th>\n",
       "      <th>CH4</th>\n",
       "      <th>CH5</th>\n",
       "      <th>Hight</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>-49.5</td>\n",
       "      <td>-64.4</td>\n",
       "      <td>-67.4</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>6.70</td>\n",
       "      <td>-1.39</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>157.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>-49.1</td>\n",
       "      <td>-66.6</td>\n",
       "      <td>-72.6</td>\n",
       "      <td>3.42</td>\n",
       "      <td>1.83</td>\n",
       "      <td>-1.63</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>7.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>-52.9</td>\n",
       "      <td>-67.9</td>\n",
       "      <td>-75.9</td>\n",
       "      <td>4.55</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.63</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>-49.5</td>\n",
       "      <td>-67.5</td>\n",
       "      <td>-72.5</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.06</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>-66.1</td>\n",
       "      <td>-76.1</td>\n",
       "      <td>5.04</td>\n",
       "      <td>1.39</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     UserCode  UAV1  UAV2  UAV3   AP1   AP2   AP3   AP4   AP5   AP6 ...   \\\n",
       "1.0     101.0 -49.5 -64.4 -67.4 -0.52  6.70 -1.39  0.64  0.29 -0.60 ...    \n",
       "2.0     101.0 -49.1 -66.6 -72.6  3.42  1.83 -1.63 -0.37 -0.30  0.03 ...    \n",
       "3.0     101.0 -52.9 -67.9 -75.9  4.55  3.17  0.08  0.69  0.63 -0.51 ...    \n",
       "4.0     101.0 -49.5 -67.5 -72.5  3.25  2.06 -0.88  0.80  0.31 -0.11 ...    \n",
       "5.0     101.0 -50.0 -66.1 -76.1  5.04  1.39 -0.57 -0.03 -0.10 -0.15 ...    \n",
       "\n",
       "     AP29  AP30    CH1    CH2    CH3    CH4    CH5  Hight      X    Y  \n",
       "1.0  0.05  0.04  157.0  157.0  157.0  157.0  157.0   60.0  200.0  1.0  \n",
       "2.0  0.03 -0.16    7.0  157.0    7.0    7.0  157.0   60.0  197.0  1.0  \n",
       "3.0 -0.23 -0.15    7.0    7.0    7.0    7.0  157.0   60.0  194.0  1.0  \n",
       "4.0  0.01 -0.09    7.0    7.0  157.0    1.0    7.0   60.0  191.0  1.0  \n",
       "5.0  0.09  0.07    7.0    1.0  157.0    7.0    2.0   60.0  188.0  1.0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import or read the datasets\n",
    "df2= pd.read_csv('TrackDataAllReduced.csv',index_col=0)\n",
    "#df2=df2.drop(df2.columns[df2.columns.str.contains('unnamed',case = False)],axis = 1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23445 23445\n"
     ]
    }
   ],
   "source": [
    "Y=df2.iloc[:,-2:]\n",
    "X=df2.iloc[:,:-2]\n",
    "\n",
    "# conversion to numpy array\n",
    "x, y = X.values, Y.values  \n",
    "\n",
    "# scaling values for model\n",
    "x_scale = MinMaxScaler()\n",
    "y_scale = MinMaxScaler() \n",
    "\n",
    "\n",
    "X = x_scale.fit_transform(x)\n",
    "Y = y_scale.fit_transform(y)   \n",
    "print(len(X),len(Y))\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.05) \n",
    "#print(X_train.shape,X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23445, 40) (23445, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train= X, Y \n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "#X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserCode</th>\n",
       "      <th>UAV1</th>\n",
       "      <th>UAV2</th>\n",
       "      <th>UAV3</th>\n",
       "      <th>AP1</th>\n",
       "      <th>AP2</th>\n",
       "      <th>AP3</th>\n",
       "      <th>AP4</th>\n",
       "      <th>AP5</th>\n",
       "      <th>AP6</th>\n",
       "      <th>...</th>\n",
       "      <th>AP29</th>\n",
       "      <th>AP30</th>\n",
       "      <th>CH1</th>\n",
       "      <th>CH2</th>\n",
       "      <th>CH3</th>\n",
       "      <th>CH4</th>\n",
       "      <th>CH5</th>\n",
       "      <th>Hight</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104</td>\n",
       "      <td>-48.3</td>\n",
       "      <td>-74.7</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>7.53</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>153</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104</td>\n",
       "      <td>-54.3</td>\n",
       "      <td>-82.1</td>\n",
       "      <td>-78.6</td>\n",
       "      <td>6.16</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111</td>\n",
       "      <td>-82.1</td>\n",
       "      <td>-70.8</td>\n",
       "      <td>-80.8</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103</td>\n",
       "      <td>-76.5</td>\n",
       "      <td>-78.8</td>\n",
       "      <td>-82.8</td>\n",
       "      <td>6.97</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>...</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>115</td>\n",
       "      <td>-48.3</td>\n",
       "      <td>-75.6</td>\n",
       "      <td>-73.5</td>\n",
       "      <td>5.17</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.43</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserCode  UAV1  UAV2  UAV3   AP1   AP2   AP3   AP4   AP5   AP6 ...  AP29  \\\n",
       "1       104 -48.3 -74.7 -67.0  7.53  0.62  1.51  0.68 -0.02 -0.65 ... -0.26   \n",
       "2       104 -54.3 -82.1 -78.6  6.16 -0.31 -0.08  0.62  0.34 -1.20 ... -0.21   \n",
       "3       111 -82.1 -70.8 -80.8  7.73 -0.28  1.25  0.18 -1.15  0.30 ...  0.50   \n",
       "4       103 -76.5 -78.8 -82.8  6.97 -0.23 -0.17  2.18  0.08 -0.98 ...  0.34   \n",
       "5       115 -48.3 -75.6 -73.5  5.17  1.16  0.96  1.47  1.43 -0.37 ... -0.33   \n",
       "\n",
       "   AP30  CH1  CH2  CH3  CH4  CH5  Hight  X   Y  \n",
       "1 -0.17    1   11  153    6    6     40  1   6  \n",
       "2  0.50    1   11    6    3    8     40  1  12  \n",
       "3 -0.35    1    1    1    1    6     60  1  15  \n",
       "4 -0.86    3    3    3    3  100     60  1  21  \n",
       "5 -0.58   11   11    1  153    2     40  1  24  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import or read the datasets\n",
    "pred = pd.read_csv('TestReduced.csv',index_col=0)\n",
    "#pred=pred2.drop(pred2.columns[pred2.columns.str.contains('unnamed',case = False)],axis = 1)\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abebe Belay\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning:\n",
      "\n",
      "Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_pred=pred.iloc[:,:-2]\n",
    "Y_pred=pred.iloc[:,-2:]\n",
    "x_pred, y_pred = X_pred.values, Y_pred.values  \n",
    "\n",
    "# scaling values for model\n",
    "x_pred_scale = MinMaxScaler()\n",
    "y_pred_scale = MinMaxScaler() \n",
    "\n",
    "Xx__pred = x_pred_scale.fit_transform(x_pred)\n",
    "Yy_pred = y_pred_scale.fit_transform(y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.21428571, 0.75877193, 0.17165669, ..., 0.02941176,\n",
       "         0.03030303, 0.        ]],\n",
       "\n",
       "       [[0.21428571, 0.62719298, 0.0239521 , ..., 0.01176471,\n",
       "         0.04242424, 0.        ]],\n",
       "\n",
       "       [[0.71428571, 0.01754386, 0.249501  , ..., 0.        ,\n",
       "         0.03030303, 1.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.5       , 0.73684211, 0.58283433, ..., 0.92941176,\n",
       "         0.10909091, 0.        ]],\n",
       "\n",
       "       [[1.        , 0.75877193, 0.67664671, ..., 0.88235294,\n",
       "         0.69090909, 1.        ]],\n",
       "\n",
       "       [[0.42857143, 0.75877193, 0.66866267, ..., 0.81176471,\n",
       "         0.95757576, 1.        ]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_x = np.reshape(Xx__pred, (Xx__pred.shape[0], 1, Xx__pred.shape[1])) \n",
    "pred_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------GRU model defining--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(feature):\n",
    "    input_shapes=(None, feature)\n",
    "    \n",
    "    print(input_shapes)\n",
    "    model_name3 = 'Tracking_GRU_' \n",
    "    model3='gru'\n",
    "    model3 = Sequential()\n",
    "    model3.add(GRU(units=512, return_sequences=True, input_shape=input_shapes))\n",
    "    model3.add(Dropout(0.2))\n",
    "    model3.add(GRU(units=256,return_sequences=True))\n",
    "    model3.add(Dropout(0.2))\n",
    "    model3.add(GRU(units=256))\n",
    "    model3.add(Dropout(0.2))\n",
    "    model3.add(Dense(2, activation='tanh'))\n",
    "    #model.add(Dense(4, activation='relu'))\n",
    "    #model.add(Dense(4, activation='softmax'))\n",
    "    model3.compile(loss='MAE', optimizer='adam', metrics=['accuracy']) \n",
    "    #model.compile(loss='mse',optimizer='rmsprop')\n",
    "    #model.compile(loss='mse', optimizer='sgd') \n",
    "    return model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(feature):\n",
    "    input_shapes=(None, feature)\n",
    "    \n",
    "    print(input_shapes)\n",
    "    model_name4 = 'Tracking_GRU_' \n",
    "    model4='gru'\n",
    "    model4 = Sequential()\n",
    "    model4.add(GRU(units=512, return_sequences=True, input_shape=input_shapes))\n",
    "    model4.add(Dropout(0.2))\n",
    "    model4.add(GRU(units=256,return_sequences=True))\n",
    "    model4.add(Dropout(0.2))\n",
    "    model4.add(GRU(units=256))\n",
    "    model4.add(Dropout(0.2))\n",
    "    model4.add(Dense(2, activation='tanh'))\n",
    "    #model.add(Dense(4, activation='relu'))\n",
    "    #model.add(Dense(4, activation='softmax'))\n",
    "    model4.compile(loss='MAE', optimizer='sgd', metrics=['accuracy']) \n",
    "    #model.compile(loss='mse',optimizer='rmsprop')\n",
    "    #model.compile(loss='mse', optimizer='sgd') \n",
    "    return model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(feature):\n",
    "    input_shapes=(None, feature)\n",
    "    \n",
    "    print(input_shapes)\n",
    "    model_name5 = 'Tracking_GRU_' \n",
    "    model5='gru'\n",
    "    model5 = Sequential()\n",
    "    model5.add(GRU(units=512, return_sequences=True, input_shape=input_shapes))\n",
    "    model5.add(Dropout(0.2))\n",
    "    model5.add(GRU(units=256,return_sequences=True))\n",
    "    model5.add(Dropout(0.2))\n",
    "    model5.add(GRU(units=256))\n",
    "    model5.add(Dropout(0.2))\n",
    "    model5.add(Dense(2, activation='tanh'))\n",
    "    #model.add(Dense(4, activation='relu'))\n",
    "    #model.add(Dense(4, activation='softmax'))\n",
    "    model5.compile(loss='MAE', optimizer='Rmsprop', metrics=['accuracy']) \n",
    "    #model.compile(loss='mse',optimizer='rmsprop')\n",
    "    #model.compile(loss='mse', optimizer='sgd') \n",
    "    return model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(feature):\n",
    "    input_shapes=(None, feature)\n",
    "    \n",
    "    print(input_shapes)\n",
    "    model_name6 = 'Tracking_GRU_' \n",
    "    model6='gru'\n",
    "    model6 = Sequential()\n",
    "    model6.add(GRU(units=512, return_sequences=True, input_shape=input_shapes))\n",
    "    model6.add(Dropout(0.2))\n",
    "    model6.add(GRU(units=256,return_sequences=True))\n",
    "    model6.add(Dropout(0.2))\n",
    "    model6.add(GRU(units=256))\n",
    "    model6.add(Dropout(0.2))\n",
    "    model6.add(Dense(2, activation='tanh'))\n",
    "    #model.add(Dense(4, activation='relu'))\n",
    "    #model.add(Dense(4, activation='softmax'))\n",
    "    model6.compile(loss='MAE', optimizer='adamax', metrics=['accuracy']) \n",
    "    #model.compile(loss='mse',optimizer='rmsprop')\n",
    "    #model.compile(loss='mse', optimizer='sgd') \n",
    "    return model6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 40)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_46 (GRU)                 (None, None, 512)         849408    \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "gru_47 (GRU)                 (None, None, 256)         590592    \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "gru_48 (GRU)                 (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,834,498\n",
      "Trainable params: 1,834,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3=gru(X_train.shape[2])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 40)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_49 (GRU)                 (None, None, 512)         849408    \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "gru_50 (GRU)                 (None, None, 256)         590592    \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "gru_51 (GRU)                 (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,834,498\n",
      "Trainable params: 1,834,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4=gru(X_train.shape[2])\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 40)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_52 (GRU)                 (None, None, 512)         849408    \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "gru_53 (GRU)                 (None, None, 256)         590592    \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "gru_54 (GRU)                 (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,834,498\n",
      "Trainable params: 1,834,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5=gru(X_train.shape[2])\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 40)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_55 (GRU)                 (None, None, 512)         849408    \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "gru_56 (GRU)                 (None, None, 256)         590592    \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "gru_57 (GRU)                 (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,834,498\n",
      "Trainable params: 1,834,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model6=gru(X_train.shape[2])\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22272 samples, validate on 1173 samples\n",
      "Epoch 1/500\n",
      "22272/22272 [==============================] - 23s 1ms/step - loss: 0.2314 - acc: 0.7501 - val_loss: 0.2247 - val_acc: 0.7298\n",
      "Epoch 2/500\n",
      "22272/22272 [==============================] - 12s 533us/step - loss: 0.2076 - acc: 0.7468 - val_loss: 0.2075 - val_acc: 0.7246\n",
      "Epoch 3/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.2005 - acc: 0.7500 - val_loss: 0.2049 - val_acc: 0.7340\n",
      "Epoch 4/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.1990 - acc: 0.7522 - val_loss: 0.2048 - val_acc: 0.7315\n",
      "Epoch 5/500\n",
      "22272/22272 [==============================] - 12s 520us/step - loss: 0.1976 - acc: 0.7530 - val_loss: 0.2070 - val_acc: 0.7494\n",
      "Epoch 6/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.1967 - acc: 0.7554 - val_loss: 0.2001 - val_acc: 0.7289\n",
      "Epoch 7/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.1949 - acc: 0.7550 - val_loss: 0.1990 - val_acc: 0.7425\n",
      "Epoch 8/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.1945 - acc: 0.7536 - val_loss: 0.1974 - val_acc: 0.7425\n",
      "Epoch 9/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.1915 - acc: 0.7561 - val_loss: 0.1945 - val_acc: 0.7332\n",
      "Epoch 10/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.1891 - acc: 0.7563 - val_loss: 0.1923 - val_acc: 0.7272\n",
      "Epoch 11/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.1869 - acc: 0.7527 - val_loss: 0.1892 - val_acc: 0.7349\n",
      "Epoch 12/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.1855 - acc: 0.7542 - val_loss: 0.1910 - val_acc: 0.7468\n",
      "Epoch 13/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.1850 - acc: 0.7576 - val_loss: 0.1887 - val_acc: 0.7442\n",
      "Epoch 14/500\n",
      "22272/22272 [==============================] - 12s 523us/step - loss: 0.1816 - acc: 0.7638 - val_loss: 0.1892 - val_acc: 0.7528\n",
      "Epoch 15/500\n",
      "22272/22272 [==============================] - 13s 570us/step - loss: 0.1771 - acc: 0.7694 - val_loss: 0.1811 - val_acc: 0.7613\n",
      "Epoch 16/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.1743 - acc: 0.7731 - val_loss: 0.1791 - val_acc: 0.7545\n",
      "Epoch 17/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.1718 - acc: 0.7786 - val_loss: 0.1873 - val_acc: 0.7553\n",
      "Epoch 18/500\n",
      "22272/22272 [==============================] - 12s 520us/step - loss: 0.1698 - acc: 0.7834 - val_loss: 0.1753 - val_acc: 0.7707\n",
      "Epoch 19/500\n",
      "22272/22272 [==============================] - 12s 524us/step - loss: 0.1680 - acc: 0.7867 - val_loss: 0.1736 - val_acc: 0.7613\n",
      "Epoch 20/500\n",
      "22272/22272 [==============================] - 12s 520us/step - loss: 0.1669 - acc: 0.7900 - val_loss: 0.1740 - val_acc: 0.7698\n",
      "Epoch 21/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.1661 - acc: 0.7929 - val_loss: 0.1725 - val_acc: 0.7690\n",
      "Epoch 22/500\n",
      "22272/22272 [==============================] - 12s 520us/step - loss: 0.1651 - acc: 0.7927 - val_loss: 0.1696 - val_acc: 0.7758\n",
      "Epoch 23/500\n",
      "22272/22272 [==============================] - 12s 522us/step - loss: 0.1641 - acc: 0.7945 - val_loss: 0.1743 - val_acc: 0.7732\n",
      "Epoch 24/500\n",
      "22272/22272 [==============================] - 12s 526us/step - loss: 0.1634 - acc: 0.7953 - val_loss: 0.1677 - val_acc: 0.7758\n",
      "Epoch 25/500\n",
      "22272/22272 [==============================] - 12s 521us/step - loss: 0.1629 - acc: 0.7980 - val_loss: 0.1673 - val_acc: 0.7732\n",
      "Epoch 26/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.1620 - acc: 0.7988 - val_loss: 0.1683 - val_acc: 0.7690\n",
      "Epoch 27/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.1621 - acc: 0.7979 - val_loss: 0.1673 - val_acc: 0.7826\n",
      "Epoch 28/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.1594 - acc: 0.8029 - val_loss: 0.1726 - val_acc: 0.7681\n",
      "Epoch 29/500\n",
      "22272/22272 [==============================] - 12s 524us/step - loss: 0.1598 - acc: 0.8001 - val_loss: 0.1665 - val_acc: 0.7835\n",
      "Epoch 30/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.1594 - acc: 0.8024 - val_loss: 0.1650 - val_acc: 0.7869\n",
      "Epoch 31/500\n",
      "22272/22272 [==============================] - 12s 522us/step - loss: 0.1573 - acc: 0.8041 - val_loss: 0.1671 - val_acc: 0.7835\n",
      "Epoch 32/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.1569 - acc: 0.8081 - val_loss: 0.1636 - val_acc: 0.7843\n",
      "Epoch 33/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.1561 - acc: 0.8073 - val_loss: 0.1637 - val_acc: 0.7903\n",
      "Epoch 34/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.1550 - acc: 0.8108 - val_loss: 0.1605 - val_acc: 0.7852\n",
      "Epoch 35/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.1541 - acc: 0.8116 - val_loss: 0.1588 - val_acc: 0.7869\n",
      "Epoch 36/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.1540 - acc: 0.8129 - val_loss: 0.1569 - val_acc: 0.8022\n",
      "Epoch 37/500\n",
      "22272/22272 [==============================] - 12s 534us/step - loss: 0.1521 - acc: 0.8137 - val_loss: 0.1595 - val_acc: 0.7997\n",
      "Epoch 38/500\n",
      "22272/22272 [==============================] - 12s 528us/step - loss: 0.1512 - acc: 0.8172 - val_loss: 0.1557 - val_acc: 0.8022\n",
      "Epoch 39/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.1514 - acc: 0.8152 - val_loss: 0.1548 - val_acc: 0.8056\n",
      "Epoch 40/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.1491 - acc: 0.8196 - val_loss: 0.1545 - val_acc: 0.8065\n",
      "Epoch 41/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.1490 - acc: 0.8197 - val_loss: 0.1557 - val_acc: 0.8082\n",
      "Epoch 42/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.1477 - acc: 0.8186 - val_loss: 0.1523 - val_acc: 0.8090\n",
      "Epoch 43/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.1477 - acc: 0.8196 - val_loss: 0.1479 - val_acc: 0.8082\n",
      "Epoch 44/500\n",
      "22272/22272 [==============================] - 13s 605us/step - loss: 0.1458 - acc: 0.8219 - val_loss: 0.1481 - val_acc: 0.8065\n",
      "Epoch 45/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.1454 - acc: 0.8261 - val_loss: 0.1501 - val_acc: 0.8184\n",
      "Epoch 46/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.1444 - acc: 0.8250 - val_loss: 0.1463 - val_acc: 0.8184\n",
      "Epoch 47/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.1426 - acc: 0.8279 - val_loss: 0.1445 - val_acc: 0.8286\n",
      "Epoch 48/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.1424 - acc: 0.8302 - val_loss: 0.1488 - val_acc: 0.8201\n",
      "Epoch 49/500\n",
      "22272/22272 [==============================] - 12s 523us/step - loss: 0.1414 - acc: 0.8276 - val_loss: 0.1430 - val_acc: 0.8193\n",
      "Epoch 50/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.1394 - acc: 0.8330 - val_loss: 0.1431 - val_acc: 0.8167\n",
      "Epoch 51/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.1392 - acc: 0.8337 - val_loss: 0.1465 - val_acc: 0.8244\n",
      "Epoch 52/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.1394 - acc: 0.8354 - val_loss: 0.1435 - val_acc: 0.8227\n",
      "Epoch 53/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.1364 - acc: 0.8364 - val_loss: 0.1391 - val_acc: 0.8201\n",
      "Epoch 54/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.1364 - acc: 0.8362 - val_loss: 0.1368 - val_acc: 0.8372\n",
      "Epoch 55/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.1357 - acc: 0.8397 - val_loss: 0.1400 - val_acc: 0.8210\n",
      "Epoch 56/500\n",
      "22272/22272 [==============================] - 12s 524us/step - loss: 0.1345 - acc: 0.8417 - val_loss: 0.1435 - val_acc: 0.8176\n",
      "Epoch 57/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.1347 - acc: 0.8418 - val_loss: 0.1379 - val_acc: 0.8406\n",
      "Epoch 58/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.1330 - acc: 0.8408 - val_loss: 0.1335 - val_acc: 0.8389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.1321 - acc: 0.8474 - val_loss: 0.1347 - val_acc: 0.8244\n",
      "Epoch 60/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.1315 - acc: 0.8453 - val_loss: 0.1293 - val_acc: 0.8474\n",
      "Epoch 61/500\n",
      "22272/22272 [==============================] - 11s 493us/step - loss: 0.1297 - acc: 0.8510 - val_loss: 0.1298 - val_acc: 0.8508\n",
      "Epoch 62/500\n",
      "22272/22272 [==============================] - 11s 490us/step - loss: 0.1291 - acc: 0.8500 - val_loss: 0.1315 - val_acc: 0.8355\n",
      "Epoch 63/500\n",
      "22272/22272 [==============================] - 11s 490us/step - loss: 0.1289 - acc: 0.8521 - val_loss: 0.1270 - val_acc: 0.8483\n",
      "Epoch 64/500\n",
      "22272/22272 [==============================] - 11s 490us/step - loss: 0.1274 - acc: 0.8556 - val_loss: 0.1316 - val_acc: 0.8440\n",
      "Epoch 65/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.1257 - acc: 0.8577 - val_loss: 0.1240 - val_acc: 0.8542\n",
      "Epoch 66/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.1251 - acc: 0.8587 - val_loss: 0.1249 - val_acc: 0.8517\n",
      "Epoch 67/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.1255 - acc: 0.8582 - val_loss: 0.1252 - val_acc: 0.8576\n",
      "Epoch 68/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.1249 - acc: 0.8587 - val_loss: 0.1285 - val_acc: 0.8474\n",
      "Epoch 69/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.1231 - acc: 0.8619 - val_loss: 0.1222 - val_acc: 0.8645\n",
      "Epoch 70/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.1216 - acc: 0.8669 - val_loss: 0.1218 - val_acc: 0.8653\n",
      "Epoch 71/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.1209 - acc: 0.8680 - val_loss: 0.1210 - val_acc: 0.8576\n",
      "Epoch 72/500\n",
      "22272/22272 [==============================] - 11s 491us/step - loss: 0.1202 - acc: 0.8682 - val_loss: 0.1202 - val_acc: 0.8627\n",
      "Epoch 73/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.1194 - acc: 0.8699 - val_loss: 0.1197 - val_acc: 0.8610\n",
      "Epoch 74/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.1186 - acc: 0.8684 - val_loss: 0.1182 - val_acc: 0.8619\n",
      "Epoch 75/500\n",
      "22272/22272 [==============================] - 11s 497us/step - loss: 0.1180 - acc: 0.8700 - val_loss: 0.1203 - val_acc: 0.8619\n",
      "Epoch 76/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.1171 - acc: 0.8717 - val_loss: 0.1161 - val_acc: 0.8721\n",
      "Epoch 77/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.1173 - acc: 0.8715 - val_loss: 0.1207 - val_acc: 0.8636\n",
      "Epoch 78/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.1164 - acc: 0.8734 - val_loss: 0.1129 - val_acc: 0.8764\n",
      "Epoch 79/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.1159 - acc: 0.8737 - val_loss: 0.1162 - val_acc: 0.8789\n",
      "Epoch 80/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.1151 - acc: 0.8759 - val_loss: 0.1158 - val_acc: 0.8602\n",
      "Epoch 81/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.1133 - acc: 0.8756 - val_loss: 0.1107 - val_acc: 0.8730\n",
      "Epoch 82/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.1132 - acc: 0.8800 - val_loss: 0.1128 - val_acc: 0.8738\n",
      "Epoch 83/500\n",
      "22272/22272 [==============================] - 11s 494us/step - loss: 0.1125 - acc: 0.8766 - val_loss: 0.1147 - val_acc: 0.8687\n",
      "Epoch 84/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.1122 - acc: 0.8779 - val_loss: 0.1081 - val_acc: 0.8764\n",
      "Epoch 85/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.1110 - acc: 0.8799 - val_loss: 0.1073 - val_acc: 0.8798\n",
      "Epoch 86/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.1106 - acc: 0.8803 - val_loss: 0.1131 - val_acc: 0.8789\n",
      "Epoch 87/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.1094 - acc: 0.8829 - val_loss: 0.1087 - val_acc: 0.8755\n",
      "Epoch 88/500\n",
      "22272/22272 [==============================] - 11s 497us/step - loss: 0.1100 - acc: 0.8821 - val_loss: 0.1101 - val_acc: 0.8764\n",
      "Epoch 89/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.1093 - acc: 0.8856 - val_loss: 0.1067 - val_acc: 0.8832\n",
      "Epoch 90/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.1091 - acc: 0.8866 - val_loss: 0.1053 - val_acc: 0.8892\n",
      "Epoch 91/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.1069 - acc: 0.8867 - val_loss: 0.1072 - val_acc: 0.8858\n",
      "Epoch 92/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.1064 - acc: 0.8896 - val_loss: 0.1120 - val_acc: 0.8781\n",
      "Epoch 93/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.1065 - acc: 0.8874 - val_loss: 0.1061 - val_acc: 0.8832\n",
      "Epoch 94/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.1060 - acc: 0.8880 - val_loss: 0.1091 - val_acc: 0.8772\n",
      "Epoch 95/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.1057 - acc: 0.8879 - val_loss: 0.1042 - val_acc: 0.8849\n",
      "Epoch 96/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.1039 - acc: 0.8915 - val_loss: 0.1027 - val_acc: 0.8858\n",
      "Epoch 97/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.1034 - acc: 0.8921 - val_loss: 0.1057 - val_acc: 0.8858\n",
      "Epoch 98/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.1033 - acc: 0.8916 - val_loss: 0.1017 - val_acc: 0.8875\n",
      "Epoch 99/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.1032 - acc: 0.8913 - val_loss: 0.1025 - val_acc: 0.8849\n",
      "Epoch 100/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.1028 - acc: 0.8940 - val_loss: 0.1033 - val_acc: 0.8926\n",
      "Epoch 101/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.1016 - acc: 0.8955 - val_loss: 0.1019 - val_acc: 0.8900\n",
      "Epoch 102/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.1015 - acc: 0.8973 - val_loss: 0.1025 - val_acc: 0.8815\n",
      "Epoch 103/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.1009 - acc: 0.8963 - val_loss: 0.0986 - val_acc: 0.8951\n",
      "Epoch 104/500\n",
      "22272/22272 [==============================] - 11s 497us/step - loss: 0.1006 - acc: 0.8973 - val_loss: 0.1032 - val_acc: 0.8917\n",
      "Epoch 105/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.1005 - acc: 0.8973 - val_loss: 0.1056 - val_acc: 0.8849\n",
      "Epoch 106/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0994 - acc: 0.8981 - val_loss: 0.1000 - val_acc: 0.8892\n",
      "Epoch 107/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0987 - acc: 0.8994 - val_loss: 0.0988 - val_acc: 0.8951\n",
      "Epoch 108/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0982 - acc: 0.9014 - val_loss: 0.0963 - val_acc: 0.8900\n",
      "Epoch 109/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0981 - acc: 0.9024 - val_loss: 0.0971 - val_acc: 0.8977\n",
      "Epoch 110/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0971 - acc: 0.9020 - val_loss: 0.1000 - val_acc: 0.8926\n",
      "Epoch 111/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0968 - acc: 0.9028 - val_loss: 0.0931 - val_acc: 0.8968\n",
      "Epoch 112/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0962 - acc: 0.9043 - val_loss: 0.0958 - val_acc: 0.8977\n",
      "Epoch 113/500\n",
      "22272/22272 [==============================] - 12s 516us/step - loss: 0.0963 - acc: 0.9019 - val_loss: 0.0936 - val_acc: 0.9003\n",
      "Epoch 114/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0946 - acc: 0.9067 - val_loss: 0.0929 - val_acc: 0.8951\n",
      "Epoch 115/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0956 - acc: 0.9055 - val_loss: 0.0925 - val_acc: 0.8986\n",
      "Epoch 116/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0948 - acc: 0.9063 - val_loss: 0.0916 - val_acc: 0.8977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0943 - acc: 0.9094 - val_loss: 0.0912 - val_acc: 0.9003\n",
      "Epoch 118/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0941 - acc: 0.9089 - val_loss: 0.0951 - val_acc: 0.8994\n",
      "Epoch 119/500\n",
      "22272/22272 [==============================] - 11s 494us/step - loss: 0.0943 - acc: 0.9067 - val_loss: 0.0971 - val_acc: 0.8934\n",
      "Epoch 120/500\n",
      "22272/22272 [==============================] - 11s 494us/step - loss: 0.0931 - acc: 0.9098 - val_loss: 0.0952 - val_acc: 0.8986\n",
      "Epoch 121/500\n",
      "22272/22272 [==============================] - 11s 494us/step - loss: 0.0920 - acc: 0.9099 - val_loss: 0.0906 - val_acc: 0.8994\n",
      "Epoch 122/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0913 - acc: 0.9143 - val_loss: 0.0911 - val_acc: 0.8977\n",
      "Epoch 123/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0917 - acc: 0.9112 - val_loss: 0.0915 - val_acc: 0.9011\n",
      "Epoch 124/500\n",
      "22272/22272 [==============================] - 12s 528us/step - loss: 0.0913 - acc: 0.9114 - val_loss: 0.0921 - val_acc: 0.9045\n",
      "Epoch 125/500\n",
      "22272/22272 [==============================] - 12s 537us/step - loss: 0.0906 - acc: 0.9126 - val_loss: 0.0890 - val_acc: 0.9054\n",
      "Epoch 126/500\n",
      "22272/22272 [==============================] - 12s 551us/step - loss: 0.0905 - acc: 0.9107 - val_loss: 0.0860 - val_acc: 0.9028\n",
      "Epoch 127/500\n",
      "22272/22272 [==============================] - 13s 568us/step - loss: 0.0908 - acc: 0.9142 - val_loss: 0.0880 - val_acc: 0.9045\n",
      "Epoch 128/500\n",
      "22272/22272 [==============================] - 13s 569us/step - loss: 0.0890 - acc: 0.9155 - val_loss: 0.0897 - val_acc: 0.9028\n",
      "Epoch 129/500\n",
      "22272/22272 [==============================] - 13s 565us/step - loss: 0.0896 - acc: 0.9141 - val_loss: 0.0884 - val_acc: 0.9037\n",
      "Epoch 130/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0883 - acc: 0.9176 - val_loss: 0.0884 - val_acc: 0.9045\n",
      "Epoch 131/500\n",
      "22272/22272 [==============================] - 12s 522us/step - loss: 0.0882 - acc: 0.9146 - val_loss: 0.0883 - val_acc: 0.9028\n",
      "Epoch 132/500\n",
      "22272/22272 [==============================] - 12s 541us/step - loss: 0.0889 - acc: 0.9157 - val_loss: 0.0897 - val_acc: 0.9088\n",
      "Epoch 133/500\n",
      "22272/22272 [==============================] - 12s 539us/step - loss: 0.0874 - acc: 0.9183 - val_loss: 0.0858 - val_acc: 0.9113\n",
      "Epoch 134/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0874 - acc: 0.9183 - val_loss: 0.0863 - val_acc: 0.9105\n",
      "Epoch 135/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.0877 - acc: 0.9180 - val_loss: 0.0841 - val_acc: 0.9079\n",
      "Epoch 136/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0864 - acc: 0.9180 - val_loss: 0.0874 - val_acc: 0.9028\n",
      "Epoch 137/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0873 - acc: 0.9198 - val_loss: 0.0869 - val_acc: 0.9062\n",
      "Epoch 138/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0864 - acc: 0.9212 - val_loss: 0.0875 - val_acc: 0.9096\n",
      "Epoch 139/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0858 - acc: 0.9210 - val_loss: 0.0867 - val_acc: 0.9088\n",
      "Epoch 140/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0858 - acc: 0.9206 - val_loss: 0.0833 - val_acc: 0.9139\n",
      "Epoch 141/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0852 - acc: 0.9213 - val_loss: 0.0836 - val_acc: 0.9122\n",
      "Epoch 142/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0843 - acc: 0.9227 - val_loss: 0.0819 - val_acc: 0.9130\n",
      "Epoch 143/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0838 - acc: 0.9230 - val_loss: 0.0837 - val_acc: 0.9173\n",
      "Epoch 144/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0839 - acc: 0.9222 - val_loss: 0.0823 - val_acc: 0.9079\n",
      "Epoch 145/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0833 - acc: 0.9223 - val_loss: 0.0823 - val_acc: 0.9139\n",
      "Epoch 146/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0832 - acc: 0.9245 - val_loss: 0.0856 - val_acc: 0.9096\n",
      "Epoch 147/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0827 - acc: 0.9246 - val_loss: 0.0842 - val_acc: 0.9088\n",
      "Epoch 148/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0822 - acc: 0.9236 - val_loss: 0.0818 - val_acc: 0.9173\n",
      "Epoch 149/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0823 - acc: 0.9252 - val_loss: 0.0821 - val_acc: 0.9139\n",
      "Epoch 150/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0824 - acc: 0.9263 - val_loss: 0.0808 - val_acc: 0.9156\n",
      "Epoch 151/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0819 - acc: 0.9253 - val_loss: 0.0816 - val_acc: 0.9139\n",
      "Epoch 152/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0814 - acc: 0.9256 - val_loss: 0.0825 - val_acc: 0.9147\n",
      "Epoch 153/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0812 - acc: 0.9251 - val_loss: 0.0826 - val_acc: 0.9182\n",
      "Epoch 154/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0812 - acc: 0.9254 - val_loss: 0.0810 - val_acc: 0.9139\n",
      "Epoch 155/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0805 - acc: 0.9256 - val_loss: 0.0783 - val_acc: 0.9199\n",
      "Epoch 156/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0810 - acc: 0.9277 - val_loss: 0.0777 - val_acc: 0.9190\n",
      "Epoch 157/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0801 - acc: 0.9275 - val_loss: 0.0810 - val_acc: 0.9216\n",
      "Epoch 158/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0796 - acc: 0.9290 - val_loss: 0.0801 - val_acc: 0.9241\n",
      "Epoch 159/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0795 - acc: 0.9302 - val_loss: 0.0832 - val_acc: 0.9190\n",
      "Epoch 160/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0795 - acc: 0.9292 - val_loss: 0.0833 - val_acc: 0.9147\n",
      "Epoch 161/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0799 - acc: 0.9305 - val_loss: 0.0773 - val_acc: 0.9241\n",
      "Epoch 162/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0786 - acc: 0.9314 - val_loss: 0.0781 - val_acc: 0.9233\n",
      "Epoch 163/500\n",
      "22272/22272 [==============================] - 12s 516us/step - loss: 0.0784 - acc: 0.9311 - val_loss: 0.0782 - val_acc: 0.9173\n",
      "Epoch 164/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0780 - acc: 0.9308 - val_loss: 0.0767 - val_acc: 0.9165\n",
      "Epoch 165/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0771 - acc: 0.9326 - val_loss: 0.0843 - val_acc: 0.9113\n",
      "Epoch 166/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0778 - acc: 0.9308 - val_loss: 0.0759 - val_acc: 0.9199\n",
      "Epoch 167/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0779 - acc: 0.9304 - val_loss: 0.0762 - val_acc: 0.9173\n",
      "Epoch 168/500\n",
      "22272/22272 [==============================] - 12s 525us/step - loss: 0.0768 - acc: 0.9318 - val_loss: 0.0809 - val_acc: 0.9190\n",
      "Epoch 169/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0770 - acc: 0.9337 - val_loss: 0.0758 - val_acc: 0.9292\n",
      "Epoch 170/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0764 - acc: 0.9338 - val_loss: 0.0757 - val_acc: 0.9258\n",
      "Epoch 171/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0763 - acc: 0.9328 - val_loss: 0.0737 - val_acc: 0.9216\n",
      "Epoch 172/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.0761 - acc: 0.9328 - val_loss: 0.0744 - val_acc: 0.9250\n",
      "Epoch 173/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0764 - acc: 0.9316 - val_loss: 0.0766 - val_acc: 0.9233\n",
      "Epoch 174/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0759 - acc: 0.9328 - val_loss: 0.0748 - val_acc: 0.9199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/500\n",
      "22272/22272 [==============================] - 11s 493us/step - loss: 0.0755 - acc: 0.9343 - val_loss: 0.0754 - val_acc: 0.9216\n",
      "Epoch 176/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.0750 - acc: 0.9357 - val_loss: 0.0758 - val_acc: 0.9190\n",
      "Epoch 177/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0754 - acc: 0.9342 - val_loss: 0.0733 - val_acc: 0.9275\n",
      "Epoch 178/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.0747 - acc: 0.9345 - val_loss: 0.0762 - val_acc: 0.9190\n",
      "Epoch 179/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.0746 - acc: 0.9347 - val_loss: 0.0794 - val_acc: 0.9216\n",
      "Epoch 180/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0743 - acc: 0.9349 - val_loss: 0.0761 - val_acc: 0.9233\n",
      "Epoch 181/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0743 - acc: 0.9366 - val_loss: 0.0751 - val_acc: 0.9199\n",
      "Epoch 182/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.0735 - acc: 0.9368 - val_loss: 0.0742 - val_acc: 0.9258\n",
      "Epoch 183/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0736 - acc: 0.9370 - val_loss: 0.0750 - val_acc: 0.9233\n",
      "Epoch 184/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0736 - acc: 0.9375 - val_loss: 0.0745 - val_acc: 0.9301\n",
      "Epoch 185/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.0733 - acc: 0.9376 - val_loss: 0.0726 - val_acc: 0.9250\n",
      "Epoch 186/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0735 - acc: 0.9361 - val_loss: 0.0723 - val_acc: 0.9258\n",
      "Epoch 187/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0730 - acc: 0.9382 - val_loss: 0.0735 - val_acc: 0.9258\n",
      "Epoch 188/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0719 - acc: 0.9371 - val_loss: 0.0695 - val_acc: 0.9301\n",
      "Epoch 189/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0718 - acc: 0.9402 - val_loss: 0.0721 - val_acc: 0.9216\n",
      "Epoch 190/500\n",
      "22272/22272 [==============================] - 11s 494us/step - loss: 0.0723 - acc: 0.9388 - val_loss: 0.0738 - val_acc: 0.9258\n",
      "Epoch 191/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0718 - acc: 0.9380 - val_loss: 0.0729 - val_acc: 0.9250\n",
      "Epoch 192/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0714 - acc: 0.9398 - val_loss: 0.0744 - val_acc: 0.9241\n",
      "Epoch 193/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0715 - acc: 0.9401 - val_loss: 0.0698 - val_acc: 0.9199\n",
      "Epoch 194/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0710 - acc: 0.9399 - val_loss: 0.0711 - val_acc: 0.9275\n",
      "Epoch 195/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0710 - acc: 0.9423 - val_loss: 0.0691 - val_acc: 0.9292\n",
      "Epoch 196/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.0702 - acc: 0.9434 - val_loss: 0.0725 - val_acc: 0.9267\n",
      "Epoch 197/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0706 - acc: 0.9397 - val_loss: 0.0718 - val_acc: 0.9292\n",
      "Epoch 198/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0708 - acc: 0.9384 - val_loss: 0.0698 - val_acc: 0.9309\n",
      "Epoch 199/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0708 - acc: 0.9414 - val_loss: 0.0707 - val_acc: 0.9352\n",
      "Epoch 200/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0701 - acc: 0.9405 - val_loss: 0.0723 - val_acc: 0.9335\n",
      "Epoch 201/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.0697 - acc: 0.9423 - val_loss: 0.0693 - val_acc: 0.9327\n",
      "Epoch 202/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0699 - acc: 0.9418 - val_loss: 0.0691 - val_acc: 0.9301\n",
      "Epoch 203/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0690 - acc: 0.9407 - val_loss: 0.0703 - val_acc: 0.9284\n",
      "Epoch 204/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0698 - acc: 0.9430 - val_loss: 0.0694 - val_acc: 0.9327\n",
      "Epoch 205/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0696 - acc: 0.9420 - val_loss: 0.0676 - val_acc: 0.9284\n",
      "Epoch 206/500\n",
      "22272/22272 [==============================] - 12s 526us/step - loss: 0.0687 - acc: 0.9433 - val_loss: 0.0677 - val_acc: 0.9344\n",
      "Epoch 207/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0685 - acc: 0.9463 - val_loss: 0.0708 - val_acc: 0.9241\n",
      "Epoch 208/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0685 - acc: 0.9454 - val_loss: 0.0727 - val_acc: 0.9241\n",
      "Epoch 209/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0678 - acc: 0.9422 - val_loss: 0.0660 - val_acc: 0.9301\n",
      "Epoch 210/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0676 - acc: 0.9437 - val_loss: 0.0656 - val_acc: 0.9352\n",
      "Epoch 211/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0677 - acc: 0.9435 - val_loss: 0.0687 - val_acc: 0.9369\n",
      "Epoch 212/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0678 - acc: 0.9434 - val_loss: 0.0655 - val_acc: 0.9378\n",
      "Epoch 213/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0677 - acc: 0.9448 - val_loss: 0.0669 - val_acc: 0.9267\n",
      "Epoch 214/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0677 - acc: 0.9411 - val_loss: 0.0678 - val_acc: 0.9344\n",
      "Epoch 215/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0674 - acc: 0.9432 - val_loss: 0.0660 - val_acc: 0.9335\n",
      "Epoch 216/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0667 - acc: 0.9442 - val_loss: 0.0651 - val_acc: 0.9395\n",
      "Epoch 217/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0669 - acc: 0.9442 - val_loss: 0.0662 - val_acc: 0.9352\n",
      "Epoch 218/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0669 - acc: 0.9444 - val_loss: 0.0678 - val_acc: 0.9369\n",
      "Epoch 219/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0665 - acc: 0.9472 - val_loss: 0.0651 - val_acc: 0.9335\n",
      "Epoch 220/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0661 - acc: 0.9445 - val_loss: 0.0658 - val_acc: 0.9309\n",
      "Epoch 221/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0663 - acc: 0.9451 - val_loss: 0.0676 - val_acc: 0.9386\n",
      "Epoch 222/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0656 - acc: 0.9483 - val_loss: 0.0652 - val_acc: 0.9369\n",
      "Epoch 223/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0661 - acc: 0.9457 - val_loss: 0.0639 - val_acc: 0.9369\n",
      "Epoch 224/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0654 - acc: 0.9450 - val_loss: 0.0668 - val_acc: 0.9309\n",
      "Epoch 225/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0654 - acc: 0.9473 - val_loss: 0.0651 - val_acc: 0.9318\n",
      "Epoch 226/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0658 - acc: 0.9479 - val_loss: 0.0645 - val_acc: 0.9284\n",
      "Epoch 227/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0657 - acc: 0.9441 - val_loss: 0.0642 - val_acc: 0.9369\n",
      "Epoch 228/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0654 - acc: 0.9468 - val_loss: 0.0653 - val_acc: 0.9335\n",
      "Epoch 229/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0648 - acc: 0.9490 - val_loss: 0.0643 - val_acc: 0.9327\n",
      "Epoch 230/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0650 - acc: 0.9462 - val_loss: 0.0645 - val_acc: 0.9369\n",
      "Epoch 231/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.0649 - acc: 0.9485 - val_loss: 0.0646 - val_acc: 0.9361\n",
      "Epoch 232/500\n",
      "22272/22272 [==============================] - 12s 516us/step - loss: 0.0644 - acc: 0.9495 - val_loss: 0.0618 - val_acc: 0.9429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0643 - acc: 0.9490 - val_loss: 0.0642 - val_acc: 0.9403\n",
      "Epoch 234/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0634 - acc: 0.9495 - val_loss: 0.0655 - val_acc: 0.9250\n",
      "Epoch 235/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.0640 - acc: 0.9492 - val_loss: 0.0616 - val_acc: 0.9361\n",
      "Epoch 236/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.0638 - acc: 0.9500 - val_loss: 0.0636 - val_acc: 0.9361\n",
      "Epoch 237/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.0637 - acc: 0.9484 - val_loss: 0.0632 - val_acc: 0.9361\n",
      "Epoch 238/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0641 - acc: 0.9470 - val_loss: 0.0631 - val_acc: 0.9318\n",
      "Epoch 239/500\n",
      "22272/22272 [==============================] - 12s 523us/step - loss: 0.0640 - acc: 0.9467 - val_loss: 0.0643 - val_acc: 0.9395\n",
      "Epoch 240/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0635 - acc: 0.9491 - val_loss: 0.0633 - val_acc: 0.9420\n",
      "Epoch 241/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0632 - acc: 0.9485 - val_loss: 0.0621 - val_acc: 0.9369\n",
      "Epoch 242/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0633 - acc: 0.9498 - val_loss: 0.0617 - val_acc: 0.9395\n",
      "Epoch 243/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0626 - acc: 0.9512 - val_loss: 0.0623 - val_acc: 0.9378\n",
      "Epoch 244/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0635 - acc: 0.9500 - val_loss: 0.0633 - val_acc: 0.9361\n",
      "Epoch 245/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0626 - acc: 0.9509 - val_loss: 0.0629 - val_acc: 0.9403\n",
      "Epoch 246/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0625 - acc: 0.9501 - val_loss: 0.0628 - val_acc: 0.9412\n",
      "Epoch 247/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0627 - acc: 0.9492 - val_loss: 0.0625 - val_acc: 0.9344\n",
      "Epoch 248/500\n",
      "22272/22272 [==============================] - 11s 497us/step - loss: 0.0627 - acc: 0.9489 - val_loss: 0.0615 - val_acc: 0.9386\n",
      "Epoch 249/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.0621 - acc: 0.9520 - val_loss: 0.0649 - val_acc: 0.9361\n",
      "Epoch 250/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0617 - acc: 0.9506 - val_loss: 0.0617 - val_acc: 0.9369\n",
      "Epoch 251/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0620 - acc: 0.9508 - val_loss: 0.0615 - val_acc: 0.9378\n",
      "Epoch 252/500\n",
      "22272/22272 [==============================] - 11s 497us/step - loss: 0.0620 - acc: 0.9498 - val_loss: 0.0609 - val_acc: 0.9420\n",
      "Epoch 253/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.0621 - acc: 0.9520 - val_loss: 0.0608 - val_acc: 0.9395\n",
      "Epoch 254/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.0615 - acc: 0.9503 - val_loss: 0.0617 - val_acc: 0.9369\n",
      "Epoch 255/500\n",
      "22272/22272 [==============================] - 11s 493us/step - loss: 0.0614 - acc: 0.9518 - val_loss: 0.0592 - val_acc: 0.9403\n",
      "Epoch 256/500\n",
      "22272/22272 [==============================] - 11s 497us/step - loss: 0.0612 - acc: 0.9518 - val_loss: 0.0614 - val_acc: 0.9412\n",
      "Epoch 257/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0613 - acc: 0.9511 - val_loss: 0.0602 - val_acc: 0.9327\n",
      "Epoch 258/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0614 - acc: 0.9508 - val_loss: 0.0587 - val_acc: 0.9463\n",
      "Epoch 259/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0611 - acc: 0.9545 - val_loss: 0.0595 - val_acc: 0.9412\n",
      "Epoch 260/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0610 - acc: 0.9517 - val_loss: 0.0599 - val_acc: 0.9318\n",
      "Epoch 261/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0608 - acc: 0.9523 - val_loss: 0.0587 - val_acc: 0.9412\n",
      "Epoch 262/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0604 - acc: 0.9516 - val_loss: 0.0597 - val_acc: 0.9386\n",
      "Epoch 263/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0610 - acc: 0.9529 - val_loss: 0.0609 - val_acc: 0.9403\n",
      "Epoch 264/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0599 - acc: 0.9543 - val_loss: 0.0595 - val_acc: 0.9335\n",
      "Epoch 265/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0601 - acc: 0.9526 - val_loss: 0.0596 - val_acc: 0.9437\n",
      "Epoch 266/500\n",
      "22272/22272 [==============================] - 11s 497us/step - loss: 0.0596 - acc: 0.9535 - val_loss: 0.0573 - val_acc: 0.9403\n",
      "Epoch 267/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.0598 - acc: 0.9538 - val_loss: 0.0611 - val_acc: 0.9344\n",
      "Epoch 268/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.0599 - acc: 0.9533 - val_loss: 0.0603 - val_acc: 0.9301\n",
      "Epoch 269/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0598 - acc: 0.9544 - val_loss: 0.0599 - val_acc: 0.9412\n",
      "Epoch 270/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0597 - acc: 0.9523 - val_loss: 0.0590 - val_acc: 0.9395\n",
      "Epoch 271/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0594 - acc: 0.9529 - val_loss: 0.0581 - val_acc: 0.9429\n",
      "Epoch 272/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0592 - acc: 0.9541 - val_loss: 0.0598 - val_acc: 0.9395\n",
      "Epoch 273/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0587 - acc: 0.9542 - val_loss: 0.0588 - val_acc: 0.9369\n",
      "Epoch 274/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.0591 - acc: 0.9542 - val_loss: 0.0574 - val_acc: 0.9463\n",
      "Epoch 275/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0591 - acc: 0.9567 - val_loss: 0.0591 - val_acc: 0.9403\n",
      "Epoch 276/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0585 - acc: 0.9522 - val_loss: 0.0585 - val_acc: 0.9463\n",
      "Epoch 277/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0589 - acc: 0.9550 - val_loss: 0.0576 - val_acc: 0.9446\n",
      "Epoch 278/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0587 - acc: 0.9530 - val_loss: 0.0585 - val_acc: 0.9471\n",
      "Epoch 279/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0586 - acc: 0.9553 - val_loss: 0.0584 - val_acc: 0.9446\n",
      "Epoch 280/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0586 - acc: 0.9541 - val_loss: 0.0577 - val_acc: 0.9437\n",
      "Epoch 281/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.0588 - acc: 0.9538 - val_loss: 0.0572 - val_acc: 0.9480\n",
      "Epoch 282/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0583 - acc: 0.9542 - val_loss: 0.0566 - val_acc: 0.9471\n",
      "Epoch 283/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0582 - acc: 0.9556 - val_loss: 0.0581 - val_acc: 0.9437\n",
      "Epoch 284/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0582 - acc: 0.9547 - val_loss: 0.0562 - val_acc: 0.9454\n",
      "Epoch 285/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0586 - acc: 0.9542 - val_loss: 0.0582 - val_acc: 0.9420\n",
      "Epoch 286/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0579 - acc: 0.9558 - val_loss: 0.0563 - val_acc: 0.9420\n",
      "Epoch 287/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0573 - acc: 0.9554 - val_loss: 0.0570 - val_acc: 0.9471\n",
      "Epoch 288/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0584 - acc: 0.9551 - val_loss: 0.0550 - val_acc: 0.9446\n",
      "Epoch 289/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0569 - acc: 0.9552 - val_loss: 0.0560 - val_acc: 0.9454\n",
      "Epoch 290/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0577 - acc: 0.9561 - val_loss: 0.0556 - val_acc: 0.9463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.0574 - acc: 0.9545 - val_loss: 0.0552 - val_acc: 0.9446\n",
      "Epoch 292/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0572 - acc: 0.9576 - val_loss: 0.0558 - val_acc: 0.9471\n",
      "Epoch 293/500\n",
      "22272/22272 [==============================] - 11s 497us/step - loss: 0.0573 - acc: 0.9563 - val_loss: 0.0555 - val_acc: 0.9454\n",
      "Epoch 294/500\n",
      "22272/22272 [==============================] - 11s 494us/step - loss: 0.0575 - acc: 0.9564 - val_loss: 0.0567 - val_acc: 0.9454\n",
      "Epoch 295/500\n",
      "22272/22272 [==============================] - 11s 492us/step - loss: 0.0570 - acc: 0.9568 - val_loss: 0.0562 - val_acc: 0.9369\n",
      "Epoch 296/500\n",
      "22272/22272 [==============================] - 11s 492us/step - loss: 0.0573 - acc: 0.9564 - val_loss: 0.0565 - val_acc: 0.9446\n",
      "Epoch 297/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0569 - acc: 0.9565 - val_loss: 0.0553 - val_acc: 0.9446\n",
      "Epoch 298/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0570 - acc: 0.9572 - val_loss: 0.0555 - val_acc: 0.9454\n",
      "Epoch 299/500\n",
      "22272/22272 [==============================] - 11s 489us/step - loss: 0.0567 - acc: 0.9562 - val_loss: 0.0541 - val_acc: 0.9429\n",
      "Epoch 300/500\n",
      "22272/22272 [==============================] - 11s 492us/step - loss: 0.0563 - acc: 0.9585 - val_loss: 0.0541 - val_acc: 0.9480\n",
      "Epoch 301/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.0566 - acc: 0.9574 - val_loss: 0.0554 - val_acc: 0.9480\n",
      "Epoch 302/500\n",
      "22272/22272 [==============================] - 11s 493us/step - loss: 0.0569 - acc: 0.9577 - val_loss: 0.0551 - val_acc: 0.9412\n",
      "Epoch 303/500\n",
      "22272/22272 [==============================] - 11s 491us/step - loss: 0.0558 - acc: 0.9586 - val_loss: 0.0554 - val_acc: 0.9514\n",
      "Epoch 304/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0559 - acc: 0.9583 - val_loss: 0.0552 - val_acc: 0.9471\n",
      "Epoch 305/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.0560 - acc: 0.9570 - val_loss: 0.0555 - val_acc: 0.9497\n",
      "Epoch 306/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.0564 - acc: 0.9563 - val_loss: 0.0588 - val_acc: 0.9471\n",
      "Epoch 307/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0563 - acc: 0.9578 - val_loss: 0.0587 - val_acc: 0.9437\n",
      "Epoch 308/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0560 - acc: 0.9573 - val_loss: 0.0545 - val_acc: 0.9429\n",
      "Epoch 309/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0555 - acc: 0.9563 - val_loss: 0.0534 - val_acc: 0.9488\n",
      "Epoch 310/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.0553 - acc: 0.9583 - val_loss: 0.0557 - val_acc: 0.9437\n",
      "Epoch 311/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0550 - acc: 0.9582 - val_loss: 0.0537 - val_acc: 0.9497\n",
      "Epoch 312/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0555 - acc: 0.9569 - val_loss: 0.0549 - val_acc: 0.9531\n",
      "Epoch 313/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0559 - acc: 0.9572 - val_loss: 0.0563 - val_acc: 0.9506\n",
      "Epoch 314/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0557 - acc: 0.9571 - val_loss: 0.0563 - val_acc: 0.9471\n",
      "Epoch 315/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0554 - acc: 0.9573 - val_loss: 0.0543 - val_acc: 0.9429\n",
      "Epoch 316/500\n",
      "22272/22272 [==============================] - 11s 492us/step - loss: 0.0551 - acc: 0.9586 - val_loss: 0.0541 - val_acc: 0.9506\n",
      "Epoch 317/500\n",
      "22272/22272 [==============================] - 11s 497us/step - loss: 0.0550 - acc: 0.9566 - val_loss: 0.0546 - val_acc: 0.9471\n",
      "Epoch 318/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0549 - acc: 0.9589 - val_loss: 0.0587 - val_acc: 0.9454\n",
      "Epoch 319/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0548 - acc: 0.9583 - val_loss: 0.0548 - val_acc: 0.9480\n",
      "Epoch 320/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.0549 - acc: 0.9592 - val_loss: 0.0557 - val_acc: 0.9480\n",
      "Epoch 321/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0549 - acc: 0.9574 - val_loss: 0.0539 - val_acc: 0.9514\n",
      "Epoch 322/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0549 - acc: 0.9589 - val_loss: 0.0531 - val_acc: 0.9506\n",
      "Epoch 323/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0548 - acc: 0.9590 - val_loss: 0.0542 - val_acc: 0.9463\n",
      "Epoch 324/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0549 - acc: 0.9595 - val_loss: 0.0558 - val_acc: 0.9454\n",
      "Epoch 325/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0544 - acc: 0.9563 - val_loss: 0.0538 - val_acc: 0.9523\n",
      "Epoch 326/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0545 - acc: 0.9596 - val_loss: 0.0532 - val_acc: 0.9523\n",
      "Epoch 327/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0542 - acc: 0.9584 - val_loss: 0.0540 - val_acc: 0.9523\n",
      "Epoch 328/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0540 - acc: 0.9590 - val_loss: 0.0535 - val_acc: 0.9488\n",
      "Epoch 329/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0543 - acc: 0.9586 - val_loss: 0.0564 - val_acc: 0.9463\n",
      "Epoch 330/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0546 - acc: 0.9595 - val_loss: 0.0563 - val_acc: 0.9429\n",
      "Epoch 331/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0539 - acc: 0.9585 - val_loss: 0.0532 - val_acc: 0.9548\n",
      "Epoch 332/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0537 - acc: 0.9609 - val_loss: 0.0529 - val_acc: 0.9488\n",
      "Epoch 333/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0536 - acc: 0.9604 - val_loss: 0.0537 - val_acc: 0.9531\n",
      "Epoch 334/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0538 - acc: 0.9596 - val_loss: 0.0535 - val_acc: 0.9523\n",
      "Epoch 335/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0539 - acc: 0.9603 - val_loss: 0.0515 - val_acc: 0.9540\n",
      "Epoch 336/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.0539 - acc: 0.9611 - val_loss: 0.0513 - val_acc: 0.9497\n",
      "Epoch 337/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0534 - acc: 0.9611 - val_loss: 0.0530 - val_acc: 0.9523\n",
      "Epoch 338/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0535 - acc: 0.9584 - val_loss: 0.0519 - val_acc: 0.9471\n",
      "Epoch 339/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0536 - acc: 0.9608 - val_loss: 0.0537 - val_acc: 0.9497\n",
      "Epoch 340/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0530 - acc: 0.9621 - val_loss: 0.0527 - val_acc: 0.9488\n",
      "Epoch 341/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.0532 - acc: 0.9616 - val_loss: 0.0522 - val_acc: 0.9480\n",
      "Epoch 342/500\n",
      "22272/22272 [==============================] - 12s 516us/step - loss: 0.0533 - acc: 0.9600 - val_loss: 0.0531 - val_acc: 0.9480\n",
      "Epoch 343/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0531 - acc: 0.9611 - val_loss: 0.0531 - val_acc: 0.9514\n",
      "Epoch 344/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0531 - acc: 0.9610 - val_loss: 0.0529 - val_acc: 0.9463\n",
      "Epoch 345/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.0528 - acc: 0.9595 - val_loss: 0.0547 - val_acc: 0.9429\n",
      "Epoch 346/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0528 - acc: 0.9599 - val_loss: 0.0523 - val_acc: 0.9497\n",
      "Epoch 347/500\n",
      "22272/22272 [==============================] - 12s 523us/step - loss: 0.0527 - acc: 0.9614 - val_loss: 0.0549 - val_acc: 0.9446\n",
      "Epoch 348/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0527 - acc: 0.9593 - val_loss: 0.0536 - val_acc: 0.9420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349/500\n",
      "22272/22272 [==============================] - 11s 490us/step - loss: 0.0530 - acc: 0.9613 - val_loss: 0.0517 - val_acc: 0.9497\n",
      "Epoch 350/500\n",
      "22272/22272 [==============================] - 11s 489us/step - loss: 0.0526 - acc: 0.9621 - val_loss: 0.0516 - val_acc: 0.9540\n",
      "Epoch 351/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.0524 - acc: 0.9600 - val_loss: 0.0501 - val_acc: 0.9523\n",
      "Epoch 352/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0525 - acc: 0.9619 - val_loss: 0.0533 - val_acc: 0.9506\n",
      "Epoch 353/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.0522 - acc: 0.9615 - val_loss: 0.0521 - val_acc: 0.9480\n",
      "Epoch 354/500\n",
      "22272/22272 [==============================] - 11s 490us/step - loss: 0.0522 - acc: 0.9614 - val_loss: 0.0506 - val_acc: 0.9514\n",
      "Epoch 355/500\n",
      "22272/22272 [==============================] - 11s 488us/step - loss: 0.0520 - acc: 0.9612 - val_loss: 0.0521 - val_acc: 0.9497\n",
      "Epoch 356/500\n",
      "22272/22272 [==============================] - 11s 491us/step - loss: 0.0524 - acc: 0.9618 - val_loss: 0.0525 - val_acc: 0.9480\n",
      "Epoch 357/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0517 - acc: 0.9635 - val_loss: 0.0533 - val_acc: 0.9488\n",
      "Epoch 358/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0522 - acc: 0.9606 - val_loss: 0.0505 - val_acc: 0.9548\n",
      "Epoch 359/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0517 - acc: 0.9610 - val_loss: 0.0531 - val_acc: 0.9497\n",
      "Epoch 360/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0517 - acc: 0.9610 - val_loss: 0.0502 - val_acc: 0.9480\n",
      "Epoch 361/500\n",
      "22272/22272 [==============================] - 11s 494us/step - loss: 0.0518 - acc: 0.9609 - val_loss: 0.0515 - val_acc: 0.9506\n",
      "Epoch 362/500\n",
      "22272/22272 [==============================] - 11s 491us/step - loss: 0.0514 - acc: 0.9614 - val_loss: 0.0505 - val_acc: 0.9488\n",
      "Epoch 363/500\n",
      "22272/22272 [==============================] - 11s 494us/step - loss: 0.0517 - acc: 0.9609 - val_loss: 0.0515 - val_acc: 0.9540\n",
      "Epoch 364/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0516 - acc: 0.9598 - val_loss: 0.0502 - val_acc: 0.9531\n",
      "Epoch 365/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0511 - acc: 0.9619 - val_loss: 0.0522 - val_acc: 0.9497\n",
      "Epoch 366/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0513 - acc: 0.9617 - val_loss: 0.0514 - val_acc: 0.9540\n",
      "Epoch 367/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0515 - acc: 0.9608 - val_loss: 0.0509 - val_acc: 0.9506\n",
      "Epoch 368/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0516 - acc: 0.9612 - val_loss: 0.0503 - val_acc: 0.9531\n",
      "Epoch 369/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.0512 - acc: 0.9632 - val_loss: 0.0518 - val_acc: 0.9497\n",
      "Epoch 370/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0513 - acc: 0.9618 - val_loss: 0.0516 - val_acc: 0.9480\n",
      "Epoch 371/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.0511 - acc: 0.9626 - val_loss: 0.0503 - val_acc: 0.9514\n",
      "Epoch 372/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.0510 - acc: 0.9619 - val_loss: 0.0509 - val_acc: 0.9540\n",
      "Epoch 373/500\n",
      "22272/22272 [==============================] - 11s 494us/step - loss: 0.0505 - acc: 0.9606 - val_loss: 0.0505 - val_acc: 0.9437\n",
      "Epoch 374/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.0503 - acc: 0.9652 - val_loss: 0.0499 - val_acc: 0.9591\n",
      "Epoch 375/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0509 - acc: 0.9633 - val_loss: 0.0518 - val_acc: 0.9463\n",
      "Epoch 376/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0509 - acc: 0.9613 - val_loss: 0.0505 - val_acc: 0.9480\n",
      "Epoch 377/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0504 - acc: 0.9625 - val_loss: 0.0490 - val_acc: 0.9582\n",
      "Epoch 378/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0506 - acc: 0.9626 - val_loss: 0.0510 - val_acc: 0.9497\n",
      "Epoch 379/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0511 - acc: 0.9626 - val_loss: 0.0514 - val_acc: 0.9506\n",
      "Epoch 380/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0503 - acc: 0.9640 - val_loss: 0.0505 - val_acc: 0.9531\n",
      "Epoch 381/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0507 - acc: 0.9645 - val_loss: 0.0504 - val_acc: 0.9523\n",
      "Epoch 382/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0506 - acc: 0.9654 - val_loss: 0.0504 - val_acc: 0.9514\n",
      "Epoch 383/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0507 - acc: 0.9641 - val_loss: 0.0499 - val_acc: 0.9523\n",
      "Epoch 384/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0503 - acc: 0.9627 - val_loss: 0.0491 - val_acc: 0.9565\n",
      "Epoch 385/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0504 - acc: 0.9640 - val_loss: 0.0501 - val_acc: 0.9514\n",
      "Epoch 386/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0502 - acc: 0.9630 - val_loss: 0.0505 - val_acc: 0.9531\n",
      "Epoch 387/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.0507 - acc: 0.9628 - val_loss: 0.0516 - val_acc: 0.9531\n",
      "Epoch 388/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0498 - acc: 0.9644 - val_loss: 0.0499 - val_acc: 0.9506\n",
      "Epoch 389/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0501 - acc: 0.9617 - val_loss: 0.0520 - val_acc: 0.9497\n",
      "Epoch 390/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0502 - acc: 0.9622 - val_loss: 0.0515 - val_acc: 0.9497\n",
      "Epoch 391/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0501 - acc: 0.9635 - val_loss: 0.0503 - val_acc: 0.9548\n",
      "Epoch 392/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0497 - acc: 0.9650 - val_loss: 0.0505 - val_acc: 0.9497\n",
      "Epoch 393/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0498 - acc: 0.9637 - val_loss: 0.0502 - val_acc: 0.9514\n",
      "Epoch 394/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0500 - acc: 0.9644 - val_loss: 0.0483 - val_acc: 0.9574\n",
      "Epoch 395/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0496 - acc: 0.9637 - val_loss: 0.0531 - val_acc: 0.9488\n",
      "Epoch 396/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0501 - acc: 0.9636 - val_loss: 0.0504 - val_acc: 0.9523\n",
      "Epoch 397/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.0500 - acc: 0.9639 - val_loss: 0.0493 - val_acc: 0.9523\n",
      "Epoch 398/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0497 - acc: 0.9629 - val_loss: 0.0498 - val_acc: 0.9548\n",
      "Epoch 399/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0498 - acc: 0.9655 - val_loss: 0.0490 - val_acc: 0.9506\n",
      "Epoch 400/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0493 - acc: 0.9652 - val_loss: 0.0483 - val_acc: 0.9591\n",
      "Epoch 401/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0494 - acc: 0.9658 - val_loss: 0.0498 - val_acc: 0.9574\n",
      "Epoch 402/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0490 - acc: 0.9635 - val_loss: 0.0479 - val_acc: 0.9540\n",
      "Epoch 403/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0497 - acc: 0.9646 - val_loss: 0.0498 - val_acc: 0.9497\n",
      "Epoch 404/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0491 - acc: 0.9648 - val_loss: 0.0516 - val_acc: 0.9506\n",
      "Epoch 405/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0495 - acc: 0.9638 - val_loss: 0.0496 - val_acc: 0.9565\n",
      "Epoch 406/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0493 - acc: 0.9641 - val_loss: 0.0477 - val_acc: 0.9565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 407/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.0494 - acc: 0.9655 - val_loss: 0.0485 - val_acc: 0.9480\n",
      "Epoch 408/500\n",
      "22272/22272 [==============================] - 11s 489us/step - loss: 0.0490 - acc: 0.9648 - val_loss: 0.0481 - val_acc: 0.9523\n",
      "Epoch 409/500\n",
      "22272/22272 [==============================] - 11s 487us/step - loss: 0.0490 - acc: 0.9638 - val_loss: 0.0488 - val_acc: 0.9488\n",
      "Epoch 410/500\n",
      "22272/22272 [==============================] - 11s 492us/step - loss: 0.0489 - acc: 0.9630 - val_loss: 0.0485 - val_acc: 0.9574\n",
      "Epoch 411/500\n",
      "22272/22272 [==============================] - 11s 497us/step - loss: 0.0487 - acc: 0.9659 - val_loss: 0.0480 - val_acc: 0.9574\n",
      "Epoch 412/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0490 - acc: 0.9648 - val_loss: 0.0489 - val_acc: 0.9531\n",
      "Epoch 413/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.0487 - acc: 0.9657 - val_loss: 0.0486 - val_acc: 0.9540\n",
      "Epoch 414/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0489 - acc: 0.9643 - val_loss: 0.0491 - val_acc: 0.9557\n",
      "Epoch 415/500\n",
      "22272/22272 [==============================] - 11s 493us/step - loss: 0.0489 - acc: 0.9639 - val_loss: 0.0477 - val_acc: 0.9557\n",
      "Epoch 416/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.0487 - acc: 0.9657 - val_loss: 0.0476 - val_acc: 0.9574\n",
      "Epoch 417/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.0485 - acc: 0.9660 - val_loss: 0.0481 - val_acc: 0.9506\n",
      "Epoch 418/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0487 - acc: 0.9655 - val_loss: 0.0488 - val_acc: 0.9523\n",
      "Epoch 419/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0480 - acc: 0.9648 - val_loss: 0.0484 - val_acc: 0.9565\n",
      "Epoch 420/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0487 - acc: 0.9639 - val_loss: 0.0479 - val_acc: 0.9565\n",
      "Epoch 421/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.0485 - acc: 0.9657 - val_loss: 0.0492 - val_acc: 0.9574\n",
      "Epoch 422/500\n",
      "22272/22272 [==============================] - 11s 493us/step - loss: 0.0481 - acc: 0.9666 - val_loss: 0.0473 - val_acc: 0.9574\n",
      "Epoch 423/500\n",
      "22272/22272 [==============================] - 11s 497us/step - loss: 0.0484 - acc: 0.9665 - val_loss: 0.0482 - val_acc: 0.9557\n",
      "Epoch 424/500\n",
      "22272/22272 [==============================] - 11s 493us/step - loss: 0.0484 - acc: 0.9645 - val_loss: 0.0471 - val_acc: 0.9557\n",
      "Epoch 425/500\n",
      "22272/22272 [==============================] - 11s 492us/step - loss: 0.0482 - acc: 0.9657 - val_loss: 0.0468 - val_acc: 0.9565\n",
      "Epoch 426/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0485 - acc: 0.9635 - val_loss: 0.0479 - val_acc: 0.9523\n",
      "Epoch 427/500\n",
      "22272/22272 [==============================] - 11s 494us/step - loss: 0.0481 - acc: 0.9655 - val_loss: 0.0495 - val_acc: 0.9557\n",
      "Epoch 428/500\n",
      "22272/22272 [==============================] - 11s 491us/step - loss: 0.0480 - acc: 0.9637 - val_loss: 0.0466 - val_acc: 0.9565\n",
      "Epoch 429/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0480 - acc: 0.9651 - val_loss: 0.0469 - val_acc: 0.9548\n",
      "Epoch 430/500\n",
      "22272/22272 [==============================] - 11s 492us/step - loss: 0.0482 - acc: 0.9647 - val_loss: 0.0474 - val_acc: 0.9565\n",
      "Epoch 431/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0476 - acc: 0.9666 - val_loss: 0.0474 - val_acc: 0.9548\n",
      "Epoch 432/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0476 - acc: 0.9649 - val_loss: 0.0477 - val_acc: 0.9591\n",
      "Epoch 433/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0475 - acc: 0.9663 - val_loss: 0.0486 - val_acc: 0.9531\n",
      "Epoch 434/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0480 - acc: 0.9643 - val_loss: 0.0472 - val_acc: 0.9557\n",
      "Epoch 435/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0478 - acc: 0.9658 - val_loss: 0.0474 - val_acc: 0.9557\n",
      "Epoch 436/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0479 - acc: 0.9666 - val_loss: 0.0469 - val_acc: 0.9565\n",
      "Epoch 437/500\n",
      "22272/22272 [==============================] - 11s 493us/step - loss: 0.0472 - acc: 0.9648 - val_loss: 0.0473 - val_acc: 0.9565\n",
      "Epoch 438/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.0479 - acc: 0.9649 - val_loss: 0.0467 - val_acc: 0.9599\n",
      "Epoch 439/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0474 - acc: 0.9674 - val_loss: 0.0480 - val_acc: 0.9591\n",
      "Epoch 440/500\n",
      "22272/22272 [==============================] - 13s 575us/step - loss: 0.0478 - acc: 0.9664 - val_loss: 0.0464 - val_acc: 0.9582\n",
      "Epoch 441/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0476 - acc: 0.9662 - val_loss: 0.0463 - val_acc: 0.9540\n",
      "Epoch 442/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0477 - acc: 0.9662 - val_loss: 0.0469 - val_acc: 0.9565\n",
      "Epoch 443/500\n",
      "22272/22272 [==============================] - 12s 530us/step - loss: 0.0472 - acc: 0.9669 - val_loss: 0.0479 - val_acc: 0.9557\n",
      "Epoch 444/500\n",
      "22272/22272 [==============================] - 12s 548us/step - loss: 0.0473 - acc: 0.9662 - val_loss: 0.0458 - val_acc: 0.9599\n",
      "Epoch 445/500\n",
      "22272/22272 [==============================] - 13s 563us/step - loss: 0.0477 - acc: 0.9657 - val_loss: 0.0479 - val_acc: 0.9599\n",
      "Epoch 446/500\n",
      "22272/22272 [==============================] - 13s 565us/step - loss: 0.0471 - acc: 0.9667 - val_loss: 0.0482 - val_acc: 0.9574\n",
      "Epoch 447/500\n",
      "22272/22272 [==============================] - 14s 642us/step - loss: 0.0471 - acc: 0.9660 - val_loss: 0.0465 - val_acc: 0.9557\n",
      "Epoch 448/500\n",
      "22272/22272 [==============================] - 13s 584us/step - loss: 0.0474 - acc: 0.9669 - val_loss: 0.0457 - val_acc: 0.9557\n",
      "Epoch 449/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.0472 - acc: 0.9665 - val_loss: 0.0479 - val_acc: 0.9531\n",
      "Epoch 450/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0468 - acc: 0.9656 - val_loss: 0.0462 - val_acc: 0.9591\n",
      "Epoch 451/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0471 - acc: 0.9668 - val_loss: 0.0466 - val_acc: 0.9574\n",
      "Epoch 452/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.0469 - acc: 0.9663 - val_loss: 0.0464 - val_acc: 0.9574\n",
      "Epoch 453/500\n",
      "22272/22272 [==============================] - 12s 529us/step - loss: 0.0471 - acc: 0.9675 - val_loss: 0.0462 - val_acc: 0.9625\n",
      "Epoch 454/500\n",
      "22272/22272 [==============================] - 12s 550us/step - loss: 0.0471 - acc: 0.9658 - val_loss: 0.0459 - val_acc: 0.9582\n",
      "Epoch 455/500\n",
      "22272/22272 [==============================] - 12s 554us/step - loss: 0.0471 - acc: 0.9659 - val_loss: 0.0468 - val_acc: 0.9514\n",
      "Epoch 456/500\n",
      "22272/22272 [==============================] - 12s 548us/step - loss: 0.0469 - acc: 0.9682 - val_loss: 0.0457 - val_acc: 0.9548\n",
      "Epoch 457/500\n",
      "22272/22272 [==============================] - 12s 524us/step - loss: 0.0470 - acc: 0.9678 - val_loss: 0.0477 - val_acc: 0.9523\n",
      "Epoch 458/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.0467 - acc: 0.9654 - val_loss: 0.0454 - val_acc: 0.9582\n",
      "Epoch 459/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0468 - acc: 0.9678 - val_loss: 0.0468 - val_acc: 0.9565\n",
      "Epoch 460/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0471 - acc: 0.9675 - val_loss: 0.0459 - val_acc: 0.9565\n",
      "Epoch 461/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0470 - acc: 0.9675 - val_loss: 0.0459 - val_acc: 0.9582\n",
      "Epoch 462/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0469 - acc: 0.9659 - val_loss: 0.0461 - val_acc: 0.9591\n",
      "Epoch 463/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0469 - acc: 0.9677 - val_loss: 0.0460 - val_acc: 0.9565\n",
      "Epoch 464/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0470 - acc: 0.9650 - val_loss: 0.0470 - val_acc: 0.9557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.0463 - acc: 0.9658 - val_loss: 0.0452 - val_acc: 0.9574\n",
      "Epoch 466/500\n",
      "22272/22272 [==============================] - 11s 491us/step - loss: 0.0464 - acc: 0.9670 - val_loss: 0.0452 - val_acc: 0.9625\n",
      "Epoch 467/500\n",
      "22272/22272 [==============================] - 11s 490us/step - loss: 0.0466 - acc: 0.9673 - val_loss: 0.0464 - val_acc: 0.9540\n",
      "Epoch 468/500\n",
      "22272/22272 [==============================] - 11s 490us/step - loss: 0.0463 - acc: 0.9672 - val_loss: 0.0473 - val_acc: 0.9582\n",
      "Epoch 469/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0464 - acc: 0.9690 - val_loss: 0.0460 - val_acc: 0.9548\n",
      "Epoch 470/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0465 - acc: 0.9674 - val_loss: 0.0454 - val_acc: 0.9591\n",
      "Epoch 471/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0462 - acc: 0.9681 - val_loss: 0.0456 - val_acc: 0.9616\n",
      "Epoch 472/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0462 - acc: 0.9684 - val_loss: 0.0455 - val_acc: 0.9591\n",
      "Epoch 473/500\n",
      "22272/22272 [==============================] - 11s 493us/step - loss: 0.0462 - acc: 0.9676 - val_loss: 0.0475 - val_acc: 0.9599\n",
      "Epoch 474/500\n",
      "22272/22272 [==============================] - 11s 497us/step - loss: 0.0466 - acc: 0.9658 - val_loss: 0.0452 - val_acc: 0.9548\n",
      "Epoch 475/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0455 - acc: 0.9688 - val_loss: 0.0459 - val_acc: 0.9608\n",
      "Epoch 476/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0463 - acc: 0.9677 - val_loss: 0.0453 - val_acc: 0.9574\n",
      "Epoch 477/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0461 - acc: 0.9672 - val_loss: 0.0462 - val_acc: 0.9565\n",
      "Epoch 478/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0458 - acc: 0.9667 - val_loss: 0.0452 - val_acc: 0.9574\n",
      "Epoch 479/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0460 - acc: 0.9679 - val_loss: 0.0458 - val_acc: 0.9608\n",
      "Epoch 480/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0464 - acc: 0.9661 - val_loss: 0.0462 - val_acc: 0.9625\n",
      "Epoch 481/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0461 - acc: 0.9669 - val_loss: 0.0463 - val_acc: 0.9574\n",
      "Epoch 482/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0459 - acc: 0.9676 - val_loss: 0.0448 - val_acc: 0.9625\n",
      "Epoch 483/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.0458 - acc: 0.9672 - val_loss: 0.0456 - val_acc: 0.9591\n",
      "Epoch 484/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0457 - acc: 0.9676 - val_loss: 0.0442 - val_acc: 0.9565\n",
      "Epoch 485/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0455 - acc: 0.9680 - val_loss: 0.0445 - val_acc: 0.9582\n",
      "Epoch 486/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0457 - acc: 0.9665 - val_loss: 0.0457 - val_acc: 0.9574\n",
      "Epoch 487/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.0460 - acc: 0.9661 - val_loss: 0.0466 - val_acc: 0.9582\n",
      "Epoch 488/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0457 - acc: 0.9681 - val_loss: 0.0449 - val_acc: 0.9574\n",
      "Epoch 489/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0456 - acc: 0.9688 - val_loss: 0.0454 - val_acc: 0.9565\n",
      "Epoch 490/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0459 - acc: 0.9670 - val_loss: 0.0470 - val_acc: 0.9574\n",
      "Epoch 491/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0454 - acc: 0.9663 - val_loss: 0.0452 - val_acc: 0.9625\n",
      "Epoch 492/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0456 - acc: 0.9674 - val_loss: 0.0457 - val_acc: 0.9582\n",
      "Epoch 493/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0453 - acc: 0.9665 - val_loss: 0.0466 - val_acc: 0.9565\n",
      "Epoch 494/500\n",
      "22272/22272 [==============================] - 12s 541us/step - loss: 0.0455 - acc: 0.9675 - val_loss: 0.0439 - val_acc: 0.9599\n",
      "Epoch 495/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0451 - acc: 0.9699 - val_loss: 0.0455 - val_acc: 0.9582\n",
      "Epoch 496/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0452 - acc: 0.9693 - val_loss: 0.0448 - val_acc: 0.9565\n",
      "Epoch 497/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0451 - acc: 0.9689 - val_loss: 0.0440 - val_acc: 0.9591\n",
      "Epoch 498/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0451 - acc: 0.9706 - val_loss: 0.0438 - val_acc: 0.9616\n",
      "Epoch 499/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0453 - acc: 0.9659 - val_loss: 0.0463 - val_acc: 0.9599\n",
      "Epoch 500/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0458 - acc: 0.9665 - val_loss: 0.0448 - val_acc: 0.9591\n"
     ]
    }
   ],
   "source": [
    "##Fitting the GRU to the Training set\n",
    "hist3=model3.fit(X_train, y_train, batch_size=300, epochs=500, validation_split=0.05, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22272 samples, validate on 1173 samples\n",
      "Epoch 1/500\n",
      "22272/22272 [==============================] - 15s 658us/step - loss: 0.2354 - acc: 0.7479 - val_loss: 0.2244 - val_acc: 0.7298\n",
      "Epoch 2/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.2090 - acc: 0.7491 - val_loss: 0.2107 - val_acc: 0.7298\n",
      "Epoch 3/500\n",
      "22272/22272 [==============================] - 11s 494us/step - loss: 0.2015 - acc: 0.7500 - val_loss: 0.2061 - val_acc: 0.7383\n",
      "Epoch 4/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.1978 - acc: 0.7535 - val_loss: 0.2062 - val_acc: 0.7442\n",
      "Epoch 5/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.1978 - acc: 0.7530 - val_loss: 0.2022 - val_acc: 0.7332\n",
      "Epoch 6/500\n",
      "22272/22272 [==============================] - 12s 526us/step - loss: 0.1971 - acc: 0.7552 - val_loss: 0.2022 - val_acc: 0.7357\n",
      "Epoch 7/500\n",
      "22272/22272 [==============================] - 12s 559us/step - loss: 0.1952 - acc: 0.7562 - val_loss: 0.1995 - val_acc: 0.7383\n",
      "Epoch 8/500\n",
      "22272/22272 [==============================] - 12s 526us/step - loss: 0.1951 - acc: 0.7540 - val_loss: 0.2005 - val_acc: 0.7425\n",
      "Epoch 9/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.1920 - acc: 0.7549 - val_loss: 0.1953 - val_acc: 0.7391\n",
      "Epoch 10/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.1887 - acc: 0.7517 - val_loss: 0.1921 - val_acc: 0.7366\n",
      "Epoch 11/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.1872 - acc: 0.7483 - val_loss: 0.1895 - val_acc: 0.7434\n",
      "Epoch 12/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.1855 - acc: 0.7532 - val_loss: 0.1912 - val_acc: 0.7442\n",
      "Epoch 13/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.1848 - acc: 0.7529 - val_loss: 0.1883 - val_acc: 0.7366\n",
      "Epoch 14/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.1819 - acc: 0.7603 - val_loss: 0.1957 - val_acc: 0.7442\n",
      "Epoch 15/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.1781 - acc: 0.7657 - val_loss: 0.1813 - val_acc: 0.7536\n",
      "Epoch 16/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.1750 - acc: 0.7705 - val_loss: 0.1798 - val_acc: 0.7519\n",
      "Epoch 17/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.1720 - acc: 0.7767 - val_loss: 0.1783 - val_acc: 0.7724\n",
      "Epoch 18/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.1706 - acc: 0.7831 - val_loss: 0.1752 - val_acc: 0.7673\n",
      "Epoch 19/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.1690 - acc: 0.7839 - val_loss: 0.1735 - val_acc: 0.7749\n",
      "Epoch 20/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.1685 - acc: 0.7846 - val_loss: 0.1744 - val_acc: 0.7553\n",
      "Epoch 21/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.1664 - acc: 0.7919 - val_loss: 0.1731 - val_acc: 0.7715\n",
      "Epoch 22/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.1653 - acc: 0.7909 - val_loss: 0.1736 - val_acc: 0.7698\n",
      "Epoch 23/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.1646 - acc: 0.7940 - val_loss: 0.1700 - val_acc: 0.7664\n",
      "Epoch 24/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.1641 - acc: 0.7949 - val_loss: 0.1691 - val_acc: 0.7783\n",
      "Epoch 25/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.1630 - acc: 0.7967 - val_loss: 0.1683 - val_acc: 0.7835\n",
      "Epoch 26/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.1625 - acc: 0.7975 - val_loss: 0.1667 - val_acc: 0.7792\n",
      "Epoch 27/500\n",
      "22272/22272 [==============================] - 12s 537us/step - loss: 0.1625 - acc: 0.7961 - val_loss: 0.1679 - val_acc: 0.7647\n",
      "Epoch 28/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.1604 - acc: 0.8016 - val_loss: 0.1697 - val_acc: 0.7741\n",
      "Epoch 29/500\n",
      "22272/22272 [==============================] - 14s 625us/step - loss: 0.1592 - acc: 0.8023 - val_loss: 0.1672 - val_acc: 0.7775\n",
      "Epoch 30/500\n",
      "22272/22272 [==============================] - 12s 546us/step - loss: 0.1593 - acc: 0.8030 - val_loss: 0.1647 - val_acc: 0.7860\n",
      "Epoch 31/500\n",
      "22272/22272 [==============================] - 12s 531us/step - loss: 0.1573 - acc: 0.8059 - val_loss: 0.1677 - val_acc: 0.7783\n",
      "Epoch 32/500\n",
      "22272/22272 [==============================] - 12s 530us/step - loss: 0.1572 - acc: 0.8051 - val_loss: 0.1649 - val_acc: 0.7843\n",
      "Epoch 33/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.1571 - acc: 0.8076 - val_loss: 0.1617 - val_acc: 0.7928\n",
      "Epoch 34/500\n",
      "22272/22272 [==============================] - 12s 529us/step - loss: 0.1547 - acc: 0.8086 - val_loss: 0.1599 - val_acc: 0.7894\n",
      "Epoch 35/500\n",
      "22272/22272 [==============================] - 12s 535us/step - loss: 0.1542 - acc: 0.8120 - val_loss: 0.1595 - val_acc: 0.7877\n",
      "Epoch 36/500\n",
      "22272/22272 [==============================] - 14s 608us/step - loss: 0.1522 - acc: 0.8139 - val_loss: 0.1604 - val_acc: 0.8014\n",
      "Epoch 37/500\n",
      "22272/22272 [==============================] - 15s 670us/step - loss: 0.1518 - acc: 0.8124 - val_loss: 0.1569 - val_acc: 0.7911\n",
      "Epoch 38/500\n",
      "22272/22272 [==============================] - 13s 604us/step - loss: 0.1521 - acc: 0.8149 - val_loss: 0.1571 - val_acc: 0.7920\n",
      "Epoch 39/500\n",
      "22272/22272 [==============================] - 14s 628us/step - loss: 0.1510 - acc: 0.8165 - val_loss: 0.1553 - val_acc: 0.7980\n",
      "Epoch 40/500\n",
      "22272/22272 [==============================] - 14s 624us/step - loss: 0.1490 - acc: 0.8173 - val_loss: 0.1568 - val_acc: 0.7988\n",
      "Epoch 41/500\n",
      "22272/22272 [==============================] - 15s 661us/step - loss: 0.1492 - acc: 0.8173 - val_loss: 0.1539 - val_acc: 0.8031s - loss: 0\n",
      "Epoch 42/500\n",
      "22272/22272 [==============================] - 16s 719us/step - loss: 0.1471 - acc: 0.8201 - val_loss: 0.1547 - val_acc: 0.7988\n",
      "Epoch 43/500\n",
      "22272/22272 [==============================] - 14s 631us/step - loss: 0.1459 - acc: 0.8228 - val_loss: 0.1515 - val_acc: 0.7971\n",
      "Epoch 44/500\n",
      "22272/22272 [==============================] - 14s 623us/step - loss: 0.1454 - acc: 0.8237 - val_loss: 0.1508 - val_acc: 0.8005\n",
      "Epoch 45/500\n",
      "22272/22272 [==============================] - 14s 633us/step - loss: 0.1462 - acc: 0.8237 - val_loss: 0.1478 - val_acc: 0.8124\n",
      "Epoch 46/500\n",
      "22272/22272 [==============================] - 14s 645us/step - loss: 0.1433 - acc: 0.8244 - val_loss: 0.1477 - val_acc: 0.8116\n",
      "Epoch 47/500\n",
      "22272/22272 [==============================] - 14s 643us/step - loss: 0.1424 - acc: 0.8282 - val_loss: 0.1476 - val_acc: 0.8150\n",
      "Epoch 48/500\n",
      "22272/22272 [==============================] - 14s 632us/step - loss: 0.1435 - acc: 0.8276 - val_loss: 0.1443 - val_acc: 0.8201\n",
      "Epoch 49/500\n",
      "22272/22272 [==============================] - 14s 638us/step - loss: 0.1412 - acc: 0.8299 - val_loss: 0.1497 - val_acc: 0.8065\n",
      "Epoch 50/500\n",
      "22272/22272 [==============================] - 14s 628us/step - loss: 0.1405 - acc: 0.8283 - val_loss: 0.1442 - val_acc: 0.8312\n",
      "Epoch 51/500\n",
      "22272/22272 [==============================] - 14s 629us/step - loss: 0.1396 - acc: 0.8329 - val_loss: 0.1437 - val_acc: 0.8193\n",
      "Epoch 52/500\n",
      "22272/22272 [==============================] - 14s 634us/step - loss: 0.1393 - acc: 0.8332 - val_loss: 0.1432 - val_acc: 0.8244\n",
      "Epoch 53/500\n",
      "22272/22272 [==============================] - 15s 672us/step - loss: 0.1376 - acc: 0.8360 - val_loss: 0.1414 - val_acc: 0.8244\n",
      "Epoch 54/500\n",
      "22272/22272 [==============================] - 15s 667us/step - loss: 0.1359 - acc: 0.8395 - val_loss: 0.1381 - val_acc: 0.8201\n",
      "Epoch 55/500\n",
      "22272/22272 [==============================] - 14s 645us/step - loss: 0.1359 - acc: 0.8386 - val_loss: 0.1416 - val_acc: 0.826962 - acc - ETA: 6s - loss: 0.1356 - acc: 0.8 - ETA: 5s\n",
      "Epoch 56/500\n",
      "22272/22272 [==============================] - 14s 646us/step - loss: 0.1346 - acc: 0.8435 - val_loss: 0.1368 - val_acc: 0.8235\n",
      "Epoch 57/500\n",
      "22272/22272 [==============================] - 14s 643us/step - loss: 0.1333 - acc: 0.8444 - val_loss: 0.1350 - val_acc: 0.8278\n",
      "Epoch 58/500\n",
      "22272/22272 [==============================] - 15s 669us/step - loss: 0.1329 - acc: 0.8457 - val_loss: 0.1321 - val_acc: 0.8465\n",
      "Epoch 59/500\n",
      "22272/22272 [==============================] - 13s 596us/step - loss: 0.1311 - acc: 0.8476 - val_loss: 0.1390 - val_acc: 0.8269\n",
      "Epoch 60/500\n",
      "22272/22272 [==============================] - 13s 592us/step - loss: 0.1319 - acc: 0.8457 - val_loss: 0.1310 - val_acc: 0.8406\n",
      "Epoch 61/500\n",
      "22272/22272 [==============================] - 13s 601us/step - loss: 0.1309 - acc: 0.8486 - val_loss: 0.1317 - val_acc: 0.8457\n",
      "Epoch 62/500\n",
      "22272/22272 [==============================] - 14s 612us/step - loss: 0.1293 - acc: 0.8510 - val_loss: 0.1300 - val_acc: 0.8500\n",
      "Epoch 63/500\n",
      "22272/22272 [==============================] - 13s 604us/step - loss: 0.1286 - acc: 0.8537 - val_loss: 0.1349 - val_acc: 0.8355loss\n",
      "Epoch 64/500\n",
      "22272/22272 [==============================] - 14s 614us/step - loss: 0.1267 - acc: 0.8545 - val_loss: 0.1274 - val_acc: 0.8431-  - ETA: 3s - loss\n",
      "Epoch 65/500\n",
      "22272/22272 [==============================] - 14s 607us/step - loss: 0.1265 - acc: 0.8566 - val_loss: 0.1292 - val_acc: 0.8542\n",
      "Epoch 66/500\n",
      "22272/22272 [==============================] - 13s 602us/step - loss: 0.1260 - acc: 0.8590 - val_loss: 0.1282 - val_acc: 0.8517\n",
      "Epoch 67/500\n",
      "22272/22272 [==============================] - 13s 606us/step - loss: 0.1248 - acc: 0.8610 - val_loss: 0.1278 - val_acc: 0.8397\n",
      "Epoch 68/500\n",
      "22272/22272 [==============================] - 13s 605us/step - loss: 0.1246 - acc: 0.8596 - val_loss: 0.1228 - val_acc: 0.8568\n",
      "Epoch 69/500\n",
      "22272/22272 [==============================] - 13s 597us/step - loss: 0.1235 - acc: 0.8606 - val_loss: 0.1239 - val_acc: 0.8568\n",
      "Epoch 70/500\n",
      "22272/22272 [==============================] - 13s 602us/step - loss: 0.1234 - acc: 0.8620 - val_loss: 0.1230 - val_acc: 0.8534\n",
      "Epoch 71/500\n",
      "22272/22272 [==============================] - 14s 619us/step - loss: 0.1220 - acc: 0.8639 - val_loss: 0.1297 - val_acc: 0.8483\n",
      "Epoch 72/500\n",
      "22272/22272 [==============================] - 13s 601us/step - loss: 0.1216 - acc: 0.8670 - val_loss: 0.1214 - val_acc: 0.8610\n",
      "Epoch 73/500\n",
      "22272/22272 [==============================] - 14s 621us/step - loss: 0.1202 - acc: 0.8684 - val_loss: 0.1221 - val_acc: 0.8517\n",
      "Epoch 74/500\n",
      "22272/22272 [==============================] - 14s 616us/step - loss: 0.1190 - acc: 0.8688 - val_loss: 0.1201 - val_acc: 0.8602\n",
      "Epoch 75/500\n",
      "22272/22272 [==============================] - 14s 621us/step - loss: 0.1195 - acc: 0.8664 - val_loss: 0.1187 - val_acc: 0.8704\n",
      "Epoch 76/500\n",
      "22272/22272 [==============================] - 14s 621us/step - loss: 0.1179 - acc: 0.8708 - val_loss: 0.1227 - val_acc: 0.8525\n",
      "Epoch 77/500\n",
      "22272/22272 [==============================] - 14s 615us/step - loss: 0.1176 - acc: 0.8730 - val_loss: 0.1188 - val_acc: 0.8568\n",
      "Epoch 78/500\n",
      "22272/22272 [==============================] - 13s 602us/step - loss: 0.1165 - acc: 0.8739 - val_loss: 0.1192 - val_acc: 0.8687\n",
      "Epoch 79/500\n",
      "22272/22272 [==============================] - 14s 611us/step - loss: 0.1154 - acc: 0.8747 - val_loss: 0.1145 - val_acc: 0.8696\n",
      "Epoch 80/500\n",
      "22272/22272 [==============================] - 14s 620us/step - loss: 0.1150 - acc: 0.8753 - val_loss: 0.1185 - val_acc: 0.8645\n",
      "Epoch 81/500\n",
      "22272/22272 [==============================] - 14s 620us/step - loss: 0.1151 - acc: 0.8748 - val_loss: 0.1135 - val_acc: 0.8696\n",
      "Epoch 82/500\n",
      "22272/22272 [==============================] - 14s 622us/step - loss: 0.1137 - acc: 0.8754 - val_loss: 0.1139 - val_acc: 0.8602\n",
      "Epoch 83/500\n",
      "22272/22272 [==============================] - 13s 605us/step - loss: 0.1128 - acc: 0.8769 - val_loss: 0.1114 - val_acc: 0.8662\n",
      "Epoch 84/500\n",
      "22272/22272 [==============================] - 14s 606us/step - loss: 0.1118 - acc: 0.8798 - val_loss: 0.1137 - val_acc: 0.8730\n",
      "Epoch 85/500\n",
      "22272/22272 [==============================] - 13s 592us/step - loss: 0.1115 - acc: 0.8796 - val_loss: 0.1114 - val_acc: 0.8721\n",
      "Epoch 86/500\n",
      "22272/22272 [==============================] - 13s 602us/step - loss: 0.1115 - acc: 0.8810 - val_loss: 0.1112 - val_acc: 0.8789\n",
      "Epoch 87/500\n",
      "22272/22272 [==============================] - 14s 615us/step - loss: 0.1103 - acc: 0.8806 - val_loss: 0.1111 - val_acc: 0.8755\n",
      "Epoch 88/500\n",
      "22272/22272 [==============================] - 14s 631us/step - loss: 0.1103 - acc: 0.8787 - val_loss: 0.1122 - val_acc: 0.8789\n",
      "Epoch 89/500\n",
      "22272/22272 [==============================] - 14s 626us/step - loss: 0.1095 - acc: 0.8834 - val_loss: 0.1111 - val_acc: 0.8747\n",
      "Epoch 90/500\n",
      "22272/22272 [==============================] - 14s 624us/step - loss: 0.1086 - acc: 0.8842 - val_loss: 0.1060 - val_acc: 0.8772.1085 -  - ETA: 8s - loss: - ETA: 4s -\n",
      "Epoch 91/500\n",
      "22272/22272 [==============================] - 14s 616us/step - loss: 0.1073 - acc: 0.8870 - val_loss: 0.1098 - val_acc: 0.8747\n",
      "Epoch 92/500\n",
      "22272/22272 [==============================] - 14s 615us/step - loss: 0.1068 - acc: 0.8884 - val_loss: 0.1129 - val_acc: 0.8713\n",
      "Epoch 93/500\n",
      "22272/22272 [==============================] - 14s 627us/step - loss: 0.1068 - acc: 0.8858 - val_loss: 0.1096 - val_acc: 0.8730\n",
      "Epoch 94/500\n",
      "22272/22272 [==============================] - 14s 618us/step - loss: 0.1071 - acc: 0.8861 - val_loss: 0.1042 - val_acc: 0.8806\n",
      "Epoch 95/500\n",
      "22272/22272 [==============================] - 12s 524us/step - loss: 0.1054 - acc: 0.8891 - val_loss: 0.1060 - val_acc: 0.8747\n",
      "Epoch 96/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.1050 - acc: 0.8910 - val_loss: 0.1048 - val_acc: 0.8772\n",
      "Epoch 97/500\n",
      "22272/22272 [==============================] - 12s 558us/step - loss: 0.1041 - acc: 0.8931 - val_loss: 0.1036 - val_acc: 0.8781\n",
      "Epoch 98/500\n",
      "22272/22272 [==============================] - 12s 560us/step - loss: 0.1041 - acc: 0.8917 - val_loss: 0.1044 - val_acc: 0.8815\n",
      "Epoch 99/500\n",
      "22272/22272 [==============================] - 12s 525us/step - loss: 0.1031 - acc: 0.8930 - val_loss: 0.1053 - val_acc: 0.8798\n",
      "Epoch 100/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.1024 - acc: 0.8929 - val_loss: 0.1020 - val_acc: 0.8849\n",
      "Epoch 101/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.1022 - acc: 0.8933 - val_loss: 0.1009 - val_acc: 0.8875\n",
      "Epoch 102/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.1022 - acc: 0.8941 - val_loss: 0.1001 - val_acc: 0.8832\n",
      "Epoch 103/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.1012 - acc: 0.8956 - val_loss: 0.1035 - val_acc: 0.8781\n",
      "Epoch 104/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.1002 - acc: 0.8979 - val_loss: 0.0981 - val_acc: 0.8824\n",
      "Epoch 105/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.1002 - acc: 0.8983 - val_loss: 0.1017 - val_acc: 0.8900\n",
      "Epoch 106/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0995 - acc: 0.8985 - val_loss: 0.0990 - val_acc: 0.8900\n",
      "Epoch 107/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0991 - acc: 0.8989 - val_loss: 0.0980 - val_acc: 0.8892\n",
      "Epoch 108/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.0979 - acc: 0.9001 - val_loss: 0.1015 - val_acc: 0.8789\n",
      "Epoch 109/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0984 - acc: 0.8998 - val_loss: 0.1003 - val_acc: 0.8875\n",
      "Epoch 110/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0979 - acc: 0.9019 - val_loss: 0.0969 - val_acc: 0.8977\n",
      "Epoch 111/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0973 - acc: 0.9010 - val_loss: 0.0999 - val_acc: 0.8892\n",
      "Epoch 112/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0971 - acc: 0.9011 - val_loss: 0.1031 - val_acc: 0.8798\n",
      "Epoch 113/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0960 - acc: 0.9037 - val_loss: 0.0987 - val_acc: 0.8986\n",
      "Epoch 114/500\n",
      "22272/22272 [==============================] - 14s 621us/step - loss: 0.0959 - acc: 0.9038 - val_loss: 0.1026 - val_acc: 0.8934\n",
      "Epoch 115/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22272/22272 [==============================] - 14s 608us/step - loss: 0.0953 - acc: 0.9040 - val_loss: 0.1004 - val_acc: 0.8900\n",
      "Epoch 116/500\n",
      "22272/22272 [==============================] - 13s 601us/step - loss: 0.0948 - acc: 0.9041 - val_loss: 0.0926 - val_acc: 0.89173 -  - ETA: 8s - loss: 0. - ETA:\n",
      "Epoch 117/500\n",
      "22272/22272 [==============================] - 13s 589us/step - loss: 0.0946 - acc: 0.9049 - val_loss: 0.0975 - val_acc: 0.8986\n",
      "Epoch 118/500\n",
      "22272/22272 [==============================] - 13s 604us/step - loss: 0.0943 - acc: 0.9084 - val_loss: 0.0926 - val_acc: 0.9003\n",
      "Epoch 119/500\n",
      "22272/22272 [==============================] - 13s 604us/step - loss: 0.0937 - acc: 0.9093 - val_loss: 0.0906 - val_acc: 0.9054\n",
      "Epoch 120/500\n",
      "22272/22272 [==============================] - 13s 600us/step - loss: 0.0927 - acc: 0.9075 - val_loss: 0.0952 - val_acc: 0.8994\n",
      "Epoch 121/500\n",
      "22272/22272 [==============================] - 13s 596us/step - loss: 0.0925 - acc: 0.9081 - val_loss: 0.0920 - val_acc: 0.9011\n",
      "Epoch 122/500\n",
      "22272/22272 [==============================] - 14s 613us/step - loss: 0.0925 - acc: 0.9096 - val_loss: 0.0924 - val_acc: 0.8968\n",
      "Epoch 123/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0915 - acc: 0.9103 - val_loss: 0.0901 - val_acc: 0.8986\n",
      "Epoch 124/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0910 - acc: 0.9104 - val_loss: 0.0916 - val_acc: 0.9045\n",
      "Epoch 125/500\n",
      "22272/22272 [==============================] - 14s 622us/step - loss: 0.0914 - acc: 0.9105 - val_loss: 0.0910 - val_acc: 0.8960\n",
      "Epoch 126/500\n",
      "22272/22272 [==============================] - 14s 636us/step - loss: 0.0907 - acc: 0.9122 - val_loss: 0.0895 - val_acc: 0.9045\n",
      "Epoch 127/500\n",
      "22272/22272 [==============================] - 14s 626us/step - loss: 0.0898 - acc: 0.9128 - val_loss: 0.0872 - val_acc: 0.9096\n",
      "Epoch 128/500\n",
      "22272/22272 [==============================] - 14s 611us/step - loss: 0.0901 - acc: 0.9129 - val_loss: 0.0863 - val_acc: 0.9122\n",
      "Epoch 129/500\n",
      "22272/22272 [==============================] - 14s 618us/step - loss: 0.0901 - acc: 0.9141 - val_loss: 0.0892 - val_acc: 0.9071\n",
      "Epoch 130/500\n",
      "22272/22272 [==============================] - 14s 617us/step - loss: 0.0886 - acc: 0.9156 - val_loss: 0.0897 - val_acc: 0.9054\n",
      "Epoch 131/500\n",
      "22272/22272 [==============================] - 14s 615us/step - loss: 0.0882 - acc: 0.9126 - val_loss: 0.0887 - val_acc: 0.9071\n",
      "Epoch 132/500\n",
      "22272/22272 [==============================] - 14s 615us/step - loss: 0.0889 - acc: 0.9133 - val_loss: 0.0894 - val_acc: 0.90200890 -\n",
      "Epoch 133/500\n",
      "22272/22272 [==============================] - 13s 606us/step - loss: 0.0884 - acc: 0.9125 - val_loss: 0.0909 - val_acc: 0.9130oss: 0.0882 - acc - ETA: 0s - loss: 0.0882 - acc: \n",
      "Epoch 134/500\n",
      "22272/22272 [==============================] - 13s 589us/step - loss: 0.0882 - acc: 0.9159 - val_loss: 0.0884 - val_acc: 0.9079\n",
      "Epoch 135/500\n",
      "22272/22272 [==============================] - 14s 616us/step - loss: 0.0869 - acc: 0.9178 - val_loss: 0.0846 - val_acc: 0.9130.0872 - a - ETA: 0s - loss: 0.0871 - acc: 0.9\n",
      "Epoch 136/500\n",
      "22272/22272 [==============================] - 14s 610us/step - loss: 0.0865 - acc: 0.9199 - val_loss: 0.0859 - val_acc: 0.9147\n",
      "Epoch 137/500\n",
      "22272/22272 [==============================] - 14s 607us/step - loss: 0.0868 - acc: 0.9158 - val_loss: 0.0918 - val_acc: 0.9028\n",
      "Epoch 138/500\n",
      "22272/22272 [==============================] - 14s 607us/step - loss: 0.0862 - acc: 0.9189 - val_loss: 0.0870 - val_acc: 0.9071\n",
      "Epoch 139/500\n",
      "22272/22272 [==============================] - 14s 621us/step - loss: 0.0852 - acc: 0.9211 - val_loss: 0.0874 - val_acc: 0.9088\n",
      "Epoch 140/500\n",
      "22272/22272 [==============================] - 14s 619us/step - loss: 0.0854 - acc: 0.9183 - val_loss: 0.0835 - val_acc: 0.9113\n",
      "Epoch 141/500\n",
      "22272/22272 [==============================] - 13s 600us/step - loss: 0.0854 - acc: 0.9194 - val_loss: 0.0845 - val_acc: 0.9156  - ETA: 0s - loss: 0.0854 - acc: 0.9\n",
      "Epoch 142/500\n",
      "22272/22272 [==============================] - 14s 607us/step - loss: 0.0857 - acc: 0.9189 - val_loss: 0.0839 - val_acc: 0.9147\n",
      "Epoch 143/500\n",
      "22272/22272 [==============================] - 14s 613us/step - loss: 0.0839 - acc: 0.9213 - val_loss: 0.0829 - val_acc: 0.9105\n",
      "Epoch 144/500\n",
      "22272/22272 [==============================] - 14s 630us/step - loss: 0.0848 - acc: 0.9210 - val_loss: 0.0821 - val_acc: 0.9156\n",
      "Epoch 145/500\n",
      "22272/22272 [==============================] - 14s 616us/step - loss: 0.0835 - acc: 0.9210 - val_loss: 0.0829 - val_acc: 0.9165\n",
      "Epoch 146/500\n",
      "22272/22272 [==============================] - 14s 617us/step - loss: 0.0835 - acc: 0.9231 - val_loss: 0.0798 - val_acc: 0.9216\n",
      "Epoch 147/500\n",
      "22272/22272 [==============================] - 14s 624us/step - loss: 0.0832 - acc: 0.9247 - val_loss: 0.0817 - val_acc: 0.9190\n",
      "Epoch 148/500\n",
      "22272/22272 [==============================] - 14s 630us/step - loss: 0.0835 - acc: 0.9224 - val_loss: 0.0831 - val_acc: 0.9147\n",
      "Epoch 149/500\n",
      "22272/22272 [==============================] - 14s 625us/step - loss: 0.0833 - acc: 0.9223 - val_loss: 0.0826 - val_acc: 0.9216\n",
      "Epoch 150/500\n",
      "22272/22272 [==============================] - 14s 649us/step - loss: 0.0819 - acc: 0.9245 - val_loss: 0.0807 - val_acc: 0.9233\n",
      "Epoch 151/500\n",
      "22272/22272 [==============================] - 14s 626us/step - loss: 0.0815 - acc: 0.9254 - val_loss: 0.0814 - val_acc: 0.9147\n",
      "Epoch 152/500\n",
      "22272/22272 [==============================] - 14s 628us/step - loss: 0.0819 - acc: 0.9257 - val_loss: 0.0825 - val_acc: 0.9216\n",
      "Epoch 153/500\n",
      "22272/22272 [==============================] - 14s 637us/step - loss: 0.0810 - acc: 0.9263 - val_loss: 0.0802 - val_acc: 0.9207\n",
      "Epoch 154/500\n",
      "22272/22272 [==============================] - 14s 630us/step - loss: 0.0810 - acc: 0.9253 - val_loss: 0.0831 - val_acc: 0.9147\n",
      "Epoch 155/500\n",
      "22272/22272 [==============================] - 14s 630us/step - loss: 0.0804 - acc: 0.9270 - val_loss: 0.0806 - val_acc: 0.9190\n",
      "Epoch 156/500\n",
      "22272/22272 [==============================] - 14s 628us/step - loss: 0.0807 - acc: 0.9264 - val_loss: 0.0821 - val_acc: 0.9156\n",
      "Epoch 157/500\n",
      "22272/22272 [==============================] - 14s 629us/step - loss: 0.0811 - acc: 0.9264 - val_loss: 0.0788 - val_acc: 0.9122\n",
      "Epoch 158/500\n",
      "22272/22272 [==============================] - 14s 631us/step - loss: 0.0796 - acc: 0.9273 - val_loss: 0.0785 - val_acc: 0.9147\n",
      "Epoch 159/500\n",
      "22272/22272 [==============================] - 14s 620us/step - loss: 0.0795 - acc: 0.9282 - val_loss: 0.0806 - val_acc: 0.9173\n",
      "Epoch 160/500\n",
      "22272/22272 [==============================] - 14s 634us/step - loss: 0.0794 - acc: 0.9293 - val_loss: 0.0787 - val_acc: 0.9173\n",
      "Epoch 161/500\n",
      "22272/22272 [==============================] - 14s 630us/step - loss: 0.0789 - acc: 0.9300 - val_loss: 0.0787 - val_acc: 0.9233\n",
      "Epoch 162/500\n",
      "22272/22272 [==============================] - 14s 634us/step - loss: 0.0792 - acc: 0.9297 - val_loss: 0.0786 - val_acc: 0.9301\n",
      "Epoch 163/500\n",
      "22272/22272 [==============================] - 14s 631us/step - loss: 0.0788 - acc: 0.9286 - val_loss: 0.0795 - val_acc: 0.9182\n",
      "Epoch 164/500\n",
      "22272/22272 [==============================] - 14s 633us/step - loss: 0.0781 - acc: 0.9325 - val_loss: 0.0796 - val_acc: 0.9224\n",
      "Epoch 165/500\n",
      "22272/22272 [==============================] - 14s 632us/step - loss: 0.0782 - acc: 0.9321 - val_loss: 0.0773 - val_acc: 0.9207- acc: 0.\n",
      "Epoch 166/500\n",
      "22272/22272 [==============================] - 14s 640us/step - loss: 0.0778 - acc: 0.9303 - val_loss: 0.0778 - val_acc: 0.9250\n",
      "Epoch 167/500\n",
      "22272/22272 [==============================] - 14s 640us/step - loss: 0.0781 - acc: 0.9307 - val_loss: 0.0761 - val_acc: 0.9224\n",
      "Epoch 168/500\n",
      "22272/22272 [==============================] - 14s 637us/step - loss: 0.0770 - acc: 0.9316 - val_loss: 0.0765 - val_acc: 0.9275ETA: 6s - loss: 0.0774  - ETA: 3s - loss\n",
      "Epoch 169/500\n",
      "22272/22272 [==============================] - 14s 639us/step - loss: 0.0767 - acc: 0.9320 - val_loss: 0.0751 - val_acc: 0.9250\n",
      "Epoch 170/500\n",
      "22272/22272 [==============================] - 14s 640us/step - loss: 0.0773 - acc: 0.9310 - val_loss: 0.0767 - val_acc: 0.9233\n",
      "Epoch 171/500\n",
      "22272/22272 [==============================] - 16s 711us/step - loss: 0.0754 - acc: 0.9312 - val_loss: 0.0759 - val_acc: 0.9267\n",
      "Epoch 172/500\n",
      "22272/22272 [==============================] - 14s 638us/step - loss: 0.0762 - acc: 0.9325 - val_loss: 0.0750 - val_acc: 0.9284\n",
      "Epoch 173/500\n",
      "22272/22272 [==============================] - 13s 563us/step - loss: 0.0760 - acc: 0.9339 - val_loss: 0.0778 - val_acc: 0.9275\n",
      "Epoch 174/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0757 - acc: 0.9341 - val_loss: 0.0798 - val_acc: 0.9199\n",
      "Epoch 175/500\n",
      "22272/22272 [==============================] - 17s 759us/step - loss: 0.0755 - acc: 0.9347 - val_loss: 0.0739 - val_acc: 0.9267\n",
      "Epoch 176/500\n",
      "22272/22272 [==============================] - 16s 711us/step - loss: 0.0750 - acc: 0.9362 - val_loss: 0.0754 - val_acc: 0.9241\n",
      "Epoch 177/500\n",
      "22272/22272 [==============================] - 16s 716us/step - loss: 0.0747 - acc: 0.9359 - val_loss: 0.0735 - val_acc: 0.9233\n",
      "Epoch 178/500\n",
      "22272/22272 [==============================] - 16s 735us/step - loss: 0.0749 - acc: 0.9343 - val_loss: 0.0751 - val_acc: 0.9301\n",
      "Epoch 179/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.0746 - acc: 0.9325 - val_loss: 0.0731 - val_acc: 0.9258\n",
      "Epoch 180/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0741 - acc: 0.9351 - val_loss: 0.0725 - val_acc: 0.9318\n",
      "Epoch 181/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0739 - acc: 0.9337 - val_loss: 0.0748 - val_acc: 0.9233\n",
      "Epoch 182/500\n",
      "22272/22272 [==============================] - 16s 710us/step - loss: 0.0738 - acc: 0.9371 - val_loss: 0.0772 - val_acc: 0.9216\n",
      "Epoch 183/500\n",
      "22272/22272 [==============================] - 14s 615us/step - loss: 0.0739 - acc: 0.9363 - val_loss: 0.0706 - val_acc: 0.9344\n",
      "Epoch 184/500\n",
      "22272/22272 [==============================] - 14s 629us/step - loss: 0.0738 - acc: 0.9378 - val_loss: 0.0808 - val_acc: 0.9156\n",
      "Epoch 185/500\n",
      "22272/22272 [==============================] - 14s 627us/step - loss: 0.0734 - acc: 0.9364 - val_loss: 0.0714 - val_acc: 0.9327\n",
      "Epoch 186/500\n",
      "22272/22272 [==============================] - 14s 623us/step - loss: 0.0730 - acc: 0.9374 - val_loss: 0.0733 - val_acc: 0.9250\n",
      "Epoch 187/500\n",
      "22272/22272 [==============================] - 14s 617us/step - loss: 0.0726 - acc: 0.9379 - val_loss: 0.0759 - val_acc: 0.9318\n",
      "Epoch 188/500\n",
      "22272/22272 [==============================] - 14s 620us/step - loss: 0.0723 - acc: 0.9353 - val_loss: 0.0690 - val_acc: 0.94034 - acc: \n",
      "Epoch 189/500\n",
      "22272/22272 [==============================] - 14s 617us/step - loss: 0.0721 - acc: 0.9360 - val_loss: 0.0717 - val_acc: 0.9335\n",
      "Epoch 190/500\n",
      "22272/22272 [==============================] - 14s 619us/step - loss: 0.0721 - acc: 0.9385 - val_loss: 0.0739 - val_acc: 0.9267\n",
      "Epoch 191/500\n",
      "22272/22272 [==============================] - 14s 640us/step - loss: 0.0712 - acc: 0.9393 - val_loss: 0.0703 - val_acc: 0.9275\n",
      "Epoch 192/500\n",
      "22272/22272 [==============================] - 14s 632us/step - loss: 0.0710 - acc: 0.9391 - val_loss: 0.0728 - val_acc: 0.9301\n",
      "Epoch 193/500\n",
      "22272/22272 [==============================] - 14s 626us/step - loss: 0.0708 - acc: 0.9391 - val_loss: 0.0702 - val_acc: 0.9335\n",
      "Epoch 194/500\n",
      "22272/22272 [==============================] - 14s 614us/step - loss: 0.0710 - acc: 0.9376 - val_loss: 0.0690 - val_acc: 0.9369\n",
      "Epoch 195/500\n",
      "22272/22272 [==============================] - 14s 630us/step - loss: 0.0706 - acc: 0.9401 - val_loss: 0.0710 - val_acc: 0.9327\n",
      "Epoch 196/500\n",
      "22272/22272 [==============================] - 14s 618us/step - loss: 0.0711 - acc: 0.9401 - val_loss: 0.0671 - val_acc: 0.9352\n",
      "Epoch 197/500\n",
      "22272/22272 [==============================] - 14s 631us/step - loss: 0.0707 - acc: 0.9390 - val_loss: 0.0695 - val_acc: 0.9284\n",
      "Epoch 198/500\n",
      "22272/22272 [==============================] - 14s 628us/step - loss: 0.0704 - acc: 0.9421 - val_loss: 0.0720 - val_acc: 0.9267\n",
      "Epoch 199/500\n",
      "22272/22272 [==============================] - 14s 622us/step - loss: 0.0708 - acc: 0.9404 - val_loss: 0.0675 - val_acc: 0.9352\n",
      "Epoch 200/500\n",
      "22272/22272 [==============================] - 14s 632us/step - loss: 0.0699 - acc: 0.9405 - val_loss: 0.0706 - val_acc: 0.9275\n",
      "Epoch 201/500\n",
      "22272/22272 [==============================] - 14s 616us/step - loss: 0.0699 - acc: 0.9389 - val_loss: 0.0674 - val_acc: 0.9309\n",
      "Epoch 202/500\n",
      "22272/22272 [==============================] - 14s 622us/step - loss: 0.0695 - acc: 0.9426 - val_loss: 0.0690 - val_acc: 0.9284\n",
      "Epoch 203/500\n",
      "22272/22272 [==============================] - 14s 636us/step - loss: 0.0690 - acc: 0.9411 - val_loss: 0.0675 - val_acc: 0.9335\n",
      "Epoch 204/500\n",
      "22272/22272 [==============================] - 15s 652us/step - loss: 0.0695 - acc: 0.9410 - val_loss: 0.0684 - val_acc: 0.9284A: 5s -\n",
      "Epoch 205/500\n",
      "22272/22272 [==============================] - 14s 624us/step - loss: 0.0693 - acc: 0.9410 - val_loss: 0.0688 - val_acc: 0.9352\n",
      "Epoch 206/500\n",
      "22272/22272 [==============================] - 14s 643us/step - loss: 0.0691 - acc: 0.9412 - val_loss: 0.0699 - val_acc: 0.9301s:\n",
      "Epoch 207/500\n",
      "22272/22272 [==============================] - 14s 630us/step - loss: 0.0687 - acc: 0.9434 - val_loss: 0.0660 - val_acc: 0.9309\n",
      "Epoch 208/500\n",
      "22272/22272 [==============================] - 14s 643us/step - loss: 0.0681 - acc: 0.9437 - val_loss: 0.0665 - val_acc: 0.9361\n",
      "Epoch 209/500\n",
      "22272/22272 [==============================] - 15s 652us/step - loss: 0.0683 - acc: 0.9411 - val_loss: 0.0671 - val_acc: 0.9386\n",
      "Epoch 210/500\n",
      "22272/22272 [==============================] - 17s 744us/step - loss: 0.0681 - acc: 0.9436 - val_loss: 0.0674 - val_acc: 0.9386\n",
      "Epoch 211/500\n",
      "22272/22272 [==============================] - 16s 727us/step - loss: 0.0679 - acc: 0.9418 - val_loss: 0.0669 - val_acc: 0.9386\n",
      "Epoch 212/500\n",
      "22272/22272 [==============================] - 16s 741us/step - loss: 0.0678 - acc: 0.9442 - val_loss: 0.0729 - val_acc: 0.9301\n",
      "Epoch 213/500\n",
      "22272/22272 [==============================] - 16s 741us/step - loss: 0.0678 - acc: 0.9427 - val_loss: 0.0688 - val_acc: 0.9335\n",
      "Epoch 214/500\n",
      "22272/22272 [==============================] - 17s 746us/step - loss: 0.0685 - acc: 0.9442 - val_loss: 0.0670 - val_acc: 0.9275\n",
      "Epoch 215/500\n",
      "22272/22272 [==============================] - 15s 677us/step - loss: 0.0677 - acc: 0.9424 - val_loss: 0.0658 - val_acc: 0.9369\n",
      "Epoch 216/500\n",
      "22272/22272 [==============================] - 14s 643us/step - loss: 0.0672 - acc: 0.9455 - val_loss: 0.0685 - val_acc: 0.9335\n",
      "Epoch 217/500\n",
      "22272/22272 [==============================] - 14s 641us/step - loss: 0.0667 - acc: 0.9454 - val_loss: 0.0657 - val_acc: 0.9344\n",
      "Epoch 218/500\n",
      "22272/22272 [==============================] - 15s 656us/step - loss: 0.0667 - acc: 0.9445 - val_loss: 0.0668 - val_acc: 0.9403\n",
      "Epoch 219/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0663 - acc: 0.9467 - val_loss: 0.0705 - val_acc: 0.9292\n",
      "Epoch 220/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0667 - acc: 0.9454 - val_loss: 0.0659 - val_acc: 0.9327\n",
      "Epoch 221/500\n",
      "22272/22272 [==============================] - 15s 668us/step - loss: 0.0660 - acc: 0.9445 - val_loss: 0.0646 - val_acc: 0.9403\n",
      "Epoch 222/500\n",
      "22272/22272 [==============================] - 14s 614us/step - loss: 0.0660 - acc: 0.9451 - val_loss: 0.0643 - val_acc: 0.9369- loss: 0.0659 - acc: 0.945\n",
      "Epoch 223/500\n",
      "22272/22272 [==============================] - 13s 593us/step - loss: 0.0660 - acc: 0.9472 - val_loss: 0.0644 - val_acc: 0.9301\n",
      "Epoch 224/500\n",
      "22272/22272 [==============================] - 12s 536us/step - loss: 0.0661 - acc: 0.9445 - val_loss: 0.0651 - val_acc: 0.9335\n",
      "Epoch 225/500\n",
      "22272/22272 [==============================] - 12s 524us/step - loss: 0.0659 - acc: 0.9437 - val_loss: 0.0673 - val_acc: 0.9335\n",
      "Epoch 226/500\n",
      "22272/22272 [==============================] - 12s 520us/step - loss: 0.0650 - acc: 0.9465 - val_loss: 0.0677 - val_acc: 0.9318\n",
      "Epoch 227/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.0653 - acc: 0.9481 - val_loss: 0.0623 - val_acc: 0.9352\n",
      "Epoch 228/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0655 - acc: 0.9473 - val_loss: 0.0654 - val_acc: 0.9344\n",
      "Epoch 229/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0651 - acc: 0.9462 - val_loss: 0.0644 - val_acc: 0.9327\n",
      "Epoch 230/500\n",
      "22272/22272 [==============================] - 12s 543us/step - loss: 0.0652 - acc: 0.9441 - val_loss: 0.0624 - val_acc: 0.9361\n",
      "Epoch 231/500\n",
      "22272/22272 [==============================] - 13s 570us/step - loss: 0.0645 - acc: 0.9472 - val_loss: 0.0630 - val_acc: 0.9454\n",
      "Epoch 232/500\n",
      "22272/22272 [==============================] - 13s 576us/step - loss: 0.0642 - acc: 0.9485 - val_loss: 0.0663 - val_acc: 0.9309\n",
      "Epoch 233/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0651 - acc: 0.9468 - val_loss: 0.0625 - val_acc: 0.9335\n",
      "Epoch 234/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0637 - acc: 0.9472 - val_loss: 0.0637 - val_acc: 0.9361\n",
      "Epoch 235/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0642 - acc: 0.9505 - val_loss: 0.0651 - val_acc: 0.9344\n",
      "Epoch 236/500\n",
      "22272/22272 [==============================] - 12s 525us/step - loss: 0.0637 - acc: 0.9482 - val_loss: 0.0627 - val_acc: 0.9395\n",
      "Epoch 237/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0642 - acc: 0.9475 - val_loss: 0.0618 - val_acc: 0.9420\n",
      "Epoch 238/500\n",
      "22272/22272 [==============================] - 14s 615us/step - loss: 0.0634 - acc: 0.9489 - val_loss: 0.0646 - val_acc: 0.9378\n",
      "Epoch 239/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0634 - acc: 0.9494 - val_loss: 0.0611 - val_acc: 0.9420\n",
      "Epoch 240/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0629 - acc: 0.9503 - val_loss: 0.0629 - val_acc: 0.9386\n",
      "Epoch 241/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0627 - acc: 0.9516 - val_loss: 0.0617 - val_acc: 0.9369\n",
      "Epoch 242/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0628 - acc: 0.9495 - val_loss: 0.0616 - val_acc: 0.9395\n",
      "Epoch 243/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0626 - acc: 0.9489 - val_loss: 0.0621 - val_acc: 0.9378\n",
      "Epoch 244/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0640 - acc: 0.9471 - val_loss: 0.0659 - val_acc: 0.9352\n",
      "Epoch 245/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0628 - acc: 0.9481 - val_loss: 0.0620 - val_acc: 0.9412\n",
      "Epoch 246/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0629 - acc: 0.9492 - val_loss: 0.0600 - val_acc: 0.9378\n",
      "Epoch 247/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0625 - acc: 0.9482 - val_loss: 0.0619 - val_acc: 0.9378\n",
      "Epoch 248/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0624 - acc: 0.9499 - val_loss: 0.0624 - val_acc: 0.9361\n",
      "Epoch 249/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0624 - acc: 0.9509 - val_loss: 0.0615 - val_acc: 0.9395\n",
      "Epoch 250/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0620 - acc: 0.9522 - val_loss: 0.0614 - val_acc: 0.9395\n",
      "Epoch 251/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0623 - acc: 0.9498 - val_loss: 0.0605 - val_acc: 0.9437\n",
      "Epoch 252/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0618 - acc: 0.9491 - val_loss: 0.0595 - val_acc: 0.9412\n",
      "Epoch 253/500\n",
      "22272/22272 [==============================] - 12s 523us/step - loss: 0.0613 - acc: 0.9500 - val_loss: 0.0594 - val_acc: 0.9437\n",
      "Epoch 254/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0612 - acc: 0.9538 - val_loss: 0.0611 - val_acc: 0.9403\n",
      "Epoch 255/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0612 - acc: 0.9516 - val_loss: 0.0618 - val_acc: 0.9386\n",
      "Epoch 256/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0611 - acc: 0.9529 - val_loss: 0.0634 - val_acc: 0.9378\n",
      "Epoch 257/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0611 - acc: 0.9509 - val_loss: 0.0624 - val_acc: 0.9429\n",
      "Epoch 258/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0609 - acc: 0.9484 - val_loss: 0.0583 - val_acc: 0.9420\n",
      "Epoch 259/500\n",
      "22272/22272 [==============================] - 12s 520us/step - loss: 0.0610 - acc: 0.9498 - val_loss: 0.0595 - val_acc: 0.9412\n",
      "Epoch 260/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0604 - acc: 0.9529 - val_loss: 0.0601 - val_acc: 0.9403\n",
      "Epoch 261/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0607 - acc: 0.9530 - val_loss: 0.0622 - val_acc: 0.9378\n",
      "Epoch 262/500\n",
      "22272/22272 [==============================] - 12s 525us/step - loss: 0.0607 - acc: 0.9508 - val_loss: 0.0610 - val_acc: 0.9403\n",
      "Epoch 263/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0604 - acc: 0.9516 - val_loss: 0.0597 - val_acc: 0.9463\n",
      "Epoch 264/500\n",
      "22272/22272 [==============================] - 12s 534us/step - loss: 0.0605 - acc: 0.9542 - val_loss: 0.0585 - val_acc: 0.9446\n",
      "Epoch 265/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0597 - acc: 0.9525 - val_loss: 0.0601 - val_acc: 0.9437\n",
      "Epoch 266/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.0602 - acc: 0.9516 - val_loss: 0.0588 - val_acc: 0.9420\n",
      "Epoch 267/500\n",
      "22272/22272 [==============================] - 12s 528us/step - loss: 0.0599 - acc: 0.9540 - val_loss: 0.0600 - val_acc: 0.9412\n",
      "Epoch 268/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0601 - acc: 0.9524 - val_loss: 0.0575 - val_acc: 0.9488\n",
      "Epoch 269/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.0599 - acc: 0.9509 - val_loss: 0.0601 - val_acc: 0.9403\n",
      "Epoch 270/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0597 - acc: 0.9533 - val_loss: 0.0583 - val_acc: 0.9429\n",
      "Epoch 271/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0598 - acc: 0.9528 - val_loss: 0.0611 - val_acc: 0.9378\n",
      "Epoch 272/500\n",
      "22272/22272 [==============================] - 12s 520us/step - loss: 0.0595 - acc: 0.9538 - val_loss: 0.0593 - val_acc: 0.9420\n",
      "Epoch 273/500\n",
      "22272/22272 [==============================] - 12s 524us/step - loss: 0.0587 - acc: 0.9547 - val_loss: 0.0597 - val_acc: 0.9420\n",
      "Epoch 274/500\n",
      "22272/22272 [==============================] - 12s 521us/step - loss: 0.0592 - acc: 0.9531 - val_loss: 0.0565 - val_acc: 0.9514\n",
      "Epoch 275/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0591 - acc: 0.9533 - val_loss: 0.0595 - val_acc: 0.9446\n",
      "Epoch 276/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0589 - acc: 0.9529 - val_loss: 0.0568 - val_acc: 0.9471\n",
      "Epoch 277/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0585 - acc: 0.9544 - val_loss: 0.0570 - val_acc: 0.9480\n",
      "Epoch 278/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0585 - acc: 0.9540 - val_loss: 0.0596 - val_acc: 0.9412\n",
      "Epoch 279/500\n",
      "22272/22272 [==============================] - 14s 615us/step - loss: 0.0591 - acc: 0.9549 - val_loss: 0.0572 - val_acc: 0.9471\n",
      "Epoch 280/500\n",
      "22272/22272 [==============================] - 16s 701us/step - loss: 0.0583 - acc: 0.9542 - val_loss: 0.0569 - val_acc: 0.9437\n",
      "Epoch 281/500\n",
      "22272/22272 [==============================] - 14s 646us/step - loss: 0.0576 - acc: 0.9554 - val_loss: 0.0566 - val_acc: 0.9429\n",
      "Epoch 282/500\n",
      "22272/22272 [==============================] - 16s 720us/step - loss: 0.0582 - acc: 0.9567 - val_loss: 0.0596 - val_acc: 0.9463\n",
      "Epoch 283/500\n",
      "22272/22272 [==============================] - 17s 741us/step - loss: 0.0585 - acc: 0.9549 - val_loss: 0.0592 - val_acc: 0.9420\n",
      "Epoch 284/500\n",
      "22272/22272 [==============================] - 15s 673us/step - loss: 0.0579 - acc: 0.9569 - val_loss: 0.0580 - val_acc: 0.9429\n",
      "Epoch 285/500\n",
      "22272/22272 [==============================] - 15s 678us/step - loss: 0.0581 - acc: 0.9552 - val_loss: 0.0585 - val_acc: 0.9437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/500\n",
      "22272/22272 [==============================] - 15s 652us/step - loss: 0.0577 - acc: 0.9563 - val_loss: 0.0556 - val_acc: 0.9497\n",
      "Epoch 287/500\n",
      "22272/22272 [==============================] - 15s 658us/step - loss: 0.0580 - acc: 0.9554 - val_loss: 0.0575 - val_acc: 0.9471\n",
      "Epoch 288/500\n",
      "22272/22272 [==============================] - 15s 668us/step - loss: 0.0578 - acc: 0.9548 - val_loss: 0.0569 - val_acc: 0.9506\n",
      "Epoch 289/500\n",
      "22272/22272 [==============================] - 15s 661us/step - loss: 0.0578 - acc: 0.9535 - val_loss: 0.0563 - val_acc: 0.9437\n",
      "Epoch 290/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0574 - acc: 0.9543 - val_loss: 0.0550 - val_acc: 0.9497\n",
      "Epoch 291/500\n",
      "22272/22272 [==============================] - 15s 671us/step - loss: 0.0572 - acc: 0.9570 - val_loss: 0.0570 - val_acc: 0.9480\n",
      "Epoch 292/500\n",
      "22272/22272 [==============================] - 15s 666us/step - loss: 0.0575 - acc: 0.9573 - val_loss: 0.0575 - val_acc: 0.9437\n",
      "Epoch 293/500\n",
      "22272/22272 [==============================] - 15s 664us/step - loss: 0.0572 - acc: 0.9570 - val_loss: 0.0567 - val_acc: 0.9497\n",
      "Epoch 294/500\n",
      "22272/22272 [==============================] - 15s 673us/step - loss: 0.0565 - acc: 0.9566 - val_loss: 0.0568 - val_acc: 0.9429\n",
      "Epoch 295/500\n",
      "22272/22272 [==============================] - 15s 663us/step - loss: 0.0569 - acc: 0.9565 - val_loss: 0.0568 - val_acc: 0.9514\n",
      "Epoch 296/500\n",
      "22272/22272 [==============================] - 15s 671us/step - loss: 0.0568 - acc: 0.9540 - val_loss: 0.0555 - val_acc: 0.9446\n",
      "Epoch 297/500\n",
      "22272/22272 [==============================] - 15s 652us/step - loss: 0.0573 - acc: 0.9558 - val_loss: 0.0560 - val_acc: 0.9497\n",
      "Epoch 298/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0566 - acc: 0.9568 - val_loss: 0.0558 - val_acc: 0.9488\n",
      "Epoch 299/500\n",
      "22272/22272 [==============================] - 15s 677us/step - loss: 0.0563 - acc: 0.9573 - val_loss: 0.0554 - val_acc: 0.9506\n",
      "Epoch 300/500\n",
      "22272/22272 [==============================] - 15s 658us/step - loss: 0.0566 - acc: 0.9582 - val_loss: 0.0566 - val_acc: 0.9471\n",
      "Epoch 301/500\n",
      "22272/22272 [==============================] - 15s 657us/step - loss: 0.0564 - acc: 0.9560 - val_loss: 0.0569 - val_acc: 0.9471\n",
      "Epoch 302/500\n",
      "22272/22272 [==============================] - 15s 659us/step - loss: 0.0566 - acc: 0.9560 - val_loss: 0.0562 - val_acc: 0.9523\n",
      "Epoch 303/500\n",
      "22272/22272 [==============================] - 15s 661us/step - loss: 0.0558 - acc: 0.9595 - val_loss: 0.0579 - val_acc: 0.9403\n",
      "Epoch 304/500\n",
      "22272/22272 [==============================] - 15s 670us/step - loss: 0.0565 - acc: 0.9555 - val_loss: 0.0571 - val_acc: 0.9429\n",
      "Epoch 305/500\n",
      "22272/22272 [==============================] - 15s 673us/step - loss: 0.0565 - acc: 0.9569 - val_loss: 0.0555 - val_acc: 0.9497\n",
      "Epoch 306/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0560 - acc: 0.9565 - val_loss: 0.0551 - val_acc: 0.9454\n",
      "Epoch 307/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0562 - acc: 0.9583 - val_loss: 0.0563 - val_acc: 0.9480\n",
      "Epoch 308/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0562 - acc: 0.9568 - val_loss: 0.0568 - val_acc: 0.9506\n",
      "Epoch 309/500\n",
      "22272/22272 [==============================] - 15s 662us/step - loss: 0.0553 - acc: 0.9576 - val_loss: 0.0544 - val_acc: 0.9514\n",
      "Epoch 310/500\n",
      "22272/22272 [==============================] - 15s 671us/step - loss: 0.0559 - acc: 0.9592 - val_loss: 0.0543 - val_acc: 0.9506\n",
      "Epoch 311/500\n",
      "22272/22272 [==============================] - 15s 665us/step - loss: 0.0560 - acc: 0.9582 - val_loss: 0.0551 - val_acc: 0.9454\n",
      "Epoch 312/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0550 - acc: 0.9581 - val_loss: 0.0537 - val_acc: 0.9531\n",
      "Epoch 313/500\n",
      "22272/22272 [==============================] - 15s 659us/step - loss: 0.0552 - acc: 0.9587 - val_loss: 0.0554 - val_acc: 0.9480\n",
      "Epoch 314/500\n",
      "22272/22272 [==============================] - 15s 675us/step - loss: 0.0546 - acc: 0.9606 - val_loss: 0.0540 - val_acc: 0.9523\n",
      "Epoch 315/500\n",
      "22272/22272 [==============================] - 15s 676us/step - loss: 0.0553 - acc: 0.9573 - val_loss: 0.0532 - val_acc: 0.9514\n",
      "Epoch 316/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0549 - acc: 0.9573 - val_loss: 0.0563 - val_acc: 0.9480\n",
      "Epoch 317/500\n",
      "22272/22272 [==============================] - 15s 679us/step - loss: 0.0555 - acc: 0.9569 - val_loss: 0.0558 - val_acc: 0.9463\n",
      "Epoch 318/500\n",
      "22272/22272 [==============================] - 16s 730us/step - loss: 0.0548 - acc: 0.9576 - val_loss: 0.0544 - val_acc: 0.9463\n",
      "Epoch 319/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.0549 - acc: 0.9570 - val_loss: 0.0551 - val_acc: 0.9506\n",
      "Epoch 320/500\n",
      "22272/22272 [==============================] - 15s 660us/step - loss: 0.0552 - acc: 0.9591 - val_loss: 0.0542 - val_acc: 0.9463\n",
      "Epoch 321/500\n",
      "22272/22272 [==============================] - 15s 659us/step - loss: 0.0543 - acc: 0.9598 - val_loss: 0.0535 - val_acc: 0.9471\n",
      "Epoch 322/500\n",
      "22272/22272 [==============================] - 17s 765us/step - loss: 0.0542 - acc: 0.9594 - val_loss: 0.0535 - val_acc: 0.9471\n",
      "Epoch 323/500\n",
      "22272/22272 [==============================] - 17s 755us/step - loss: 0.0545 - acc: 0.9584 - val_loss: 0.0550 - val_acc: 0.9471\n",
      "Epoch 324/500\n",
      "22272/22272 [==============================] - 16s 724us/step - loss: 0.0551 - acc: 0.9569 - val_loss: 0.0559 - val_acc: 0.9454\n",
      "Epoch 325/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0545 - acc: 0.9599 - val_loss: 0.0538 - val_acc: 0.9531\n",
      "Epoch 326/500\n",
      "22272/22272 [==============================] - 15s 668us/step - loss: 0.0547 - acc: 0.9593 - val_loss: 0.0546 - val_acc: 0.9531\n",
      "Epoch 327/500\n",
      "22272/22272 [==============================] - 15s 686us/step - loss: 0.0542 - acc: 0.9599 - val_loss: 0.0541 - val_acc: 0.9540\n",
      "Epoch 328/500\n",
      "22272/22272 [==============================] - 15s 662us/step - loss: 0.0540 - acc: 0.9589 - val_loss: 0.0529 - val_acc: 0.9540\n",
      "Epoch 329/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0537 - acc: 0.9594 - val_loss: 0.0518 - val_acc: 0.9548\n",
      "Epoch 330/500\n",
      "22272/22272 [==============================] - 16s 736us/step - loss: 0.0544 - acc: 0.9590 - val_loss: 0.0529 - val_acc: 0.9497\n",
      "Epoch 331/500\n",
      "22272/22272 [==============================] - 15s 676us/step - loss: 0.0539 - acc: 0.9594 - val_loss: 0.0532 - val_acc: 0.9463\n",
      "Epoch 332/500\n",
      "22272/22272 [==============================] - 14s 642us/step - loss: 0.0536 - acc: 0.9612 - val_loss: 0.0533 - val_acc: 0.9446\n",
      "Epoch 333/500\n",
      "22272/22272 [==============================] - 14s 625us/step - loss: 0.0539 - acc: 0.9586 - val_loss: 0.0523 - val_acc: 0.9480\n",
      "Epoch 334/500\n",
      "22272/22272 [==============================] - 14s 633us/step - loss: 0.0541 - acc: 0.9597 - val_loss: 0.0524 - val_acc: 0.9557\n",
      "Epoch 335/500\n",
      "22272/22272 [==============================] - 14s 644us/step - loss: 0.0537 - acc: 0.9576 - val_loss: 0.0524 - val_acc: 0.9497\n",
      "Epoch 336/500\n",
      "22272/22272 [==============================] - 14s 647us/step - loss: 0.0534 - acc: 0.9585 - val_loss: 0.0553 - val_acc: 0.9488\n",
      "Epoch 337/500\n",
      "22272/22272 [==============================] - 14s 642us/step - loss: 0.0536 - acc: 0.9582 - val_loss: 0.0517 - val_acc: 0.9480\n",
      "Epoch 338/500\n",
      "22272/22272 [==============================] - 14s 640us/step - loss: 0.0532 - acc: 0.9584 - val_loss: 0.0517 - val_acc: 0.9531\n",
      "Epoch 339/500\n",
      "22272/22272 [==============================] - 15s 656us/step - loss: 0.0530 - acc: 0.9613 - val_loss: 0.0521 - val_acc: 0.9514\n",
      "Epoch 340/500\n",
      "22272/22272 [==============================] - 14s 638us/step - loss: 0.0529 - acc: 0.9626 - val_loss: 0.0514 - val_acc: 0.9523 0.962\n",
      "Epoch 341/500\n",
      "22272/22272 [==============================] - 14s 640us/step - loss: 0.0533 - acc: 0.9578 - val_loss: 0.0515 - val_acc: 0.9514\n",
      "Epoch 342/500\n",
      "22272/22272 [==============================] - 14s 649us/step - loss: 0.0532 - acc: 0.9602 - val_loss: 0.0529 - val_acc: 0.9497\n",
      "Epoch 343/500\n",
      "22272/22272 [==============================] - 14s 614us/step - loss: 0.0532 - acc: 0.9612 - val_loss: 0.0541 - val_acc: 0.9497\n",
      "Epoch 344/500\n",
      "22272/22272 [==============================] - 14s 626us/step - loss: 0.0524 - acc: 0.9591 - val_loss: 0.0523 - val_acc: 0.9548\n",
      "Epoch 345/500\n",
      "22272/22272 [==============================] - 14s 615us/step - loss: 0.0529 - acc: 0.9610 - val_loss: 0.0537 - val_acc: 0.9463c\n",
      "Epoch 346/500\n",
      "22272/22272 [==============================] - 13s 603us/step - loss: 0.0530 - acc: 0.9602 - val_loss: 0.0522 - val_acc: 0.9471\n",
      "Epoch 347/500\n",
      "22272/22272 [==============================] - 14s 632us/step - loss: 0.0531 - acc: 0.9604 - val_loss: 0.0514 - val_acc: 0.9471\n",
      "Epoch 348/500\n",
      "22272/22272 [==============================] - 14s 636us/step - loss: 0.0530 - acc: 0.9595 - val_loss: 0.0518 - val_acc: 0.9514\n",
      "Epoch 349/500\n",
      "22272/22272 [==============================] - 14s 621us/step - loss: 0.0525 - acc: 0.9610 - val_loss: 0.0525 - val_acc: 0.9557\n",
      "Epoch 350/500\n",
      "22272/22272 [==============================] - 13s 602us/step - loss: 0.0519 - acc: 0.9621 - val_loss: 0.0507 - val_acc: 0.9574\n",
      "Epoch 351/500\n",
      "22272/22272 [==============================] - 14s 611us/step - loss: 0.0524 - acc: 0.9626 - val_loss: 0.0518 - val_acc: 0.9488\n",
      "Epoch 352/500\n",
      "22272/22272 [==============================] - 14s 607us/step - loss: 0.0521 - acc: 0.9605 - val_loss: 0.0525 - val_acc: 0.9523\n",
      "Epoch 353/500\n",
      "22272/22272 [==============================] - 14s 620us/step - loss: 0.0523 - acc: 0.9614 - val_loss: 0.0511 - val_acc: 0.9497\n",
      "Epoch 354/500\n",
      "22272/22272 [==============================] - 14s 612us/step - loss: 0.0524 - acc: 0.9617 - val_loss: 0.0520 - val_acc: 0.9471\n",
      "Epoch 355/500\n",
      "22272/22272 [==============================] - 14s 625us/step - loss: 0.0526 - acc: 0.9606 - val_loss: 0.0501 - val_acc: 0.9540\n",
      "Epoch 356/500\n",
      "22272/22272 [==============================] - 14s 615us/step - loss: 0.0520 - acc: 0.9622 - val_loss: 0.0502 - val_acc: 0.9506\n",
      "Epoch 357/500\n",
      "22272/22272 [==============================] - 14s 620us/step - loss: 0.0517 - acc: 0.9614 - val_loss: 0.0503 - val_acc: 0.9506\n",
      "Epoch 358/500\n",
      "22272/22272 [==============================] - 14s 630us/step - loss: 0.0523 - acc: 0.9606 - val_loss: 0.0498 - val_acc: 0.9557\n",
      "Epoch 359/500\n",
      "22272/22272 [==============================] - 14s 624us/step - loss: 0.0517 - acc: 0.9594 - val_loss: 0.0506 - val_acc: 0.9506\n",
      "Epoch 360/500\n",
      "22272/22272 [==============================] - 14s 628us/step - loss: 0.0518 - acc: 0.9616 - val_loss: 0.0510 - val_acc: 0.9523\n",
      "Epoch 361/500\n",
      "22272/22272 [==============================] - 14s 626us/step - loss: 0.0517 - acc: 0.9601 - val_loss: 0.0508 - val_acc: 0.9506\n",
      "Epoch 362/500\n",
      "22272/22272 [==============================] - 14s 609us/step - loss: 0.0513 - acc: 0.9619 - val_loss: 0.0528 - val_acc: 0.9480\n",
      "Epoch 363/500\n",
      "22272/22272 [==============================] - 14s 620us/step - loss: 0.0517 - acc: 0.9599 - val_loss: 0.0499 - val_acc: 0.9514\n",
      "Epoch 364/500\n",
      "22272/22272 [==============================] - 14s 607us/step - loss: 0.0514 - acc: 0.9602 - val_loss: 0.0509 - val_acc: 0.9497\n",
      "Epoch 365/500\n",
      "22272/22272 [==============================] - 14s 616us/step - loss: 0.0518 - acc: 0.9604 - val_loss: 0.0514 - val_acc: 0.94540.0519  - ETA: 0s - loss: 0.0518 - acc: 0.9\n",
      "Epoch 366/500\n",
      "22272/22272 [==============================] - 14s 622us/step - loss: 0.0514 - acc: 0.9613 - val_loss: 0.0522 - val_acc: 0.9446\n",
      "Epoch 367/500\n",
      "22272/22272 [==============================] - 14s 632us/step - loss: 0.0514 - acc: 0.9629 - val_loss: 0.0483 - val_acc: 0.9531\n",
      "Epoch 368/500\n",
      "22272/22272 [==============================] - 14s 634us/step - loss: 0.0515 - acc: 0.9598 - val_loss: 0.0490 - val_acc: 0.9582\n",
      "Epoch 369/500\n",
      "22272/22272 [==============================] - 14s 611us/step - loss: 0.0513 - acc: 0.9633 - val_loss: 0.0505 - val_acc: 0.9506\n",
      "Epoch 370/500\n",
      "22272/22272 [==============================] - 14s 613us/step - loss: 0.0513 - acc: 0.9598 - val_loss: 0.0507 - val_acc: 0.9480\n",
      "Epoch 371/500\n",
      "22272/22272 [==============================] - 13s 595us/step - loss: 0.0511 - acc: 0.9626 - val_loss: 0.0490 - val_acc: 0.9608\n",
      "Epoch 372/500\n",
      "22272/22272 [==============================] - 13s 597us/step - loss: 0.0507 - acc: 0.9627 - val_loss: 0.0504 - val_acc: 0.9531\n",
      "Epoch 373/500\n",
      "22272/22272 [==============================] - 14s 609us/step - loss: 0.0509 - acc: 0.9623 - val_loss: 0.0501 - val_acc: 0.9565ETA: 1s - loss: 0.0509 - ac\n",
      "Epoch 374/500\n",
      "22272/22272 [==============================] - 14s 631us/step - loss: 0.0511 - acc: 0.9613 - val_loss: 0.0491 - val_acc: 0.9565\n",
      "Epoch 375/500\n",
      "22272/22272 [==============================] - 14s 620us/step - loss: 0.0506 - acc: 0.9639 - val_loss: 0.0503 - val_acc: 0.9488\n",
      "Epoch 376/500\n",
      "22272/22272 [==============================] - 14s 609us/step - loss: 0.0507 - acc: 0.9623 - val_loss: 0.0494 - val_acc: 0.9557\n",
      "Epoch 377/500\n",
      "22272/22272 [==============================] - 14s 612us/step - loss: 0.0510 - acc: 0.9610 - val_loss: 0.0506 - val_acc: 0.9531\n",
      "Epoch 378/500\n",
      "22272/22272 [==============================] - 14s 642us/step - loss: 0.0507 - acc: 0.9633 - val_loss: 0.0489 - val_acc: 0.9540\n",
      "Epoch 379/500\n",
      "22272/22272 [==============================] - 14s 632us/step - loss: 0.0507 - acc: 0.9621 - val_loss: 0.0488 - val_acc: 0.9591\n",
      "Epoch 380/500\n",
      "22272/22272 [==============================] - 14s 631us/step - loss: 0.0503 - acc: 0.9630 - val_loss: 0.0490 - val_acc: 0.9608 loss: 0.0503 - acc\n",
      "Epoch 381/500\n",
      "22272/22272 [==============================] - 14s 624us/step - loss: 0.0504 - acc: 0.9643 - val_loss: 0.0509 - val_acc: 0.9565\n",
      "Epoch 382/500\n",
      "22272/22272 [==============================] - 14s 614us/step - loss: 0.0503 - acc: 0.9629 - val_loss: 0.0495 - val_acc: 0.9565\n",
      "Epoch 383/500\n",
      "22272/22272 [==============================] - 14s 635us/step - loss: 0.0501 - acc: 0.9613 - val_loss: 0.0499 - val_acc: 0.9565\n",
      "Epoch 384/500\n",
      "22272/22272 [==============================] - 14s 645us/step - loss: 0.0503 - acc: 0.9633 - val_loss: 0.0498 - val_acc: 0.9523\n",
      "Epoch 385/500\n",
      "22272/22272 [==============================] - 14s 639us/step - loss: 0.0505 - acc: 0.9623 - val_loss: 0.0503 - val_acc: 0.9548\n",
      "Epoch 386/500\n",
      "22272/22272 [==============================] - 14s 632us/step - loss: 0.0503 - acc: 0.9625 - val_loss: 0.0495 - val_acc: 0.9548\n",
      "Epoch 387/500\n",
      "22272/22272 [==============================] - 14s 636us/step - loss: 0.0502 - acc: 0.9624 - val_loss: 0.0490 - val_acc: 0.9625\n",
      "Epoch 388/500\n",
      "22272/22272 [==============================] - 14s 628us/step - loss: 0.0504 - acc: 0.9633 - val_loss: 0.0482 - val_acc: 0.9557\n",
      "Epoch 389/500\n",
      "22272/22272 [==============================] - 14s 621us/step - loss: 0.0499 - acc: 0.9645 - val_loss: 0.0492 - val_acc: 0.9506\n",
      "Epoch 390/500\n",
      "22272/22272 [==============================] - 14s 632us/step - loss: 0.0501 - acc: 0.9621 - val_loss: 0.0493 - val_acc: 0.9557\n",
      "Epoch 391/500\n",
      "22272/22272 [==============================] - 14s 639us/step - loss: 0.0502 - acc: 0.9621 - val_loss: 0.0496 - val_acc: 0.9565\n",
      "Epoch 392/500\n",
      "22272/22272 [==============================] - 15s 653us/step - loss: 0.0501 - acc: 0.9627 - val_loss: 0.0502 - val_acc: 0.9540\n",
      "Epoch 393/500\n",
      "22272/22272 [==============================] - 15s 660us/step - loss: 0.0497 - acc: 0.9630 - val_loss: 0.0493 - val_acc: 0.9582\n",
      "Epoch 394/500\n",
      "22272/22272 [==============================] - 14s 640us/step - loss: 0.0501 - acc: 0.9621 - val_loss: 0.0485 - val_acc: 0.9616\n",
      "Epoch 395/500\n",
      "22272/22272 [==============================] - 14s 629us/step - loss: 0.0498 - acc: 0.9637 - val_loss: 0.0498 - val_acc: 0.95572s - loss: 0.0498\n",
      "Epoch 396/500\n",
      "22272/22272 [==============================] - 14s 643us/step - loss: 0.0501 - acc: 0.9633 - val_loss: 0.0493 - val_acc: 0.9582\n",
      "Epoch 397/500\n",
      "22272/22272 [==============================] - 14s 637us/step - loss: 0.0501 - acc: 0.9625 - val_loss: 0.0492 - val_acc: 0.9608\n",
      "Epoch 398/500\n",
      "22272/22272 [==============================] - 14s 632us/step - loss: 0.0494 - acc: 0.9636 - val_loss: 0.0472 - val_acc: 0.9574\n",
      "Epoch 399/500\n",
      "22272/22272 [==============================] - 14s 636us/step - loss: 0.0494 - acc: 0.9630 - val_loss: 0.0503 - val_acc: 0.9540\n",
      "Epoch 400/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22272/22272 [==============================] - 14s 633us/step - loss: 0.0496 - acc: 0.9654 - val_loss: 0.0493 - val_acc: 0.9582\n",
      "Epoch 401/500\n",
      "22272/22272 [==============================] - 13s 605us/step - loss: 0.0497 - acc: 0.9624 - val_loss: 0.0489 - val_acc: 0.95910s - loss: 0.0496 - acc: 0.\n",
      "Epoch 402/500\n",
      "22272/22272 [==============================] - 14s 609us/step - loss: 0.0494 - acc: 0.9621 - val_loss: 0.0485 - val_acc: 0.9591\n",
      "Epoch 403/500\n",
      "22272/22272 [==============================] - 14s 613us/step - loss: 0.0494 - acc: 0.9646 - val_loss: 0.0472 - val_acc: 0.9565: 0.0493 - a\n",
      "Epoch 404/500\n",
      "22272/22272 [==============================] - 14s 619us/step - loss: 0.0492 - acc: 0.9645 - val_loss: 0.0470 - val_acc: 0.9557\n",
      "Epoch 405/500\n",
      "22272/22272 [==============================] - 14s 608us/step - loss: 0.0490 - acc: 0.9653 - val_loss: 0.0480 - val_acc: 0.9608\n",
      "Epoch 406/500\n",
      "22272/22272 [==============================] - 14s 615us/step - loss: 0.0489 - acc: 0.9635 - val_loss: 0.0473 - val_acc: 0.9557\n",
      "Epoch 407/500\n",
      "22272/22272 [==============================] - 14s 622us/step - loss: 0.0494 - acc: 0.9628 - val_loss: 0.0480 - val_acc: 0.9548\n",
      "Epoch 408/500\n",
      "22272/22272 [==============================] - 13s 606us/step - loss: 0.0490 - acc: 0.9639 - val_loss: 0.0470 - val_acc: 0.9574\n",
      "Epoch 409/500\n",
      "22272/22272 [==============================] - 14s 635us/step - loss: 0.0488 - acc: 0.9644 - val_loss: 0.0470 - val_acc: 0.9557\n",
      "Epoch 410/500\n",
      "22272/22272 [==============================] - 14s 608us/step - loss: 0.0496 - acc: 0.9638 - val_loss: 0.0485 - val_acc: 0.9565\n",
      "Epoch 411/500\n",
      "22272/22272 [==============================] - 13s 594us/step - loss: 0.0492 - acc: 0.9630 - val_loss: 0.0475 - val_acc: 0.9616\n",
      "Epoch 412/500\n",
      "22272/22272 [==============================] - 14s 607us/step - loss: 0.0492 - acc: 0.9629 - val_loss: 0.0479 - val_acc: 0.9565\n",
      "Epoch 413/500\n",
      "22272/22272 [==============================] - 14s 624us/step - loss: 0.0485 - acc: 0.9652 - val_loss: 0.0479 - val_acc: 0.963383 \n",
      "Epoch 414/500\n",
      "22272/22272 [==============================] - 14s 608us/step - loss: 0.0485 - acc: 0.9654 - val_loss: 0.0472 - val_acc: 0.9574 0.0485 -\n",
      "Epoch 415/500\n",
      "22272/22272 [==============================] - 14s 614us/step - loss: 0.0485 - acc: 0.9661 - val_loss: 0.0496 - val_acc: 0.9565\n",
      "Epoch 416/500\n",
      "22272/22272 [==============================] - 14s 613us/step - loss: 0.0485 - acc: 0.9638 - val_loss: 0.0492 - val_acc: 0.9548\n",
      "Epoch 417/500\n",
      "22272/22272 [==============================] - 14s 610us/step - loss: 0.0488 - acc: 0.9660 - val_loss: 0.0481 - val_acc: 0.9531\n",
      "Epoch 418/500\n",
      "22272/22272 [==============================] - 14s 645us/step - loss: 0.0491 - acc: 0.9648 - val_loss: 0.0484 - val_acc: 0.9574\n",
      "Epoch 419/500\n",
      "22272/22272 [==============================] - 14s 636us/step - loss: 0.0485 - acc: 0.9651 - val_loss: 0.0469 - val_acc: 0.9616\n",
      "Epoch 420/500\n",
      "22272/22272 [==============================] - 14s 622us/step - loss: 0.0486 - acc: 0.9658 - val_loss: 0.0482 - val_acc: 0.9582\n",
      "Epoch 421/500\n",
      "22272/22272 [==============================] - 14s 624us/step - loss: 0.0482 - acc: 0.9656 - val_loss: 0.0474 - val_acc: 0.9591\n",
      "Epoch 422/500\n",
      "22272/22272 [==============================] - 14s 634us/step - loss: 0.0483 - acc: 0.9644 - val_loss: 0.0466 - val_acc: 0.9650\n",
      "Epoch 423/500\n",
      "22272/22272 [==============================] - 14s 626us/step - loss: 0.0482 - acc: 0.9663 - val_loss: 0.0489 - val_acc: 0.9591\n",
      "Epoch 424/500\n",
      "22272/22272 [==============================] - 14s 612us/step - loss: 0.0485 - acc: 0.9631 - val_loss: 0.0472 - val_acc: 0.9599\n",
      "Epoch 425/500\n",
      "22272/22272 [==============================] - 13s 569us/step - loss: 0.0484 - acc: 0.9663 - val_loss: 0.0495 - val_acc: 0.9608\n",
      "Epoch 426/500\n",
      "22272/22272 [==============================] - 12s 528us/step - loss: 0.0485 - acc: 0.9664 - val_loss: 0.0471 - val_acc: 0.9540\n",
      "Epoch 427/500\n",
      "22272/22272 [==============================] - 13s 564us/step - loss: 0.0481 - acc: 0.9645 - val_loss: 0.0458 - val_acc: 0.9642\n",
      "Epoch 428/500\n",
      "22272/22272 [==============================] - 12s 548us/step - loss: 0.0481 - acc: 0.9641 - val_loss: 0.0465 - val_acc: 0.9574\n",
      "Epoch 429/500\n",
      "22272/22272 [==============================] - 12s 524us/step - loss: 0.0480 - acc: 0.9633 - val_loss: 0.0461 - val_acc: 0.9616\n",
      "Epoch 430/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0483 - acc: 0.9647 - val_loss: 0.0477 - val_acc: 0.9625\n",
      "Epoch 431/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0477 - acc: 0.9656 - val_loss: 0.0472 - val_acc: 0.9599\n",
      "Epoch 432/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0474 - acc: 0.9652 - val_loss: 0.0476 - val_acc: 0.9582\n",
      "Epoch 433/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0474 - acc: 0.9663 - val_loss: 0.0466 - val_acc: 0.9591\n",
      "Epoch 434/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0478 - acc: 0.9644 - val_loss: 0.0460 - val_acc: 0.9591\n",
      "Epoch 435/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0481 - acc: 0.9645 - val_loss: 0.0469 - val_acc: 0.9557\n",
      "Epoch 436/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0477 - acc: 0.9655 - val_loss: 0.0475 - val_acc: 0.9582\n",
      "Epoch 437/500\n",
      "22272/22272 [==============================] - 12s 521us/step - loss: 0.0478 - acc: 0.9665 - val_loss: 0.0474 - val_acc: 0.9633\n",
      "Epoch 438/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0477 - acc: 0.9665 - val_loss: 0.0496 - val_acc: 0.9531\n",
      "Epoch 439/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0478 - acc: 0.9665 - val_loss: 0.0471 - val_acc: 0.9591\n",
      "Epoch 440/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0476 - acc: 0.9676 - val_loss: 0.0469 - val_acc: 0.9608\n",
      "Epoch 441/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0470 - acc: 0.9672 - val_loss: 0.0454 - val_acc: 0.9574\n",
      "Epoch 442/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0478 - acc: 0.9647 - val_loss: 0.0470 - val_acc: 0.9557\n",
      "Epoch 443/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.0476 - acc: 0.9670 - val_loss: 0.0455 - val_acc: 0.9616\n",
      "Epoch 444/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.0472 - acc: 0.9665 - val_loss: 0.0457 - val_acc: 0.9591\n",
      "Epoch 445/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0472 - acc: 0.9663 - val_loss: 0.0460 - val_acc: 0.9599\n",
      "Epoch 446/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0471 - acc: 0.9666 - val_loss: 0.0478 - val_acc: 0.9574\n",
      "Epoch 447/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0469 - acc: 0.9648 - val_loss: 0.0461 - val_acc: 0.9625\n",
      "Epoch 448/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0470 - acc: 0.9663 - val_loss: 0.0455 - val_acc: 0.9642\n",
      "Epoch 449/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0474 - acc: 0.9658 - val_loss: 0.0472 - val_acc: 0.9574\n",
      "Epoch 450/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0474 - acc: 0.9660 - val_loss: 0.0448 - val_acc: 0.9616\n",
      "Epoch 451/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0470 - acc: 0.9666 - val_loss: 0.0473 - val_acc: 0.9608\n",
      "Epoch 452/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0470 - acc: 0.9662 - val_loss: 0.0460 - val_acc: 0.9565\n",
      "Epoch 453/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0468 - acc: 0.9666 - val_loss: 0.0450 - val_acc: 0.9599\n",
      "Epoch 454/500\n",
      "22272/22272 [==============================] - 12s 521us/step - loss: 0.0472 - acc: 0.9658 - val_loss: 0.0445 - val_acc: 0.9642\n",
      "Epoch 455/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0466 - acc: 0.9675 - val_loss: 0.0458 - val_acc: 0.9642\n",
      "Epoch 456/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0470 - acc: 0.9678 - val_loss: 0.0467 - val_acc: 0.9599\n",
      "Epoch 457/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.0470 - acc: 0.9660 - val_loss: 0.0451 - val_acc: 0.9599\n",
      "Epoch 458/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0465 - acc: 0.9667 - val_loss: 0.0465 - val_acc: 0.9574\n",
      "Epoch 459/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0465 - acc: 0.9665 - val_loss: 0.0454 - val_acc: 0.9608\n",
      "Epoch 460/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0464 - acc: 0.9685 - val_loss: 0.0464 - val_acc: 0.9548\n",
      "Epoch 461/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0468 - acc: 0.9653 - val_loss: 0.0463 - val_acc: 0.9565\n",
      "Epoch 462/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0468 - acc: 0.9662 - val_loss: 0.0456 - val_acc: 0.9599\n",
      "Epoch 463/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0464 - acc: 0.9669 - val_loss: 0.0451 - val_acc: 0.9625\n",
      "Epoch 464/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0465 - acc: 0.9653 - val_loss: 0.0483 - val_acc: 0.9608\n",
      "Epoch 465/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0469 - acc: 0.9652 - val_loss: 0.0467 - val_acc: 0.9599\n",
      "Epoch 466/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0464 - acc: 0.9679 - val_loss: 0.0464 - val_acc: 0.9565\n",
      "Epoch 467/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0467 - acc: 0.9679 - val_loss: 0.0464 - val_acc: 0.9599\n",
      "Epoch 468/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0462 - acc: 0.9655 - val_loss: 0.0438 - val_acc: 0.9633\n",
      "Epoch 469/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0464 - acc: 0.9676 - val_loss: 0.0443 - val_acc: 0.9599\n",
      "Epoch 470/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0460 - acc: 0.9662 - val_loss: 0.0445 - val_acc: 0.9633\n",
      "Epoch 471/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0463 - acc: 0.9659 - val_loss: 0.0454 - val_acc: 0.9668\n",
      "Epoch 472/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0462 - acc: 0.9667 - val_loss: 0.0438 - val_acc: 0.9599\n",
      "Epoch 473/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0463 - acc: 0.9661 - val_loss: 0.0459 - val_acc: 0.9591\n",
      "Epoch 474/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0465 - acc: 0.9684 - val_loss: 0.0461 - val_acc: 0.9659\n",
      "Epoch 475/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0459 - acc: 0.9683 - val_loss: 0.0443 - val_acc: 0.9642\n",
      "Epoch 476/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0459 - acc: 0.9664 - val_loss: 0.0449 - val_acc: 0.9642\n",
      "Epoch 477/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.0456 - acc: 0.9687 - val_loss: 0.0440 - val_acc: 0.9668\n",
      "Epoch 478/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0460 - acc: 0.9667 - val_loss: 0.0458 - val_acc: 0.9608\n",
      "Epoch 479/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0459 - acc: 0.9695 - val_loss: 0.0445 - val_acc: 0.9650\n",
      "Epoch 480/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0460 - acc: 0.9673 - val_loss: 0.0432 - val_acc: 0.9633\n",
      "Epoch 481/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0456 - acc: 0.9691 - val_loss: 0.0460 - val_acc: 0.9591\n",
      "Epoch 482/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0461 - acc: 0.9651 - val_loss: 0.0442 - val_acc: 0.9676\n",
      "Epoch 483/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0451 - acc: 0.9676 - val_loss: 0.0437 - val_acc: 0.9625\n",
      "Epoch 484/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0457 - acc: 0.9686 - val_loss: 0.0451 - val_acc: 0.9642\n",
      "Epoch 485/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0456 - acc: 0.9674 - val_loss: 0.0443 - val_acc: 0.9608\n",
      "Epoch 486/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0459 - acc: 0.9675 - val_loss: 0.0451 - val_acc: 0.9582\n",
      "Epoch 487/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0455 - acc: 0.9665 - val_loss: 0.0454 - val_acc: 0.9625\n",
      "Epoch 488/500\n",
      "22272/22272 [==============================] - 12s 527us/step - loss: 0.0461 - acc: 0.9686 - val_loss: 0.0449 - val_acc: 0.9650\n",
      "Epoch 489/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0454 - acc: 0.9683 - val_loss: 0.0440 - val_acc: 0.9633\n",
      "Epoch 490/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0453 - acc: 0.9687 - val_loss: 0.0432 - val_acc: 0.9599\n",
      "Epoch 491/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0453 - acc: 0.9678 - val_loss: 0.0441 - val_acc: 0.9599\n",
      "Epoch 492/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0454 - acc: 0.9659 - val_loss: 0.0435 - val_acc: 0.9650\n",
      "Epoch 493/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0453 - acc: 0.9674 - val_loss: 0.0441 - val_acc: 0.9633\n",
      "Epoch 494/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0450 - acc: 0.9681 - val_loss: 0.0450 - val_acc: 0.9633\n",
      "Epoch 495/500\n",
      "22272/22272 [==============================] - 12s 542us/step - loss: 0.0452 - acc: 0.9665 - val_loss: 0.0441 - val_acc: 0.9642\n",
      "Epoch 496/500\n",
      "22272/22272 [==============================] - 12s 553us/step - loss: 0.0454 - acc: 0.9664 - val_loss: 0.0447 - val_acc: 0.9616\n",
      "Epoch 497/500\n",
      "22272/22272 [==============================] - 13s 564us/step - loss: 0.0451 - acc: 0.9681 - val_loss: 0.0439 - val_acc: 0.9625\n",
      "Epoch 498/500\n",
      "22272/22272 [==============================] - 13s 578us/step - loss: 0.0450 - acc: 0.9677 - val_loss: 0.0439 - val_acc: 0.9625\n",
      "Epoch 499/500\n",
      "22272/22272 [==============================] - 13s 591us/step - loss: 0.0447 - acc: 0.9680 - val_loss: 0.0441 - val_acc: 0.9642\n",
      "Epoch 500/500\n",
      "22272/22272 [==============================] - 13s 584us/step - loss: 0.0454 - acc: 0.9678 - val_loss: 0.0447 - val_acc: 0.9625\n"
     ]
    }
   ],
   "source": [
    "##Fitting the GRU to the Training set\n",
    "hist4=model4.fit(X_train, y_train, batch_size=300, epochs=500, validation_split=0.05, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22272 samples, validate on 1173 samples\n",
      "Epoch 1/500\n",
      "22272/22272 [==============================] - 15s 685us/step - loss: 0.2302 - acc: 0.7488 - val_loss: 0.2179 - val_acc: 0.7263\n",
      "Epoch 2/500\n",
      "22272/22272 [==============================] - 11s 497us/step - loss: 0.2048 - acc: 0.7485 - val_loss: 0.2056 - val_acc: 0.7340\n",
      "Epoch 3/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.1993 - acc: 0.7517 - val_loss: 0.2055 - val_acc: 0.7366\n",
      "Epoch 4/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.1989 - acc: 0.7515 - val_loss: 0.2035 - val_acc: 0.7315\n",
      "Epoch 5/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.1964 - acc: 0.7537 - val_loss: 0.2057 - val_acc: 0.7425\n",
      "Epoch 6/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.1970 - acc: 0.7549 - val_loss: 0.2024 - val_acc: 0.7460\n",
      "Epoch 7/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.1955 - acc: 0.7557 - val_loss: 0.2014 - val_acc: 0.7366\n",
      "Epoch 8/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.1933 - acc: 0.7570 - val_loss: 0.2018 - val_acc: 0.7494\n",
      "Epoch 9/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.1908 - acc: 0.7517 - val_loss: 0.1955 - val_acc: 0.7417\n",
      "Epoch 10/500\n",
      "22272/22272 [==============================] - 12s 543us/step - loss: 0.1888 - acc: 0.7535 - val_loss: 0.1983 - val_acc: 0.7442\n",
      "Epoch 11/500\n",
      "22272/22272 [==============================] - 13s 562us/step - loss: 0.1874 - acc: 0.7529 - val_loss: 0.1977 - val_acc: 0.7383\n",
      "Epoch 12/500\n",
      "22272/22272 [==============================] - 12s 522us/step - loss: 0.1855 - acc: 0.7534 - val_loss: 0.1909 - val_acc: 0.7442\n",
      "Epoch 13/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.1835 - acc: 0.7600 - val_loss: 0.1922 - val_acc: 0.7477\n",
      "Epoch 14/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.1800 - acc: 0.7672 - val_loss: 0.1913 - val_acc: 0.7442\n",
      "Epoch 15/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.1765 - acc: 0.7700 - val_loss: 0.1912 - val_acc: 0.7357\n",
      "Epoch 16/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.1746 - acc: 0.7733 - val_loss: 0.1780 - val_acc: 0.7639\n",
      "Epoch 17/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.1711 - acc: 0.7803 - val_loss: 0.1763 - val_acc: 0.7724\n",
      "Epoch 18/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.1701 - acc: 0.7825 - val_loss: 0.1746 - val_acc: 0.7741\n",
      "Epoch 19/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.1684 - acc: 0.7902 - val_loss: 0.1767 - val_acc: 0.7715\n",
      "Epoch 20/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.1666 - acc: 0.7919 - val_loss: 0.1716 - val_acc: 0.7775\n",
      "Epoch 21/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.1648 - acc: 0.7947 - val_loss: 0.1753 - val_acc: 0.7673\n",
      "Epoch 22/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.1644 - acc: 0.7974 - val_loss: 0.1699 - val_acc: 0.7766\n",
      "Epoch 23/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.1641 - acc: 0.7978 - val_loss: 0.1695 - val_acc: 0.7758\n",
      "Epoch 24/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.1618 - acc: 0.8006 - val_loss: 0.1684 - val_acc: 0.7783\n",
      "Epoch 25/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.1611 - acc: 0.8019 - val_loss: 0.1660 - val_acc: 0.7835\n",
      "Epoch 26/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.1613 - acc: 0.8006 - val_loss: 0.1662 - val_acc: 0.7818\n",
      "Epoch 27/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.1602 - acc: 0.8026 - val_loss: 0.1665 - val_acc: 0.7869\n",
      "Epoch 28/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.1596 - acc: 0.8054 - val_loss: 0.1651 - val_acc: 0.7818\n",
      "Epoch 29/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.1576 - acc: 0.8083 - val_loss: 0.1677 - val_acc: 0.7783\n",
      "Epoch 30/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.1567 - acc: 0.8086 - val_loss: 0.1629 - val_acc: 0.7920\n",
      "Epoch 31/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.1556 - acc: 0.8110 - val_loss: 0.1623 - val_acc: 0.7911\n",
      "Epoch 32/500\n",
      "22272/22272 [==============================] - 12s 522us/step - loss: 0.1556 - acc: 0.8090 - val_loss: 0.1600 - val_acc: 0.7997\n",
      "Epoch 33/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.1547 - acc: 0.8134 - val_loss: 0.1597 - val_acc: 0.7937\n",
      "Epoch 34/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.1541 - acc: 0.8116 - val_loss: 0.1596 - val_acc: 0.7877\n",
      "Epoch 35/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.1531 - acc: 0.8124 - val_loss: 0.1616 - val_acc: 0.7980\n",
      "Epoch 36/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.1515 - acc: 0.8174 - val_loss: 0.1538 - val_acc: 0.8022\n",
      "Epoch 37/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.1499 - acc: 0.8182 - val_loss: 0.1534 - val_acc: 0.8065\n",
      "Epoch 38/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.1494 - acc: 0.8187 - val_loss: 0.1525 - val_acc: 0.7988\n",
      "Epoch 39/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.1495 - acc: 0.8201 - val_loss: 0.1558 - val_acc: 0.7988\n",
      "Epoch 40/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.1483 - acc: 0.8191 - val_loss: 0.1534 - val_acc: 0.8014\n",
      "Epoch 41/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.1460 - acc: 0.8232 - val_loss: 0.1482 - val_acc: 0.8031\n",
      "Epoch 42/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.1460 - acc: 0.8225 - val_loss: 0.1486 - val_acc: 0.8159\n",
      "Epoch 43/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.1450 - acc: 0.8251 - val_loss: 0.1491 - val_acc: 0.8065\n",
      "Epoch 44/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.1446 - acc: 0.8247 - val_loss: 0.1520 - val_acc: 0.8031\n",
      "Epoch 45/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.1429 - acc: 0.8272 - val_loss: 0.1502 - val_acc: 0.8150\n",
      "Epoch 46/500\n",
      "22272/22272 [==============================] - 12s 526us/step - loss: 0.1426 - acc: 0.8267 - val_loss: 0.1445 - val_acc: 0.8201\n",
      "Epoch 47/500\n",
      "22272/22272 [==============================] - 12s 528us/step - loss: 0.1421 - acc: 0.8296 - val_loss: 0.1479 - val_acc: 0.8073\n",
      "Epoch 48/500\n",
      "22272/22272 [==============================] - 12s 526us/step - loss: 0.1406 - acc: 0.8332 - val_loss: 0.1478 - val_acc: 0.8167\n",
      "Epoch 49/500\n",
      "22272/22272 [==============================] - 12s 516us/step - loss: 0.1393 - acc: 0.8341 - val_loss: 0.1406 - val_acc: 0.8244\n",
      "Epoch 50/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.1384 - acc: 0.8338 - val_loss: 0.1415 - val_acc: 0.8193\n",
      "Epoch 51/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.1376 - acc: 0.8378 - val_loss: 0.1401 - val_acc: 0.8269\n",
      "Epoch 52/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.1370 - acc: 0.8377 - val_loss: 0.1415 - val_acc: 0.8278\n",
      "Epoch 53/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.1357 - acc: 0.8399 - val_loss: 0.1432 - val_acc: 0.8210\n",
      "Epoch 54/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.1345 - acc: 0.8424 - val_loss: 0.1363 - val_acc: 0.8372\n",
      "Epoch 55/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.1336 - acc: 0.8429 - val_loss: 0.1367 - val_acc: 0.8440\n",
      "Epoch 56/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.1333 - acc: 0.8473 - val_loss: 0.1403 - val_acc: 0.8363\n",
      "Epoch 57/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.1321 - acc: 0.8475 - val_loss: 0.1336 - val_acc: 0.8363\n",
      "Epoch 58/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.1315 - acc: 0.8491 - val_loss: 0.1344 - val_acc: 0.8508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.1296 - acc: 0.8495 - val_loss: 0.1336 - val_acc: 0.8346\n",
      "Epoch 60/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.1295 - acc: 0.8531 - val_loss: 0.1314 - val_acc: 0.8406\n",
      "Epoch 61/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.1282 - acc: 0.8530 - val_loss: 0.1317 - val_acc: 0.8423\n",
      "Epoch 62/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.1271 - acc: 0.8559 - val_loss: 0.1292 - val_acc: 0.8389\n",
      "Epoch 63/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.1265 - acc: 0.8565 - val_loss: 0.1263 - val_acc: 0.8500\n",
      "Epoch 64/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.1266 - acc: 0.8539 - val_loss: 0.1295 - val_acc: 0.8440\n",
      "Epoch 65/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.1247 - acc: 0.8596 - val_loss: 0.1234 - val_acc: 0.8559\n",
      "Epoch 66/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.1236 - acc: 0.8606 - val_loss: 0.1250 - val_acc: 0.8508\n",
      "Epoch 67/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.1229 - acc: 0.8605 - val_loss: 0.1277 - val_acc: 0.8491\n",
      "Epoch 68/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.1225 - acc: 0.8649 - val_loss: 0.1274 - val_acc: 0.8474\n",
      "Epoch 69/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.1215 - acc: 0.8641 - val_loss: 0.1214 - val_acc: 0.8593\n",
      "Epoch 70/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.1210 - acc: 0.8652 - val_loss: 0.1221 - val_acc: 0.8576\n",
      "Epoch 71/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.1201 - acc: 0.8662 - val_loss: 0.1279 - val_acc: 0.8406\n",
      "Epoch 72/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.1194 - acc: 0.8666 - val_loss: 0.1195 - val_acc: 0.8585\n",
      "Epoch 73/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.1182 - acc: 0.8711 - val_loss: 0.1189 - val_acc: 0.8568\n",
      "Epoch 74/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.1186 - acc: 0.8694 - val_loss: 0.1234 - val_acc: 0.8593\n",
      "Epoch 75/500\n",
      "22272/22272 [==============================] - 12s 521us/step - loss: 0.1171 - acc: 0.8713 - val_loss: 0.1185 - val_acc: 0.8696\n",
      "Epoch 76/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.1162 - acc: 0.8741 - val_loss: 0.1132 - val_acc: 0.8679\n",
      "Epoch 77/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.1156 - acc: 0.8763 - val_loss: 0.1160 - val_acc: 0.8721\n",
      "Epoch 78/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.1142 - acc: 0.8767 - val_loss: 0.1137 - val_acc: 0.8653\n",
      "Epoch 79/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.1139 - acc: 0.8760 - val_loss: 0.1138 - val_acc: 0.8704\n",
      "Epoch 80/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.1140 - acc: 0.8761 - val_loss: 0.1156 - val_acc: 0.8636\n",
      "Epoch 81/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.1130 - acc: 0.8790 - val_loss: 0.1163 - val_acc: 0.8602\n",
      "Epoch 82/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.1118 - acc: 0.8790 - val_loss: 0.1128 - val_acc: 0.8670\n",
      "Epoch 83/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.1122 - acc: 0.8794 - val_loss: 0.1101 - val_acc: 0.8696\n",
      "Epoch 84/500\n",
      "22272/22272 [==============================] - 12s 530us/step - loss: 0.1111 - acc: 0.8804 - val_loss: 0.1109 - val_acc: 0.8755\n",
      "Epoch 85/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.1103 - acc: 0.8831 - val_loss: 0.1139 - val_acc: 0.8696\n",
      "Epoch 86/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.1106 - acc: 0.8842 - val_loss: 0.1090 - val_acc: 0.8764\n",
      "Epoch 87/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.1083 - acc: 0.8852 - val_loss: 0.1053 - val_acc: 0.8849\n",
      "Epoch 88/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.1085 - acc: 0.8843 - val_loss: 0.1087 - val_acc: 0.8789\n",
      "Epoch 89/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.1072 - acc: 0.8864 - val_loss: 0.1081 - val_acc: 0.8721\n",
      "Epoch 90/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.1069 - acc: 0.8874 - val_loss: 0.1062 - val_acc: 0.8687\n",
      "Epoch 91/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.1062 - acc: 0.8890 - val_loss: 0.1086 - val_acc: 0.8832\n",
      "Epoch 92/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.1054 - acc: 0.8892 - val_loss: 0.1083 - val_acc: 0.8772\n",
      "Epoch 93/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.1050 - acc: 0.8917 - val_loss: 0.1044 - val_acc: 0.8815\n",
      "Epoch 94/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.1047 - acc: 0.8908 - val_loss: 0.1059 - val_acc: 0.8841\n",
      "Epoch 95/500\n",
      "22272/22272 [==============================] - 12s 520us/step - loss: 0.1035 - acc: 0.8932 - val_loss: 0.1024 - val_acc: 0.8841\n",
      "Epoch 96/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.1027 - acc: 0.8932 - val_loss: 0.1019 - val_acc: 0.8900\n",
      "Epoch 97/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.1032 - acc: 0.8934 - val_loss: 0.1028 - val_acc: 0.8806\n",
      "Epoch 98/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.1025 - acc: 0.8969 - val_loss: 0.1014 - val_acc: 0.8994\n",
      "Epoch 99/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.1015 - acc: 0.8966 - val_loss: 0.1021 - val_acc: 0.8849\n",
      "Epoch 100/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.1013 - acc: 0.8975 - val_loss: 0.1006 - val_acc: 0.8883\n",
      "Epoch 101/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.1005 - acc: 0.8991 - val_loss: 0.1024 - val_acc: 0.8900\n",
      "Epoch 102/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.1000 - acc: 0.8987 - val_loss: 0.1033 - val_acc: 0.8900\n",
      "Epoch 103/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0998 - acc: 0.9004 - val_loss: 0.1025 - val_acc: 0.8815\n",
      "Epoch 104/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0991 - acc: 0.8993 - val_loss: 0.0981 - val_acc: 0.8917\n",
      "Epoch 105/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.0985 - acc: 0.8986 - val_loss: 0.0967 - val_acc: 0.8986\n",
      "Epoch 106/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0981 - acc: 0.9002 - val_loss: 0.0959 - val_acc: 0.8960\n",
      "Epoch 107/500\n",
      "22272/22272 [==============================] - 12s 522us/step - loss: 0.0975 - acc: 0.9043 - val_loss: 0.1005 - val_acc: 0.8943\n",
      "Epoch 108/500\n",
      "22272/22272 [==============================] - 12s 550us/step - loss: 0.0970 - acc: 0.9015 - val_loss: 0.1003 - val_acc: 0.8892\n",
      "Epoch 109/500\n",
      "22272/22272 [==============================] - 12s 520us/step - loss: 0.0960 - acc: 0.9023 - val_loss: 0.0953 - val_acc: 0.9037\n",
      "Epoch 110/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0964 - acc: 0.9054 - val_loss: 0.0999 - val_acc: 0.8866\n",
      "Epoch 111/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0954 - acc: 0.9049 - val_loss: 0.0959 - val_acc: 0.8934\n",
      "Epoch 112/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0949 - acc: 0.9069 - val_loss: 0.0969 - val_acc: 0.8815\n",
      "Epoch 113/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0947 - acc: 0.9060 - val_loss: 0.0955 - val_acc: 0.8951\n",
      "Epoch 114/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0941 - acc: 0.9062 - val_loss: 0.0936 - val_acc: 0.8943\n",
      "Epoch 115/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0945 - acc: 0.9050 - val_loss: 0.0923 - val_acc: 0.9003\n",
      "Epoch 116/500\n",
      "22272/22272 [==============================] - 12s 520us/step - loss: 0.0931 - acc: 0.9087 - val_loss: 0.0993 - val_acc: 0.8883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.0935 - acc: 0.9080 - val_loss: 0.0937 - val_acc: 0.9079\n",
      "Epoch 118/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0922 - acc: 0.9104 - val_loss: 0.0958 - val_acc: 0.8934\n",
      "Epoch 119/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0923 - acc: 0.9102 - val_loss: 0.0924 - val_acc: 0.9011\n",
      "Epoch 120/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0918 - acc: 0.9105 - val_loss: 0.0931 - val_acc: 0.9122\n",
      "Epoch 121/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0913 - acc: 0.9118 - val_loss: 0.0876 - val_acc: 0.9037\n",
      "Epoch 122/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0902 - acc: 0.9149 - val_loss: 0.0901 - val_acc: 0.9088\n",
      "Epoch 123/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0906 - acc: 0.9118 - val_loss: 0.0912 - val_acc: 0.9020\n",
      "Epoch 124/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0906 - acc: 0.9117 - val_loss: 0.0967 - val_acc: 0.8917\n",
      "Epoch 125/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0890 - acc: 0.9141 - val_loss: 0.0929 - val_acc: 0.9079\n",
      "Epoch 126/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0895 - acc: 0.9143 - val_loss: 0.0900 - val_acc: 0.9105\n",
      "Epoch 127/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0889 - acc: 0.9149 - val_loss: 0.0889 - val_acc: 0.9113\n",
      "Epoch 128/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0892 - acc: 0.9161 - val_loss: 0.0901 - val_acc: 0.8968\n",
      "Epoch 129/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.0889 - acc: 0.9137 - val_loss: 0.0946 - val_acc: 0.8994\n",
      "Epoch 130/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0874 - acc: 0.9173 - val_loss: 0.0838 - val_acc: 0.9088\n",
      "Epoch 131/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0871 - acc: 0.9154 - val_loss: 0.0849 - val_acc: 0.9147\n",
      "Epoch 132/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0868 - acc: 0.9159 - val_loss: 0.0850 - val_acc: 0.9113\n",
      "Epoch 133/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0871 - acc: 0.9180 - val_loss: 0.0856 - val_acc: 0.9088\n",
      "Epoch 134/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0863 - acc: 0.9178 - val_loss: 0.0906 - val_acc: 0.9045\n",
      "Epoch 135/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0866 - acc: 0.9176 - val_loss: 0.0897 - val_acc: 0.9130\n",
      "Epoch 136/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0854 - acc: 0.9189 - val_loss: 0.0848 - val_acc: 0.9165\n",
      "Epoch 137/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0849 - acc: 0.9204 - val_loss: 0.0858 - val_acc: 0.9165\n",
      "Epoch 138/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0845 - acc: 0.9206 - val_loss: 0.0867 - val_acc: 0.9130\n",
      "Epoch 139/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0843 - acc: 0.9211 - val_loss: 0.0850 - val_acc: 0.9156\n",
      "Epoch 140/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0847 - acc: 0.9203 - val_loss: 0.0874 - val_acc: 0.9113\n",
      "Epoch 141/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0845 - acc: 0.9227 - val_loss: 0.0844 - val_acc: 0.9079\n",
      "Epoch 142/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0836 - acc: 0.9215 - val_loss: 0.0821 - val_acc: 0.9147\n",
      "Epoch 143/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0836 - acc: 0.9212 - val_loss: 0.0888 - val_acc: 0.9045\n",
      "Epoch 144/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0825 - acc: 0.9209 - val_loss: 0.0827 - val_acc: 0.9190\n",
      "Epoch 145/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0827 - acc: 0.9224 - val_loss: 0.0861 - val_acc: 0.9190\n",
      "Epoch 146/500\n",
      "22272/22272 [==============================] - 12s 524us/step - loss: 0.0834 - acc: 0.9243 - val_loss: 0.0809 - val_acc: 0.9199\n",
      "Epoch 147/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0825 - acc: 0.9240 - val_loss: 0.0868 - val_acc: 0.9113\n",
      "Epoch 148/500\n",
      "22272/22272 [==============================] - 12s 522us/step - loss: 0.0825 - acc: 0.9227 - val_loss: 0.0809 - val_acc: 0.9233\n",
      "Epoch 149/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0813 - acc: 0.9292 - val_loss: 0.0801 - val_acc: 0.9156\n",
      "Epoch 150/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0814 - acc: 0.9246 - val_loss: 0.0815 - val_acc: 0.9190\n",
      "Epoch 151/500\n",
      "22272/22272 [==============================] - 12s 527us/step - loss: 0.0810 - acc: 0.9260 - val_loss: 0.0803 - val_acc: 0.9190\n",
      "Epoch 152/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0802 - acc: 0.9268 - val_loss: 0.0792 - val_acc: 0.9241\n",
      "Epoch 153/500\n",
      "22272/22272 [==============================] - 12s 516us/step - loss: 0.0811 - acc: 0.9275 - val_loss: 0.0833 - val_acc: 0.9190\n",
      "Epoch 154/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0802 - acc: 0.9263 - val_loss: 0.0806 - val_acc: 0.9173\n",
      "Epoch 155/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0799 - acc: 0.9272 - val_loss: 0.0788 - val_acc: 0.9147\n",
      "Epoch 156/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0794 - acc: 0.9288 - val_loss: 0.0831 - val_acc: 0.9130\n",
      "Epoch 157/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0795 - acc: 0.9262 - val_loss: 0.0817 - val_acc: 0.9147\n",
      "Epoch 158/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0806 - acc: 0.9260 - val_loss: 0.0785 - val_acc: 0.9216\n",
      "Epoch 159/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0782 - acc: 0.9281 - val_loss: 0.0813 - val_acc: 0.9165\n",
      "Epoch 160/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0789 - acc: 0.9298 - val_loss: 0.0793 - val_acc: 0.9258\n",
      "Epoch 161/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.0781 - acc: 0.9317 - val_loss: 0.0761 - val_acc: 0.9241\n",
      "Epoch 162/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0780 - acc: 0.9298 - val_loss: 0.0806 - val_acc: 0.9233\n",
      "Epoch 163/500\n",
      "22272/22272 [==============================] - 12s 522us/step - loss: 0.0779 - acc: 0.9305 - val_loss: 0.0771 - val_acc: 0.9250\n",
      "Epoch 164/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0772 - acc: 0.9307 - val_loss: 0.0780 - val_acc: 0.9301\n",
      "Epoch 165/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0767 - acc: 0.9332 - val_loss: 0.0786 - val_acc: 0.9258\n",
      "Epoch 166/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0777 - acc: 0.9303 - val_loss: 0.0785 - val_acc: 0.9182\n",
      "Epoch 167/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0764 - acc: 0.9338 - val_loss: 0.0752 - val_acc: 0.9250\n",
      "Epoch 168/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0764 - acc: 0.9324 - val_loss: 0.0780 - val_acc: 0.9216\n",
      "Epoch 169/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0762 - acc: 0.9291 - val_loss: 0.0764 - val_acc: 0.9224\n",
      "Epoch 170/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0761 - acc: 0.9321 - val_loss: 0.0769 - val_acc: 0.9318\n",
      "Epoch 171/500\n",
      "22272/22272 [==============================] - 12s 522us/step - loss: 0.0761 - acc: 0.9319 - val_loss: 0.0752 - val_acc: 0.9233\n",
      "Epoch 172/500\n",
      "22272/22272 [==============================] - 12s 529us/step - loss: 0.0759 - acc: 0.9318 - val_loss: 0.0749 - val_acc: 0.9309\n",
      "Epoch 173/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0757 - acc: 0.9330 - val_loss: 0.0747 - val_acc: 0.9199\n",
      "Epoch 174/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0750 - acc: 0.9339 - val_loss: 0.0743 - val_acc: 0.9241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0753 - acc: 0.9350 - val_loss: 0.0735 - val_acc: 0.9241\n",
      "Epoch 176/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0745 - acc: 0.9354 - val_loss: 0.0782 - val_acc: 0.9275\n",
      "Epoch 177/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0747 - acc: 0.9357 - val_loss: 0.0755 - val_acc: 0.9216\n",
      "Epoch 178/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0742 - acc: 0.9352 - val_loss: 0.0727 - val_acc: 0.9318\n",
      "Epoch 179/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0737 - acc: 0.9376 - val_loss: 0.0735 - val_acc: 0.9267\n",
      "Epoch 180/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0742 - acc: 0.9369 - val_loss: 0.0765 - val_acc: 0.9250\n",
      "Epoch 181/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0731 - acc: 0.9365 - val_loss: 0.0735 - val_acc: 0.9267\n",
      "Epoch 182/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0732 - acc: 0.9361 - val_loss: 0.0735 - val_acc: 0.9156\n",
      "Epoch 183/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0730 - acc: 0.9376 - val_loss: 0.0712 - val_acc: 0.9165\n",
      "Epoch 184/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0727 - acc: 0.9361 - val_loss: 0.0724 - val_acc: 0.9275\n",
      "Epoch 185/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0725 - acc: 0.9384 - val_loss: 0.0733 - val_acc: 0.9250\n",
      "Epoch 186/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0726 - acc: 0.9369 - val_loss: 0.0751 - val_acc: 0.9267\n",
      "Epoch 187/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0721 - acc: 0.9363 - val_loss: 0.0718 - val_acc: 0.9241\n",
      "Epoch 188/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0718 - acc: 0.9397 - val_loss: 0.0705 - val_acc: 0.9344\n",
      "Epoch 189/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0721 - acc: 0.9392 - val_loss: 0.0721 - val_acc: 0.9207\n",
      "Epoch 190/500\n",
      "22272/22272 [==============================] - 12s 520us/step - loss: 0.0717 - acc: 0.9368 - val_loss: 0.0693 - val_acc: 0.9335\n",
      "Epoch 191/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0711 - acc: 0.9399 - val_loss: 0.0706 - val_acc: 0.9284\n",
      "Epoch 192/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0712 - acc: 0.9388 - val_loss: 0.0696 - val_acc: 0.9318\n",
      "Epoch 193/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0714 - acc: 0.9414 - val_loss: 0.0719 - val_acc: 0.9190\n",
      "Epoch 194/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0712 - acc: 0.9385 - val_loss: 0.0717 - val_acc: 0.9318\n",
      "Epoch 195/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0710 - acc: 0.9397 - val_loss: 0.0699 - val_acc: 0.9301\n",
      "Epoch 196/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0704 - acc: 0.9402 - val_loss: 0.0722 - val_acc: 0.9284\n",
      "Epoch 197/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0706 - acc: 0.9386 - val_loss: 0.0741 - val_acc: 0.9250\n",
      "Epoch 198/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0702 - acc: 0.9395 - val_loss: 0.0706 - val_acc: 0.9267\n",
      "Epoch 199/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0698 - acc: 0.9386 - val_loss: 0.0734 - val_acc: 0.9258\n",
      "Epoch 200/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0689 - acc: 0.9415 - val_loss: 0.0722 - val_acc: 0.9216\n",
      "Epoch 201/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0706 - acc: 0.9364 - val_loss: 0.0716 - val_acc: 0.9318\n",
      "Epoch 202/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0700 - acc: 0.9395 - val_loss: 0.0740 - val_acc: 0.9292\n",
      "Epoch 203/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0688 - acc: 0.9423 - val_loss: 0.0692 - val_acc: 0.9327\n",
      "Epoch 204/500\n",
      "22272/22272 [==============================] - 12s 521us/step - loss: 0.0688 - acc: 0.9421 - val_loss: 0.0721 - val_acc: 0.9327\n",
      "Epoch 205/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0687 - acc: 0.9431 - val_loss: 0.0678 - val_acc: 0.9309\n",
      "Epoch 206/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0683 - acc: 0.9428 - val_loss: 0.0697 - val_acc: 0.9344\n",
      "Epoch 207/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0682 - acc: 0.9418 - val_loss: 0.0682 - val_acc: 0.9386\n",
      "Epoch 208/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0685 - acc: 0.9423 - val_loss: 0.0724 - val_acc: 0.9327\n",
      "Epoch 209/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0675 - acc: 0.9440 - val_loss: 0.0688 - val_acc: 0.9301\n",
      "Epoch 210/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0681 - acc: 0.9442 - val_loss: 0.0685 - val_acc: 0.9318\n",
      "Epoch 211/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0679 - acc: 0.9439 - val_loss: 0.0680 - val_acc: 0.9327\n",
      "Epoch 212/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0676 - acc: 0.9433 - val_loss: 0.0708 - val_acc: 0.9292\n",
      "Epoch 213/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0682 - acc: 0.9424 - val_loss: 0.0676 - val_acc: 0.9284\n",
      "Epoch 214/500\n",
      "22272/22272 [==============================] - 12s 532us/step - loss: 0.0672 - acc: 0.9440 - val_loss: 0.0688 - val_acc: 0.9361\n",
      "Epoch 215/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.0669 - acc: 0.9443 - val_loss: 0.0659 - val_acc: 0.9250\n",
      "Epoch 216/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0670 - acc: 0.9445 - val_loss: 0.0685 - val_acc: 0.9301\n",
      "Epoch 217/500\n",
      "22272/22272 [==============================] - 12s 527us/step - loss: 0.0668 - acc: 0.9425 - val_loss: 0.0659 - val_acc: 0.9344\n",
      "Epoch 218/500\n",
      "22272/22272 [==============================] - 13s 570us/step - loss: 0.0664 - acc: 0.9466 - val_loss: 0.0662 - val_acc: 0.9258\n",
      "Epoch 219/500\n",
      "22272/22272 [==============================] - 12s 525us/step - loss: 0.0661 - acc: 0.9451 - val_loss: 0.0654 - val_acc: 0.9361\n",
      "Epoch 220/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0658 - acc: 0.9463 - val_loss: 0.0683 - val_acc: 0.9309\n",
      "Epoch 221/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0659 - acc: 0.9442 - val_loss: 0.0668 - val_acc: 0.9301\n",
      "Epoch 222/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0668 - acc: 0.9428 - val_loss: 0.0673 - val_acc: 0.9284\n",
      "Epoch 223/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.0658 - acc: 0.9466 - val_loss: 0.0663 - val_acc: 0.9361\n",
      "Epoch 224/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0659 - acc: 0.9465 - val_loss: 0.0687 - val_acc: 0.9292\n",
      "Epoch 225/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0662 - acc: 0.9433 - val_loss: 0.0670 - val_acc: 0.9327\n",
      "Epoch 226/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0654 - acc: 0.9463 - val_loss: 0.0659 - val_acc: 0.9318\n",
      "Epoch 227/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0651 - acc: 0.9470 - val_loss: 0.0672 - val_acc: 0.9318\n",
      "Epoch 228/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0661 - acc: 0.9451 - val_loss: 0.0641 - val_acc: 0.9309\n",
      "Epoch 229/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0650 - acc: 0.9457 - val_loss: 0.0654 - val_acc: 0.9412\n",
      "Epoch 230/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0646 - acc: 0.9469 - val_loss: 0.0638 - val_acc: 0.9378\n",
      "Epoch 231/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0649 - acc: 0.9458 - val_loss: 0.0667 - val_acc: 0.9318\n",
      "Epoch 232/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0645 - acc: 0.9486 - val_loss: 0.0646 - val_acc: 0.9403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/500\n",
      "22272/22272 [==============================] - 11s 497us/step - loss: 0.0642 - acc: 0.9481 - val_loss: 0.0649 - val_acc: 0.9344\n",
      "Epoch 234/500\n",
      "22272/22272 [==============================] - 11s 498us/step - loss: 0.0644 - acc: 0.9487 - val_loss: 0.0643 - val_acc: 0.9344\n",
      "Epoch 235/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0643 - acc: 0.9476 - val_loss: 0.0648 - val_acc: 0.9309\n",
      "Epoch 236/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0645 - acc: 0.9468 - val_loss: 0.0651 - val_acc: 0.9344\n",
      "Epoch 237/500\n",
      "22272/22272 [==============================] - 12s 539us/step - loss: 0.0642 - acc: 0.9477 - val_loss: 0.0640 - val_acc: 0.9369\n",
      "Epoch 238/500\n",
      "22272/22272 [==============================] - 12s 545us/step - loss: 0.0639 - acc: 0.9476 - val_loss: 0.0631 - val_acc: 0.9412\n",
      "Epoch 239/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0639 - acc: 0.9481 - val_loss: 0.0627 - val_acc: 0.9361\n",
      "Epoch 240/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0640 - acc: 0.9494 - val_loss: 0.0650 - val_acc: 0.9386\n",
      "Epoch 241/500\n",
      "22272/22272 [==============================] - 12s 522us/step - loss: 0.0639 - acc: 0.9454 - val_loss: 0.0621 - val_acc: 0.9369\n",
      "Epoch 242/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0634 - acc: 0.9485 - val_loss: 0.0656 - val_acc: 0.9403\n",
      "Epoch 243/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.0633 - acc: 0.9483 - val_loss: 0.0639 - val_acc: 0.9361\n",
      "Epoch 244/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0625 - acc: 0.9503 - val_loss: 0.0619 - val_acc: 0.9403\n",
      "Epoch 245/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0623 - acc: 0.9497 - val_loss: 0.0620 - val_acc: 0.9344\n",
      "Epoch 246/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0625 - acc: 0.9472 - val_loss: 0.0643 - val_acc: 0.9352\n",
      "Epoch 247/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0632 - acc: 0.9506 - val_loss: 0.0634 - val_acc: 0.9378\n",
      "Epoch 248/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0621 - acc: 0.9507 - val_loss: 0.0623 - val_acc: 0.9386\n",
      "Epoch 249/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0625 - acc: 0.9504 - val_loss: 0.0635 - val_acc: 0.9352\n",
      "Epoch 250/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0624 - acc: 0.9503 - val_loss: 0.0642 - val_acc: 0.9361\n",
      "Epoch 251/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0619 - acc: 0.9514 - val_loss: 0.0618 - val_acc: 0.9412\n",
      "Epoch 252/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0619 - acc: 0.9497 - val_loss: 0.0638 - val_acc: 0.9378\n",
      "Epoch 253/500\n",
      "22272/22272 [==============================] - 12s 516us/step - loss: 0.0619 - acc: 0.9503 - val_loss: 0.0644 - val_acc: 0.9327\n",
      "Epoch 254/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0622 - acc: 0.9514 - val_loss: 0.0623 - val_acc: 0.9403\n",
      "Epoch 255/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0609 - acc: 0.9524 - val_loss: 0.0618 - val_acc: 0.9327\n",
      "Epoch 256/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0614 - acc: 0.9514 - val_loss: 0.0617 - val_acc: 0.9395\n",
      "Epoch 257/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0615 - acc: 0.9494 - val_loss: 0.0619 - val_acc: 0.9344\n",
      "Epoch 258/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0607 - acc: 0.9520 - val_loss: 0.0613 - val_acc: 0.9429\n",
      "Epoch 259/500\n",
      "22272/22272 [==============================] - 12s 521us/step - loss: 0.0613 - acc: 0.9491 - val_loss: 0.0628 - val_acc: 0.9378\n",
      "Epoch 260/500\n",
      "22272/22272 [==============================] - 12s 529us/step - loss: 0.0609 - acc: 0.9526 - val_loss: 0.0612 - val_acc: 0.9403\n",
      "Epoch 261/500\n",
      "22272/22272 [==============================] - 12s 543us/step - loss: 0.0608 - acc: 0.9508 - val_loss: 0.0593 - val_acc: 0.9361\n",
      "Epoch 262/500\n",
      "22272/22272 [==============================] - 12s 543us/step - loss: 0.0609 - acc: 0.9530 - val_loss: 0.0614 - val_acc: 0.9420\n",
      "Epoch 263/500\n",
      "22272/22272 [==============================] - 12s 531us/step - loss: 0.0605 - acc: 0.9522 - val_loss: 0.0604 - val_acc: 0.9369\n",
      "Epoch 264/500\n",
      "22272/22272 [==============================] - 12s 552us/step - loss: 0.0605 - acc: 0.9535 - val_loss: 0.0616 - val_acc: 0.9412\n",
      "Epoch 265/500\n",
      "22272/22272 [==============================] - 12s 540us/step - loss: 0.0604 - acc: 0.9528 - val_loss: 0.0605 - val_acc: 0.9446\n",
      "Epoch 266/500\n",
      "22272/22272 [==============================] - 12s 537us/step - loss: 0.0605 - acc: 0.9502 - val_loss: 0.0613 - val_acc: 0.9395\n",
      "Epoch 267/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0601 - acc: 0.9541 - val_loss: 0.0617 - val_acc: 0.9378\n",
      "Epoch 268/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0600 - acc: 0.9523 - val_loss: 0.0598 - val_acc: 0.9403\n",
      "Epoch 269/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0596 - acc: 0.9504 - val_loss: 0.0597 - val_acc: 0.9412\n",
      "Epoch 270/500\n",
      "22272/22272 [==============================] - 12s 549us/step - loss: 0.0602 - acc: 0.9516 - val_loss: 0.0588 - val_acc: 0.9437\n",
      "Epoch 271/500\n",
      "22272/22272 [==============================] - 12s 548us/step - loss: 0.0596 - acc: 0.9532 - val_loss: 0.0596 - val_acc: 0.9361\n",
      "Epoch 272/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0596 - acc: 0.9520 - val_loss: 0.0605 - val_acc: 0.9412\n",
      "Epoch 273/500\n",
      "22272/22272 [==============================] - 11s 492us/step - loss: 0.0592 - acc: 0.9537 - val_loss: 0.0600 - val_acc: 0.9395\n",
      "Epoch 274/500\n",
      "22272/22272 [==============================] - 11s 491us/step - loss: 0.0594 - acc: 0.9537 - val_loss: 0.0608 - val_acc: 0.9403\n",
      "Epoch 275/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0594 - acc: 0.9536 - val_loss: 0.0602 - val_acc: 0.9412\n",
      "Epoch 276/500\n",
      "22272/22272 [==============================] - 11s 493us/step - loss: 0.0592 - acc: 0.9534 - val_loss: 0.0623 - val_acc: 0.9420\n",
      "Epoch 277/500\n",
      "22272/22272 [==============================] - 12s 523us/step - loss: 0.0588 - acc: 0.9525 - val_loss: 0.0581 - val_acc: 0.9395\n",
      "Epoch 278/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0590 - acc: 0.9552 - val_loss: 0.0587 - val_acc: 0.9429\n",
      "Epoch 279/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0586 - acc: 0.9542 - val_loss: 0.0603 - val_acc: 0.9395\n",
      "Epoch 280/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0588 - acc: 0.9541 - val_loss: 0.0585 - val_acc: 0.9446\n",
      "Epoch 281/500\n",
      "22272/22272 [==============================] - 11s 502us/step - loss: 0.0588 - acc: 0.9539 - val_loss: 0.0590 - val_acc: 0.9446\n",
      "Epoch 282/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0582 - acc: 0.9546 - val_loss: 0.0584 - val_acc: 0.9412\n",
      "Epoch 283/500\n",
      "22272/22272 [==============================] - 12s 520us/step - loss: 0.0581 - acc: 0.9552 - val_loss: 0.0581 - val_acc: 0.9446\n",
      "Epoch 284/500\n",
      "22272/22272 [==============================] - 11s 497us/step - loss: 0.0581 - acc: 0.9540 - val_loss: 0.0598 - val_acc: 0.9378\n",
      "Epoch 285/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0581 - acc: 0.9533 - val_loss: 0.0612 - val_acc: 0.9412\n",
      "Epoch 286/500\n",
      "22272/22272 [==============================] - 12s 523us/step - loss: 0.0581 - acc: 0.9554 - val_loss: 0.0647 - val_acc: 0.9395\n",
      "Epoch 287/500\n",
      "22272/22272 [==============================] - 12s 525us/step - loss: 0.0579 - acc: 0.9546 - val_loss: 0.0573 - val_acc: 0.9437\n",
      "Epoch 288/500\n",
      "22272/22272 [==============================] - 12s 527us/step - loss: 0.0576 - acc: 0.9552 - val_loss: 0.0578 - val_acc: 0.9412\n",
      "Epoch 289/500\n",
      "22272/22272 [==============================] - 12s 553us/step - loss: 0.0575 - acc: 0.9556 - val_loss: 0.0590 - val_acc: 0.9454\n",
      "Epoch 290/500\n",
      "22272/22272 [==============================] - 12s 524us/step - loss: 0.0573 - acc: 0.9536 - val_loss: 0.0578 - val_acc: 0.9454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0574 - acc: 0.9556 - val_loss: 0.0595 - val_acc: 0.9412\n",
      "Epoch 292/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0577 - acc: 0.9522 - val_loss: 0.0567 - val_acc: 0.9386\n",
      "Epoch 293/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0573 - acc: 0.9571 - val_loss: 0.0579 - val_acc: 0.9429\n",
      "Epoch 294/500\n",
      "22272/22272 [==============================] - 12s 522us/step - loss: 0.0577 - acc: 0.9542 - val_loss: 0.0562 - val_acc: 0.9352\n",
      "Epoch 295/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.0572 - acc: 0.9566 - val_loss: 0.0559 - val_acc: 0.9403\n",
      "Epoch 296/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0572 - acc: 0.9551 - val_loss: 0.0566 - val_acc: 0.9471\n",
      "Epoch 297/500\n",
      "22272/22272 [==============================] - 12s 524us/step - loss: 0.0569 - acc: 0.9564 - val_loss: 0.0586 - val_acc: 0.9420\n",
      "Epoch 298/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0569 - acc: 0.9554 - val_loss: 0.0552 - val_acc: 0.9386\n",
      "Epoch 299/500\n",
      "22272/22272 [==============================] - 12s 538us/step - loss: 0.0566 - acc: 0.9560 - val_loss: 0.0571 - val_acc: 0.9488\n",
      "Epoch 300/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0572 - acc: 0.9569 - val_loss: 0.0561 - val_acc: 0.9420\n",
      "Epoch 301/500\n",
      "22272/22272 [==============================] - 12s 522us/step - loss: 0.0569 - acc: 0.9558 - val_loss: 0.0566 - val_acc: 0.9403\n",
      "Epoch 302/500\n",
      "22272/22272 [==============================] - 12s 522us/step - loss: 0.0563 - acc: 0.9563 - val_loss: 0.0562 - val_acc: 0.9403\n",
      "Epoch 303/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0566 - acc: 0.9558 - val_loss: 0.0547 - val_acc: 0.9488\n",
      "Epoch 304/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0563 - acc: 0.9546 - val_loss: 0.0566 - val_acc: 0.9429\n",
      "Epoch 305/500\n",
      "22272/22272 [==============================] - 12s 520us/step - loss: 0.0561 - acc: 0.9570 - val_loss: 0.0565 - val_acc: 0.9454\n",
      "Epoch 306/500\n",
      "22272/22272 [==============================] - 12s 542us/step - loss: 0.0564 - acc: 0.9567 - val_loss: 0.0567 - val_acc: 0.9506\n",
      "Epoch 307/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.0561 - acc: 0.9578 - val_loss: 0.0562 - val_acc: 0.9480\n",
      "Epoch 308/500\n",
      "22272/22272 [==============================] - 12s 537us/step - loss: 0.0566 - acc: 0.9559 - val_loss: 0.0542 - val_acc: 0.9463\n",
      "Epoch 309/500\n",
      "22272/22272 [==============================] - 13s 595us/step - loss: 0.0563 - acc: 0.9579 - val_loss: 0.0561 - val_acc: 0.9540\n",
      "Epoch 310/500\n",
      "22272/22272 [==============================] - 12s 524us/step - loss: 0.0554 - acc: 0.9585 - val_loss: 0.0566 - val_acc: 0.9446\n",
      "Epoch 311/500\n",
      "22272/22272 [==============================] - 12s 522us/step - loss: 0.0555 - acc: 0.9566 - val_loss: 0.0565 - val_acc: 0.9514\n",
      "Epoch 312/500\n",
      "22272/22272 [==============================] - 12s 536us/step - loss: 0.0559 - acc: 0.9566 - val_loss: 0.0530 - val_acc: 0.9523\n",
      "Epoch 313/500\n",
      "22272/22272 [==============================] - 14s 625us/step - loss: 0.0557 - acc: 0.9556 - val_loss: 0.0540 - val_acc: 0.9506\n",
      "Epoch 314/500\n",
      "22272/22272 [==============================] - 14s 638us/step - loss: 0.0555 - acc: 0.9591 - val_loss: 0.0553 - val_acc: 0.9471\n",
      "Epoch 315/500\n",
      "22272/22272 [==============================] - 14s 627us/step - loss: 0.0551 - acc: 0.9583 - val_loss: 0.0567 - val_acc: 0.9429\n",
      "Epoch 316/500\n",
      "22272/22272 [==============================] - 13s 595us/step - loss: 0.0553 - acc: 0.9574 - val_loss: 0.0550 - val_acc: 0.9514\n",
      "Epoch 317/500\n",
      "22272/22272 [==============================] - 12s 543us/step - loss: 0.0553 - acc: 0.9572 - val_loss: 0.0559 - val_acc: 0.9454\n",
      "Epoch 318/500\n",
      "22272/22272 [==============================] - 12s 526us/step - loss: 0.0555 - acc: 0.9567 - val_loss: 0.0544 - val_acc: 0.9420\n",
      "Epoch 319/500\n",
      "22272/22272 [==============================] - 12s 536us/step - loss: 0.0551 - acc: 0.9591 - val_loss: 0.0540 - val_acc: 0.9471\n",
      "Epoch 320/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0553 - acc: 0.9568 - val_loss: 0.0543 - val_acc: 0.9514\n",
      "Epoch 321/500\n",
      "22272/22272 [==============================] - 12s 547us/step - loss: 0.0547 - acc: 0.9587 - val_loss: 0.0559 - val_acc: 0.94541s - loss: 0.0546 - ac\n",
      "Epoch 322/500\n",
      "22272/22272 [==============================] - 12s 543us/step - loss: 0.0550 - acc: 0.9579 - val_loss: 0.0560 - val_acc: 0.9497\n",
      "Epoch 323/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0546 - acc: 0.9579 - val_loss: 0.0553 - val_acc: 0.9480\n",
      "Epoch 324/500\n",
      "22272/22272 [==============================] - 12s 561us/step - loss: 0.0545 - acc: 0.9585 - val_loss: 0.0538 - val_acc: 0.9471\n",
      "Epoch 325/500\n",
      "22272/22272 [==============================] - 13s 580us/step - loss: 0.0544 - acc: 0.9589 - val_loss: 0.0553 - val_acc: 0.9429\n",
      "Epoch 326/500\n",
      "22272/22272 [==============================] - 12s 556us/step - loss: 0.0546 - acc: 0.9590 - val_loss: 0.0533 - val_acc: 0.9471\n",
      "Epoch 327/500\n",
      "22272/22272 [==============================] - 13s 570us/step - loss: 0.0544 - acc: 0.9582 - val_loss: 0.0553 - val_acc: 0.9446\n",
      "Epoch 328/500\n",
      "22272/22272 [==============================] - 12s 542us/step - loss: 0.0540 - acc: 0.9595 - val_loss: 0.0532 - val_acc: 0.9446\n",
      "Epoch 329/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0543 - acc: 0.9595 - val_loss: 0.0550 - val_acc: 0.9488\n",
      "Epoch 330/500\n",
      "22272/22272 [==============================] - 12s 520us/step - loss: 0.0543 - acc: 0.9587 - val_loss: 0.0547 - val_acc: 0.9497\n",
      "Epoch 331/500\n",
      "22272/22272 [==============================] - 12s 525us/step - loss: 0.0541 - acc: 0.9592 - val_loss: 0.0549 - val_acc: 0.9531\n",
      "Epoch 332/500\n",
      "22272/22272 [==============================] - 12s 528us/step - loss: 0.0543 - acc: 0.9560 - val_loss: 0.0535 - val_acc: 0.9514\n",
      "Epoch 333/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0544 - acc: 0.9594 - val_loss: 0.0541 - val_acc: 0.9454\n",
      "Epoch 334/500\n",
      "22272/22272 [==============================] - 12s 528us/step - loss: 0.0535 - acc: 0.9575 - val_loss: 0.0532 - val_acc: 0.9497\n",
      "Epoch 335/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0538 - acc: 0.9613 - val_loss: 0.0543 - val_acc: 0.9480\n",
      "Epoch 336/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0540 - acc: 0.9612 - val_loss: 0.0545 - val_acc: 0.9480\n",
      "Epoch 337/500\n",
      "22272/22272 [==============================] - 12s 521us/step - loss: 0.0536 - acc: 0.9613 - val_loss: 0.0533 - val_acc: 0.9471\n",
      "Epoch 338/500\n",
      "22272/22272 [==============================] - 13s 572us/step - loss: 0.0538 - acc: 0.9599 - val_loss: 0.0533 - val_acc: 0.9437\n",
      "Epoch 339/500\n",
      "22272/22272 [==============================] - 13s 582us/step - loss: 0.0532 - acc: 0.9599 - val_loss: 0.0530 - val_acc: 0.9420\n",
      "Epoch 340/500\n",
      "22272/22272 [==============================] - 13s 582us/step - loss: 0.0530 - acc: 0.9605 - val_loss: 0.0540 - val_acc: 0.9420\n",
      "Epoch 341/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.0533 - acc: 0.9593 - val_loss: 0.0562 - val_acc: 0.9429\n",
      "Epoch 342/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0530 - acc: 0.9597 - val_loss: 0.0515 - val_acc: 0.9454\n",
      "Epoch 343/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0531 - acc: 0.9590 - val_loss: 0.0533 - val_acc: 0.9429\n",
      "Epoch 344/500\n",
      "22272/22272 [==============================] - 12s 537us/step - loss: 0.0533 - acc: 0.9586 - val_loss: 0.0528 - val_acc: 0.9429\n",
      "Epoch 345/500\n",
      "22272/22272 [==============================] - 13s 588us/step - loss: 0.0531 - acc: 0.9592 - val_loss: 0.0545 - val_acc: 0.9369\n",
      "Epoch 346/500\n",
      "22272/22272 [==============================] - 11s 499us/step - loss: 0.0535 - acc: 0.9608 - val_loss: 0.0525 - val_acc: 0.9446\n",
      "Epoch 347/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0529 - acc: 0.9588 - val_loss: 0.0523 - val_acc: 0.9497\n",
      "Epoch 348/500\n",
      "22272/22272 [==============================] - 13s 581us/step - loss: 0.0527 - acc: 0.9621 - val_loss: 0.0532 - val_acc: 0.9395\n",
      "Epoch 349/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0530 - acc: 0.9615 - val_loss: 0.0544 - val_acc: 0.9386\n",
      "Epoch 350/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0528 - acc: 0.9606 - val_loss: 0.0525 - val_acc: 0.9497\n",
      "Epoch 351/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0526 - acc: 0.9598 - val_loss: 0.0527 - val_acc: 0.9429\n",
      "Epoch 352/500\n",
      "22272/22272 [==============================] - 12s 529us/step - loss: 0.0529 - acc: 0.9600 - val_loss: 0.0537 - val_acc: 0.9463\n",
      "Epoch 353/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0527 - acc: 0.9611 - val_loss: 0.0511 - val_acc: 0.9454\n",
      "Epoch 354/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0527 - acc: 0.9596 - val_loss: 0.0544 - val_acc: 0.9429\n",
      "Epoch 355/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0525 - acc: 0.9626 - val_loss: 0.0513 - val_acc: 0.9454\n",
      "Epoch 356/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.0529 - acc: 0.9595 - val_loss: 0.0523 - val_acc: 0.9403\n",
      "Epoch 357/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0526 - acc: 0.9602 - val_loss: 0.0538 - val_acc: 0.9386\n",
      "Epoch 358/500\n",
      "22272/22272 [==============================] - 12s 531us/step - loss: 0.0519 - acc: 0.9616 - val_loss: 0.0515 - val_acc: 0.9378\n",
      "Epoch 359/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0524 - acc: 0.9608 - val_loss: 0.0519 - val_acc: 0.9420\n",
      "Epoch 360/500\n",
      "22272/22272 [==============================] - 12s 539us/step - loss: 0.0514 - acc: 0.9611 - val_loss: 0.0516 - val_acc: 0.9488\n",
      "Epoch 361/500\n",
      "22272/22272 [==============================] - 12s 540us/step - loss: 0.0525 - acc: 0.9605 - val_loss: 0.0536 - val_acc: 0.9429\n",
      "Epoch 362/500\n",
      "22272/22272 [==============================] - 12s 530us/step - loss: 0.0517 - acc: 0.9590 - val_loss: 0.0526 - val_acc: 0.9471\n",
      "Epoch 363/500\n",
      "22272/22272 [==============================] - 12s 521us/step - loss: 0.0520 - acc: 0.9603 - val_loss: 0.0522 - val_acc: 0.9471\n",
      "Epoch 364/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0524 - acc: 0.9609 - val_loss: 0.0527 - val_acc: 0.9471\n",
      "Epoch 365/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0519 - acc: 0.9619 - val_loss: 0.0504 - val_acc: 0.9497\n",
      "Epoch 366/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0517 - acc: 0.9630 - val_loss: 0.0515 - val_acc: 0.9446\n",
      "Epoch 367/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0514 - acc: 0.9622 - val_loss: 0.0511 - val_acc: 0.9480\n",
      "Epoch 368/500\n",
      "22272/22272 [==============================] - 11s 501us/step - loss: 0.0513 - acc: 0.9619 - val_loss: 0.0512 - val_acc: 0.9412\n",
      "Epoch 369/500\n",
      "22272/22272 [==============================] - 11s 497us/step - loss: 0.0518 - acc: 0.9604 - val_loss: 0.0517 - val_acc: 0.9463\n",
      "Epoch 370/500\n",
      "22272/22272 [==============================] - 11s 492us/step - loss: 0.0512 - acc: 0.9636 - val_loss: 0.0508 - val_acc: 0.9454\n",
      "Epoch 371/500\n",
      "22272/22272 [==============================] - 11s 494us/step - loss: 0.0516 - acc: 0.9608 - val_loss: 0.0521 - val_acc: 0.9488\n",
      "Epoch 372/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0514 - acc: 0.9630 - val_loss: 0.0513 - val_acc: 0.9523\n",
      "Epoch 373/500\n",
      "22272/22272 [==============================] - 11s 496us/step - loss: 0.0511 - acc: 0.9615 - val_loss: 0.0519 - val_acc: 0.9506\n",
      "Epoch 374/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.0512 - acc: 0.9621 - val_loss: 0.0506 - val_acc: 0.9506\n",
      "Epoch 375/500\n",
      "22272/22272 [==============================] - 11s 490us/step - loss: 0.0514 - acc: 0.9621 - val_loss: 0.0498 - val_acc: 0.9471\n",
      "Epoch 376/500\n",
      "22272/22272 [==============================] - 11s 504us/step - loss: 0.0512 - acc: 0.9611 - val_loss: 0.0500 - val_acc: 0.9480\n",
      "Epoch 377/500\n",
      "22272/22272 [==============================] - 17s 770us/step - loss: 0.0510 - acc: 0.9620 - val_loss: 0.0508 - val_acc: 0.9506\n",
      "Epoch 378/500\n",
      "22272/22272 [==============================] - 17s 778us/step - loss: 0.0511 - acc: 0.9627 - val_loss: 0.0498 - val_acc: 0.9471\n",
      "Epoch 379/500\n",
      "22272/22272 [==============================] - 18s 814us/step - loss: 0.0505 - acc: 0.9616 - val_loss: 0.0489 - val_acc: 0.9497\n",
      "Epoch 380/500\n",
      "22272/22272 [==============================] - 17s 777us/step - loss: 0.0507 - acc: 0.9622 - val_loss: 0.0499 - val_acc: 0.9480\n",
      "Epoch 381/500\n",
      "22272/22272 [==============================] - 16s 738us/step - loss: 0.0509 - acc: 0.9628 - val_loss: 0.0500 - val_acc: 0.9454\n",
      "Epoch 382/500\n",
      "22272/22272 [==============================] - 16s 724us/step - loss: 0.0504 - acc: 0.9640 - val_loss: 0.0501 - val_acc: 0.9497\n",
      "Epoch 383/500\n",
      "22272/22272 [==============================] - 16s 717us/step - loss: 0.0508 - acc: 0.9622 - val_loss: 0.0535 - val_acc: 0.9471\n",
      "Epoch 384/500\n",
      "22272/22272 [==============================] - 16s 723us/step - loss: 0.0505 - acc: 0.9628 - val_loss: 0.0500 - val_acc: 0.9497\n",
      "Epoch 385/500\n",
      "22272/22272 [==============================] - 16s 730us/step - loss: 0.0509 - acc: 0.9619 - val_loss: 0.0540 - val_acc: 0.9412\n",
      "Epoch 386/500\n",
      "22272/22272 [==============================] - 17s 753us/step - loss: 0.0505 - acc: 0.9639 - val_loss: 0.0504 - val_acc: 0.9514\n",
      "Epoch 387/500\n",
      "22272/22272 [==============================] - 16s 721us/step - loss: 0.0504 - acc: 0.9625 - val_loss: 0.0493 - val_acc: 0.9488\n",
      "Epoch 388/500\n",
      "22272/22272 [==============================] - 15s 694us/step - loss: 0.0508 - acc: 0.9628 - val_loss: 0.0512 - val_acc: 0.9471\n",
      "Epoch 389/500\n",
      "22272/22272 [==============================] - 16s 726us/step - loss: 0.0503 - acc: 0.9626 - val_loss: 0.0498 - val_acc: 0.9514\n",
      "Epoch 390/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.0500 - acc: 0.9640 - val_loss: 0.0495 - val_acc: 0.9480\n",
      "Epoch 391/500\n",
      "22272/22272 [==============================] - 17s 742us/step - loss: 0.0504 - acc: 0.9641 - val_loss: 0.0501 - val_acc: 0.9497\n",
      "Epoch 392/500\n",
      "22272/22272 [==============================] - 16s 729us/step - loss: 0.0499 - acc: 0.9626 - val_loss: 0.0503 - val_acc: 0.9480\n",
      "Epoch 393/500\n",
      "22272/22272 [==============================] - 16s 732us/step - loss: 0.0505 - acc: 0.9631 - val_loss: 0.0503 - val_acc: 0.9446\n",
      "Epoch 394/500\n",
      "22272/22272 [==============================] - 17s 760us/step - loss: 0.0502 - acc: 0.9635 - val_loss: 0.0497 - val_acc: 0.9497\n",
      "Epoch 395/500\n",
      "22272/22272 [==============================] - 16s 734us/step - loss: 0.0501 - acc: 0.9637 - val_loss: 0.0496 - val_acc: 0.9506\n",
      "Epoch 396/500\n",
      "22272/22272 [==============================] - 16s 724us/step - loss: 0.0499 - acc: 0.9645 - val_loss: 0.0493 - val_acc: 0.9531\n",
      "Epoch 397/500\n",
      "22272/22272 [==============================] - 16s 718us/step - loss: 0.0502 - acc: 0.9625 - val_loss: 0.0515 - val_acc: 0.9531\n",
      "Epoch 398/500\n",
      "22272/22272 [==============================] - 16s 726us/step - loss: 0.0497 - acc: 0.9624 - val_loss: 0.0493 - val_acc: 0.9582\n",
      "Epoch 399/500\n",
      "22272/22272 [==============================] - 16s 722us/step - loss: 0.0500 - acc: 0.9633 - val_loss: 0.0503 - val_acc: 0.9488\n",
      "Epoch 400/500\n",
      "22272/22272 [==============================] - 16s 735us/step - loss: 0.0497 - acc: 0.9630 - val_loss: 0.0505 - val_acc: 0.9514\n",
      "Epoch 401/500\n",
      "22272/22272 [==============================] - 16s 726us/step - loss: 0.0498 - acc: 0.9635 - val_loss: 0.0497 - val_acc: 0.9497\n",
      "Epoch 402/500\n",
      "22272/22272 [==============================] - 16s 716us/step - loss: 0.0494 - acc: 0.9626 - val_loss: 0.0492 - val_acc: 0.9548\n",
      "Epoch 403/500\n",
      "22272/22272 [==============================] - 16s 725us/step - loss: 0.0499 - acc: 0.9626 - val_loss: 0.0500 - val_acc: 0.9548\n",
      "Epoch 404/500\n",
      "22272/22272 [==============================] - 16s 727us/step - loss: 0.0496 - acc: 0.9633 - val_loss: 0.0504 - val_acc: 0.9523\n",
      "Epoch 405/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0492 - acc: 0.9654 - val_loss: 0.0501 - val_acc: 0.9514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0492 - acc: 0.9643 - val_loss: 0.0491 - val_acc: 0.9497\n",
      "Epoch 407/500\n",
      "22272/22272 [==============================] - 16s 726us/step - loss: 0.0493 - acc: 0.9624 - val_loss: 0.0480 - val_acc: 0.9523\n",
      "Epoch 408/500\n",
      "22272/22272 [==============================] - 16s 700us/step - loss: 0.0493 - acc: 0.9639 - val_loss: 0.0480 - val_acc: 0.9540\n",
      "Epoch 409/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0492 - acc: 0.9641 - val_loss: 0.0489 - val_acc: 0.9523\n",
      "Epoch 410/500\n",
      "22272/22272 [==============================] - 17s 741us/step - loss: 0.0492 - acc: 0.9643 - val_loss: 0.0495 - val_acc: 0.9480\n",
      "Epoch 411/500\n",
      "22272/22272 [==============================] - 16s 726us/step - loss: 0.0492 - acc: 0.9640 - val_loss: 0.0497 - val_acc: 0.9540\n",
      "Epoch 412/500\n",
      "22272/22272 [==============================] - 16s 716us/step - loss: 0.0490 - acc: 0.9639 - val_loss: 0.0489 - val_acc: 0.9514\n",
      "Epoch 413/500\n",
      "22272/22272 [==============================] - 15s 690us/step - loss: 0.0490 - acc: 0.9651 - val_loss: 0.0476 - val_acc: 0.9540\n",
      "Epoch 414/500\n",
      "22272/22272 [==============================] - 15s 684us/step - loss: 0.0485 - acc: 0.9653 - val_loss: 0.0490 - val_acc: 0.9540\n",
      "Epoch 415/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0490 - acc: 0.9632 - val_loss: 0.0487 - val_acc: 0.9506\n",
      "Epoch 416/500\n",
      "22272/22272 [==============================] - 16s 725us/step - loss: 0.0492 - acc: 0.9633 - val_loss: 0.0485 - val_acc: 0.9514\n",
      "Epoch 417/500\n",
      "22272/22272 [==============================] - 16s 724us/step - loss: 0.0494 - acc: 0.9636 - val_loss: 0.0488 - val_acc: 0.9514\n",
      "Epoch 418/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.0483 - acc: 0.9642 - val_loss: 0.0481 - val_acc: 0.9540\n",
      "Epoch 419/500\n",
      "22272/22272 [==============================] - 16s 726us/step - loss: 0.0489 - acc: 0.9633 - val_loss: 0.0490 - val_acc: 0.9497\n",
      "Epoch 420/500\n",
      "22272/22272 [==============================] - 16s 720us/step - loss: 0.0485 - acc: 0.9647 - val_loss: 0.0502 - val_acc: 0.9531\n",
      "Epoch 421/500\n",
      "22272/22272 [==============================] - 16s 726us/step - loss: 0.0485 - acc: 0.9651 - val_loss: 0.0499 - val_acc: 0.9471\n",
      "Epoch 422/500\n",
      "22272/22272 [==============================] - 16s 734us/step - loss: 0.0488 - acc: 0.9650 - val_loss: 0.0483 - val_acc: 0.9531\n",
      "Epoch 423/500\n",
      "22272/22272 [==============================] - 16s 724us/step - loss: 0.0482 - acc: 0.9636 - val_loss: 0.0495 - val_acc: 0.9531\n",
      "Epoch 424/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.0480 - acc: 0.9662 - val_loss: 0.0483 - val_acc: 0.9506\n",
      "Epoch 425/500\n",
      "22272/22272 [==============================] - 17s 745us/step - loss: 0.0486 - acc: 0.9639 - val_loss: 0.0512 - val_acc: 0.9429\n",
      "Epoch 426/500\n",
      "22272/22272 [==============================] - 16s 729us/step - loss: 0.0485 - acc: 0.9644 - val_loss: 0.0483 - val_acc: 0.9506\n",
      "Epoch 427/500\n",
      "22272/22272 [==============================] - 17s 764us/step - loss: 0.0483 - acc: 0.9649 - val_loss: 0.0466 - val_acc: 0.9557\n",
      "Epoch 428/500\n",
      "22272/22272 [==============================] - 16s 728us/step - loss: 0.0484 - acc: 0.9652 - val_loss: 0.0474 - val_acc: 0.9531\n",
      "Epoch 429/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.0482 - acc: 0.9646 - val_loss: 0.0467 - val_acc: 0.9471\n",
      "Epoch 430/500\n",
      "22272/22272 [==============================] - 16s 720us/step - loss: 0.0483 - acc: 0.9651 - val_loss: 0.0479 - val_acc: 0.9523\n",
      "Epoch 431/500\n",
      "22272/22272 [==============================] - 16s 714us/step - loss: 0.0483 - acc: 0.9642 - val_loss: 0.0476 - val_acc: 0.9497\n",
      "Epoch 432/500\n",
      "22272/22272 [==============================] - 16s 720us/step - loss: 0.0478 - acc: 0.9657 - val_loss: 0.0493 - val_acc: 0.9514\n",
      "Epoch 433/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.0486 - acc: 0.9641 - val_loss: 0.0469 - val_acc: 0.9565\n",
      "Epoch 434/500\n",
      "22272/22272 [==============================] - 16s 719us/step - loss: 0.0478 - acc: 0.9666 - val_loss: 0.0500 - val_acc: 0.9531\n",
      "Epoch 435/500\n",
      "22272/22272 [==============================] - 16s 723us/step - loss: 0.0480 - acc: 0.9648 - val_loss: 0.0471 - val_acc: 0.9582\n",
      "Epoch 436/500\n",
      "22272/22272 [==============================] - 16s 719us/step - loss: 0.0476 - acc: 0.9660 - val_loss: 0.0502 - val_acc: 0.9497\n",
      "Epoch 437/500\n",
      "22272/22272 [==============================] - 16s 733us/step - loss: 0.0477 - acc: 0.9659 - val_loss: 0.0474 - val_acc: 0.9488\n",
      "Epoch 438/500\n",
      "22272/22272 [==============================] - 17s 744us/step - loss: 0.0482 - acc: 0.9649 - val_loss: 0.0478 - val_acc: 0.9497\n",
      "Epoch 439/500\n",
      "22272/22272 [==============================] - 16s 733us/step - loss: 0.0476 - acc: 0.9661 - val_loss: 0.0480 - val_acc: 0.9557\n",
      "Epoch 440/500\n",
      "22272/22272 [==============================] - 16s 724us/step - loss: 0.0479 - acc: 0.9668 - val_loss: 0.0495 - val_acc: 0.9471\n",
      "Epoch 441/500\n",
      "22272/22272 [==============================] - 16s 729us/step - loss: 0.0475 - acc: 0.9665 - val_loss: 0.0469 - val_acc: 0.9506\n",
      "Epoch 442/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.0475 - acc: 0.9665 - val_loss: 0.0477 - val_acc: 0.9514\n",
      "Epoch 443/500\n",
      "22272/22272 [==============================] - 16s 723us/step - loss: 0.0480 - acc: 0.9651 - val_loss: 0.0482 - val_acc: 0.9523\n",
      "Epoch 444/500\n",
      "22272/22272 [==============================] - 16s 716us/step - loss: 0.0480 - acc: 0.9648 - val_loss: 0.0471 - val_acc: 0.9488\n",
      "Epoch 445/500\n",
      "22272/22272 [==============================] - 15s 696us/step - loss: 0.0474 - acc: 0.9659 - val_loss: 0.0482 - val_acc: 0.9471\n",
      "Epoch 446/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.0475 - acc: 0.9665 - val_loss: 0.0484 - val_acc: 0.9523\n",
      "Epoch 447/500\n",
      "22272/22272 [==============================] - 16s 730us/step - loss: 0.0474 - acc: 0.9662 - val_loss: 0.0477 - val_acc: 0.9531\n",
      "Epoch 448/500\n",
      "22272/22272 [==============================] - 16s 732us/step - loss: 0.0475 - acc: 0.9651 - val_loss: 0.0475 - val_acc: 0.9480\n",
      "Epoch 449/500\n",
      "22272/22272 [==============================] - 16s 730us/step - loss: 0.0472 - acc: 0.9659 - val_loss: 0.0482 - val_acc: 0.9548\n",
      "Epoch 450/500\n",
      "22272/22272 [==============================] - 17s 752us/step - loss: 0.0479 - acc: 0.9650 - val_loss: 0.0465 - val_acc: 0.9531\n",
      "Epoch 451/500\n",
      "22272/22272 [==============================] - 16s 720us/step - loss: 0.0473 - acc: 0.9654 - val_loss: 0.0488 - val_acc: 0.9514\n",
      "Epoch 452/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.0477 - acc: 0.9649 - val_loss: 0.0497 - val_acc: 0.9488\n",
      "Epoch 453/500\n",
      "22272/22272 [==============================] - 16s 698us/step - loss: 0.0473 - acc: 0.9656 - val_loss: 0.0464 - val_acc: 0.9531\n",
      "Epoch 454/500\n",
      "22272/22272 [==============================] - 16s 702us/step - loss: 0.0478 - acc: 0.9651 - val_loss: 0.0471 - val_acc: 0.9565\n",
      "Epoch 455/500\n",
      "22272/22272 [==============================] - 16s 717us/step - loss: 0.0473 - acc: 0.9661 - val_loss: 0.0462 - val_acc: 0.9548\n",
      "Epoch 456/500\n",
      "22272/22272 [==============================] - 16s 714us/step - loss: 0.0469 - acc: 0.9670 - val_loss: 0.0484 - val_acc: 0.9531\n",
      "Epoch 457/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0470 - acc: 0.9671 - val_loss: 0.0479 - val_acc: 0.9523\n",
      "Epoch 458/500\n",
      "22272/22272 [==============================] - 16s 714us/step - loss: 0.0468 - acc: 0.9669 - val_loss: 0.0486 - val_acc: 0.9531\n",
      "Epoch 459/500\n",
      "22272/22272 [==============================] - 16s 721us/step - loss: 0.0467 - acc: 0.9671 - val_loss: 0.0464 - val_acc: 0.9548\n",
      "Epoch 460/500\n",
      "22272/22272 [==============================] - 16s 707us/step - loss: 0.0468 - acc: 0.9669 - val_loss: 0.0469 - val_acc: 0.9582\n",
      "Epoch 461/500\n",
      "22272/22272 [==============================] - 16s 704us/step - loss: 0.0468 - acc: 0.9665 - val_loss: 0.0482 - val_acc: 0.9514\n",
      "Epoch 462/500\n",
      "22272/22272 [==============================] - 16s 714us/step - loss: 0.0469 - acc: 0.9664 - val_loss: 0.0459 - val_acc: 0.9565\n",
      "Epoch 463/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.0465 - acc: 0.9673 - val_loss: 0.0475 - val_acc: 0.9488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 464/500\n",
      "22272/22272 [==============================] - 15s 693us/step - loss: 0.0463 - acc: 0.9675 - val_loss: 0.0481 - val_acc: 0.9506\n",
      "Epoch 465/500\n",
      "22272/22272 [==============================] - 16s 697us/step - loss: 0.0470 - acc: 0.9646 - val_loss: 0.0481 - val_acc: 0.9531\n",
      "Epoch 466/500\n",
      "22272/22272 [==============================] - 16s 708us/step - loss: 0.0467 - acc: 0.9671 - val_loss: 0.0463 - val_acc: 0.9599\n",
      "Epoch 467/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.0463 - acc: 0.9682 - val_loss: 0.0474 - val_acc: 0.9591\n",
      "Epoch 468/500\n",
      "22272/22272 [==============================] - 17s 745us/step - loss: 0.0466 - acc: 0.9680 - val_loss: 0.0471 - val_acc: 0.9574\n",
      "Epoch 469/500\n",
      "22272/22272 [==============================] - 16s 716us/step - loss: 0.0469 - acc: 0.9653 - val_loss: 0.0475 - val_acc: 0.9540\n",
      "Epoch 470/500\n",
      "22272/22272 [==============================] - 15s 691us/step - loss: 0.0459 - acc: 0.9656 - val_loss: 0.0477 - val_acc: 0.9480\n",
      "Epoch 471/500\n",
      "22272/22272 [==============================] - 16s 710us/step - loss: 0.0465 - acc: 0.9676 - val_loss: 0.0473 - val_acc: 0.9531\n",
      "Epoch 472/500\n",
      "22272/22272 [==============================] - 15s 681us/step - loss: 0.0464 - acc: 0.9675 - val_loss: 0.0463 - val_acc: 0.9565\n",
      "Epoch 473/500\n",
      "22272/22272 [==============================] - 16s 723us/step - loss: 0.0467 - acc: 0.9674 - val_loss: 0.0470 - val_acc: 0.9531\n",
      "Epoch 474/500\n",
      "22272/22272 [==============================] - 16s 706us/step - loss: 0.0460 - acc: 0.9657 - val_loss: 0.0465 - val_acc: 0.9565\n",
      "Epoch 475/500\n",
      "22272/22272 [==============================] - 14s 625us/step - loss: 0.0463 - acc: 0.9659 - val_loss: 0.0459 - val_acc: 0.9531\n",
      "Epoch 476/500\n",
      "22272/22272 [==============================] - 13s 577us/step - loss: 0.0462 - acc: 0.9688 - val_loss: 0.0474 - val_acc: 0.9574\n",
      "Epoch 477/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0462 - acc: 0.9679 - val_loss: 0.0461 - val_acc: 0.9574\n",
      "Epoch 478/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0461 - acc: 0.9686 - val_loss: 0.0458 - val_acc: 0.9523\n",
      "Epoch 479/500\n",
      "22272/22272 [==============================] - 12s 527us/step - loss: 0.0465 - acc: 0.9653 - val_loss: 0.0465 - val_acc: 0.9514\n",
      "Epoch 480/500\n",
      "22272/22272 [==============================] - 13s 602us/step - loss: 0.0456 - acc: 0.9687 - val_loss: 0.0463 - val_acc: 0.9514\n",
      "Epoch 481/500\n",
      "22272/22272 [==============================] - 12s 535us/step - loss: 0.0462 - acc: 0.9662 - val_loss: 0.0445 - val_acc: 0.9540\n",
      "Epoch 482/500\n",
      "22272/22272 [==============================] - 14s 645us/step - loss: 0.0462 - acc: 0.9682 - val_loss: 0.0460 - val_acc: 0.9540\n",
      "Epoch 483/500\n",
      "22272/22272 [==============================] - 12s 552us/step - loss: 0.0459 - acc: 0.9687 - val_loss: 0.0448 - val_acc: 0.9574\n",
      "Epoch 484/500\n",
      "22272/22272 [==============================] - 12s 541us/step - loss: 0.0459 - acc: 0.9677 - val_loss: 0.0464 - val_acc: 0.9574\n",
      "Epoch 485/500\n",
      "22272/22272 [==============================] - 13s 565us/step - loss: 0.0458 - acc: 0.9682 - val_loss: 0.0455 - val_acc: 0.9548\n",
      "Epoch 486/500\n",
      "22272/22272 [==============================] - 11s 493us/step - loss: 0.0459 - acc: 0.9671 - val_loss: 0.0455 - val_acc: 0.9582\n",
      "Epoch 487/500\n",
      "22272/22272 [==============================] - 11s 494us/step - loss: 0.0459 - acc: 0.9669 - val_loss: 0.0452 - val_acc: 0.9540\n",
      "Epoch 488/500\n",
      "22272/22272 [==============================] - 11s 503us/step - loss: 0.0460 - acc: 0.9666 - val_loss: 0.0465 - val_acc: 0.9523\n",
      "Epoch 489/500\n",
      "22272/22272 [==============================] - 11s 495us/step - loss: 0.0460 - acc: 0.9673 - val_loss: 0.0446 - val_acc: 0.9599\n",
      "Epoch 490/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0458 - acc: 0.9660 - val_loss: 0.0449 - val_acc: 0.9557\n",
      "Epoch 491/500\n",
      "22272/22272 [==============================] - 12s 535us/step - loss: 0.0458 - acc: 0.9683 - val_loss: 0.0447 - val_acc: 0.9565\n",
      "Epoch 492/500\n",
      "22272/22272 [==============================] - 13s 596us/step - loss: 0.0458 - acc: 0.9668 - val_loss: 0.0461 - val_acc: 0.9548\n",
      "Epoch 493/500\n",
      "22272/22272 [==============================] - 14s 636us/step - loss: 0.0457 - acc: 0.9666 - val_loss: 0.0446 - val_acc: 0.9582\n",
      "Epoch 494/500\n",
      "22272/22272 [==============================] - 13s 579us/step - loss: 0.0455 - acc: 0.9662 - val_loss: 0.0455 - val_acc: 0.9608\n",
      "Epoch 495/500\n",
      "22272/22272 [==============================] - 12s 548us/step - loss: 0.0451 - acc: 0.9696 - val_loss: 0.0447 - val_acc: 0.9565\n",
      "Epoch 496/500\n",
      "22272/22272 [==============================] - 12s 558us/step - loss: 0.0454 - acc: 0.9680 - val_loss: 0.0454 - val_acc: 0.9531\n",
      "Epoch 497/500\n",
      "22272/22272 [==============================] - 12s 526us/step - loss: 0.0453 - acc: 0.9686 - val_loss: 0.0447 - val_acc: 0.9557\n",
      "Epoch 498/500\n",
      "22272/22272 [==============================] - 12s 520us/step - loss: 0.0455 - acc: 0.9678 - val_loss: 0.0439 - val_acc: 0.9540\n",
      "Epoch 499/500\n",
      "22272/22272 [==============================] - 12s 533us/step - loss: 0.0455 - acc: 0.9659 - val_loss: 0.0448 - val_acc: 0.9557\n",
      "Epoch 500/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0452 - acc: 0.9681 - val_loss: 0.0451 - val_acc: 0.9531\n"
     ]
    }
   ],
   "source": [
    "##Fitting the GRU to the Training set\n",
    "hist5=model5.fit(X_train, y_train, batch_size=300, epochs=500, validation_split=0.05, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22272 samples, validate on 1173 samples\n",
      "Epoch 1/500\n",
      "22272/22272 [==============================] - 17s 747us/step - loss: 0.2305 - acc: 0.7509 - val_loss: 0.2229 - val_acc: 0.7298\n",
      "Epoch 2/500\n",
      "22272/22272 [==============================] - 12s 561us/step - loss: 0.2060 - acc: 0.7458 - val_loss: 0.2075 - val_acc: 0.7263\n",
      "Epoch 3/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.1998 - acc: 0.7513 - val_loss: 0.2035 - val_acc: 0.7298\n",
      "Epoch 4/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.1983 - acc: 0.7527 - val_loss: 0.2034 - val_acc: 0.7272\n",
      "Epoch 5/500\n",
      "22272/22272 [==============================] - 12s 530us/step - loss: 0.1980 - acc: 0.7519 - val_loss: 0.2115 - val_acc: 0.7451\n",
      "Epoch 6/500\n",
      "22272/22272 [==============================] - 12s 539us/step - loss: 0.1967 - acc: 0.7555 - val_loss: 0.2020 - val_acc: 0.7349\n",
      "Epoch 7/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.1960 - acc: 0.7544 - val_loss: 0.2000 - val_acc: 0.7323\n",
      "Epoch 8/500\n",
      "22272/22272 [==============================] - 12s 522us/step - loss: 0.1937 - acc: 0.7544 - val_loss: 0.1993 - val_acc: 0.7366\n",
      "Epoch 9/500\n",
      "22272/22272 [==============================] - 12s 532us/step - loss: 0.1924 - acc: 0.7548 - val_loss: 0.2048 - val_acc: 0.7485\n",
      "Epoch 10/500\n",
      "22272/22272 [==============================] - 12s 550us/step - loss: 0.1896 - acc: 0.7553 - val_loss: 0.1936 - val_acc: 0.7460\n",
      "Epoch 11/500\n",
      "22272/22272 [==============================] - 12s 549us/step - loss: 0.1872 - acc: 0.7528 - val_loss: 0.1948 - val_acc: 0.7408\n",
      "Epoch 12/500\n",
      "22272/22272 [==============================] - 12s 548us/step - loss: 0.1866 - acc: 0.7549 - val_loss: 0.1903 - val_acc: 0.7408\n",
      "Epoch 13/500\n",
      "22272/22272 [==============================] - 12s 525us/step - loss: 0.1845 - acc: 0.7566 - val_loss: 0.1890 - val_acc: 0.7451\n",
      "Epoch 14/500\n",
      "22272/22272 [==============================] - 12s 553us/step - loss: 0.1814 - acc: 0.7632 - val_loss: 0.1848 - val_acc: 0.7477\n",
      "Epoch 15/500\n",
      "22272/22272 [==============================] - 12s 533us/step - loss: 0.1773 - acc: 0.7675 - val_loss: 0.1832 - val_acc: 0.7570\n",
      "Epoch 16/500\n",
      "22272/22272 [==============================] - 12s 529us/step - loss: 0.1746 - acc: 0.7733 - val_loss: 0.1789 - val_acc: 0.7545\n",
      "Epoch 17/500\n",
      "22272/22272 [==============================] - 12s 541us/step - loss: 0.1730 - acc: 0.7766 - val_loss: 0.1776 - val_acc: 0.7604\n",
      "Epoch 18/500\n",
      "22272/22272 [==============================] - 12s 555us/step - loss: 0.1705 - acc: 0.7839 - val_loss: 0.1777 - val_acc: 0.7732\n",
      "Epoch 19/500\n",
      "22272/22272 [==============================] - 12s 554us/step - loss: 0.1680 - acc: 0.7852 - val_loss: 0.1790 - val_acc: 0.7587\n",
      "Epoch 20/500\n",
      "22272/22272 [==============================] - 13s 561us/step - loss: 0.1674 - acc: 0.7876 - val_loss: 0.1713 - val_acc: 0.7783\n",
      "Epoch 21/500\n",
      "22272/22272 [==============================] - 12s 554us/step - loss: 0.1664 - acc: 0.7918 - val_loss: 0.1715 - val_acc: 0.7741\n",
      "Epoch 22/500\n",
      "22272/22272 [==============================] - 12s 555us/step - loss: 0.1648 - acc: 0.7947 - val_loss: 0.1749 - val_acc: 0.7715\n",
      "Epoch 23/500\n",
      "22272/22272 [==============================] - 12s 533us/step - loss: 0.1639 - acc: 0.7956 - val_loss: 0.1712 - val_acc: 0.7732\n",
      "Epoch 24/500\n",
      "22272/22272 [==============================] - 12s 548us/step - loss: 0.1643 - acc: 0.7962 - val_loss: 0.1693 - val_acc: 0.7741\n",
      "Epoch 25/500\n",
      "22272/22272 [==============================] - 13s 574us/step - loss: 0.1620 - acc: 0.8001 - val_loss: 0.1730 - val_acc: 0.7749\n",
      "Epoch 26/500\n",
      "22272/22272 [==============================] - 13s 594us/step - loss: 0.1613 - acc: 0.7993 - val_loss: 0.1723 - val_acc: 0.7766\n",
      "Epoch 27/500\n",
      "22272/22272 [==============================] - 12s 561us/step - loss: 0.1606 - acc: 0.8007 - val_loss: 0.1694 - val_acc: 0.7732\n",
      "Epoch 28/500\n",
      "22272/22272 [==============================] - 12s 541us/step - loss: 0.1588 - acc: 0.8046 - val_loss: 0.1679 - val_acc: 0.7758\n",
      "Epoch 29/500\n",
      "22272/22272 [==============================] - 12s 545us/step - loss: 0.1586 - acc: 0.8058 - val_loss: 0.1671 - val_acc: 0.7809\n",
      "Epoch 30/500\n",
      "22272/22272 [==============================] - 12s 533us/step - loss: 0.1574 - acc: 0.8050 - val_loss: 0.1617 - val_acc: 0.7903\n",
      "Epoch 31/500\n",
      "22272/22272 [==============================] - 12s 549us/step - loss: 0.1560 - acc: 0.8103 - val_loss: 0.1695 - val_acc: 0.7758\n",
      "Epoch 32/500\n",
      "22272/22272 [==============================] - 12s 529us/step - loss: 0.1556 - acc: 0.8110 - val_loss: 0.1674 - val_acc: 0.7792\n",
      "Epoch 33/500\n",
      "22272/22272 [==============================] - 12s 558us/step - loss: 0.1550 - acc: 0.8097 - val_loss: 0.1620 - val_acc: 0.7903\n",
      "Epoch 34/500\n",
      "22272/22272 [==============================] - 12s 555us/step - loss: 0.1532 - acc: 0.8153 - val_loss: 0.1581 - val_acc: 0.7971\n",
      "Epoch 35/500\n",
      "22272/22272 [==============================] - 12s 541us/step - loss: 0.1533 - acc: 0.8136 - val_loss: 0.1570 - val_acc: 0.7903\n",
      "Epoch 36/500\n",
      "22272/22272 [==============================] - 13s 577us/step - loss: 0.1518 - acc: 0.8140 - val_loss: 0.1627 - val_acc: 0.7886\n",
      "Epoch 37/500\n",
      "22272/22272 [==============================] - 13s 566us/step - loss: 0.1502 - acc: 0.8165 - val_loss: 0.1574 - val_acc: 0.7997\n",
      "Epoch 38/500\n",
      "22272/22272 [==============================] - 13s 566us/step - loss: 0.1493 - acc: 0.8188 - val_loss: 0.1529 - val_acc: 0.8048\n",
      "Epoch 39/500\n",
      "22272/22272 [==============================] - 12s 556us/step - loss: 0.1487 - acc: 0.8190 - val_loss: 0.1506 - val_acc: 0.8124\n",
      "Epoch 40/500\n",
      "22272/22272 [==============================] - 12s 554us/step - loss: 0.1475 - acc: 0.8201 - val_loss: 0.1509 - val_acc: 0.8073\n",
      "Epoch 41/500\n",
      "22272/22272 [==============================] - 12s 542us/step - loss: 0.1467 - acc: 0.8241 - val_loss: 0.1499 - val_acc: 0.8176\n",
      "Epoch 42/500\n",
      "22272/22272 [==============================] - 12s 553us/step - loss: 0.1455 - acc: 0.8239 - val_loss: 0.1478 - val_acc: 0.8150\n",
      "Epoch 43/500\n",
      "22272/22272 [==============================] - 12s 539us/step - loss: 0.1448 - acc: 0.8264 - val_loss: 0.1470 - val_acc: 0.8184\n",
      "Epoch 44/500\n",
      "22272/22272 [==============================] - 13s 569us/step - loss: 0.1444 - acc: 0.8254 - val_loss: 0.1446 - val_acc: 0.8150\n",
      "Epoch 45/500\n",
      "22272/22272 [==============================] - 13s 592us/step - loss: 0.1435 - acc: 0.8265 - val_loss: 0.1480 - val_acc: 0.8184\n",
      "Epoch 46/500\n",
      "22272/22272 [==============================] - 13s 567us/step - loss: 0.1422 - acc: 0.8272 - val_loss: 0.1451 - val_acc: 0.8150\n",
      "Epoch 47/500\n",
      "22272/22272 [==============================] - 12s 548us/step - loss: 0.1420 - acc: 0.8274 - val_loss: 0.1441 - val_acc: 0.8244\n",
      "Epoch 48/500\n",
      "22272/22272 [==============================] - 13s 581us/step - loss: 0.1402 - acc: 0.8307 - val_loss: 0.1431 - val_acc: 0.8303\n",
      "Epoch 49/500\n",
      "22272/22272 [==============================] - 14s 614us/step - loss: 0.1399 - acc: 0.8327 - val_loss: 0.1427 - val_acc: 0.8142\n",
      "Epoch 50/500\n",
      "22272/22272 [==============================] - 14s 626us/step - loss: 0.1396 - acc: 0.8332 - val_loss: 0.1429 - val_acc: 0.8269\n",
      "Epoch 51/500\n",
      "22272/22272 [==============================] - 13s 603us/step - loss: 0.1386 - acc: 0.8340 - val_loss: 0.1391 - val_acc: 0.8261\n",
      "Epoch 52/500\n",
      "22272/22272 [==============================] - 13s 576us/step - loss: 0.1371 - acc: 0.8370 - val_loss: 0.1419 - val_acc: 0.8252\n",
      "Epoch 53/500\n",
      "22272/22272 [==============================] - 13s 566us/step - loss: 0.1365 - acc: 0.8392 - val_loss: 0.1394 - val_acc: 0.8286\n",
      "Epoch 54/500\n",
      "22272/22272 [==============================] - 14s 642us/step - loss: 0.1358 - acc: 0.8396 - val_loss: 0.1428 - val_acc: 0.8261\n",
      "Epoch 55/500\n",
      "22272/22272 [==============================] - 13s 602us/step - loss: 0.1348 - acc: 0.8405 - val_loss: 0.1439 - val_acc: 0.8227\n",
      "Epoch 56/500\n",
      "22272/22272 [==============================] - 14s 624us/step - loss: 0.1345 - acc: 0.8441 - val_loss: 0.1397 - val_acc: 0.8269\n",
      "Epoch 57/500\n",
      "22272/22272 [==============================] - 12s 551us/step - loss: 0.1325 - acc: 0.8464 - val_loss: 0.1329 - val_acc: 0.8431\n",
      "Epoch 58/500\n",
      "22272/22272 [==============================] - 13s 584us/step - loss: 0.1324 - acc: 0.8479 - val_loss: 0.1363 - val_acc: 0.8321\n",
      "Epoch 59/500\n",
      "22272/22272 [==============================] - 14s 615us/step - loss: 0.1316 - acc: 0.8464 - val_loss: 0.1335 - val_acc: 0.8380\n",
      "Epoch 60/500\n",
      "22272/22272 [==============================] - 12s 557us/step - loss: 0.1314 - acc: 0.8489 - val_loss: 0.1339 - val_acc: 0.8303\n",
      "Epoch 61/500\n",
      "22272/22272 [==============================] - 12s 557us/step - loss: 0.1303 - acc: 0.8476 - val_loss: 0.1291 - val_acc: 0.8355\n",
      "Epoch 62/500\n",
      "22272/22272 [==============================] - 12s 550us/step - loss: 0.1288 - acc: 0.8535 - val_loss: 0.1320 - val_acc: 0.8414\n",
      "Epoch 63/500\n",
      "22272/22272 [==============================] - 12s 545us/step - loss: 0.1275 - acc: 0.8545 - val_loss: 0.1270 - val_acc: 0.8440\n",
      "Epoch 64/500\n",
      "22272/22272 [==============================] - 12s 534us/step - loss: 0.1265 - acc: 0.8570 - val_loss: 0.1347 - val_acc: 0.8372\n",
      "Epoch 65/500\n",
      "22272/22272 [==============================] - 12s 541us/step - loss: 0.1263 - acc: 0.8575 - val_loss: 0.1305 - val_acc: 0.8448\n",
      "Epoch 66/500\n",
      "22272/22272 [==============================] - 13s 604us/step - loss: 0.1251 - acc: 0.8569 - val_loss: 0.1241 - val_acc: 0.8491\n",
      "Epoch 67/500\n",
      "22272/22272 [==============================] - 13s 568us/step - loss: 0.1242 - acc: 0.8622 - val_loss: 0.1252 - val_acc: 0.8568\n",
      "Epoch 68/500\n",
      "22272/22272 [==============================] - 13s 593us/step - loss: 0.1242 - acc: 0.8589 - val_loss: 0.1219 - val_acc: 0.8593\n",
      "Epoch 69/500\n",
      "22272/22272 [==============================] - 13s 606us/step - loss: 0.1228 - acc: 0.8665 - val_loss: 0.1240 - val_acc: 0.8551\n",
      "Epoch 70/500\n",
      "22272/22272 [==============================] - 13s 570us/step - loss: 0.1217 - acc: 0.8661 - val_loss: 0.1234 - val_acc: 0.8602\n",
      "Epoch 71/500\n",
      "22272/22272 [==============================] - 13s 598us/step - loss: 0.1216 - acc: 0.8647 - val_loss: 0.1210 - val_acc: 0.8653\n",
      "Epoch 72/500\n",
      "22272/22272 [==============================] - 14s 610us/step - loss: 0.1202 - acc: 0.8672 - val_loss: 0.1191 - val_acc: 0.8593\n",
      "Epoch 73/500\n",
      "22272/22272 [==============================] - 12s 559us/step - loss: 0.1194 - acc: 0.8679 - val_loss: 0.1222 - val_acc: 0.8747\n",
      "Epoch 74/500\n",
      "22272/22272 [==============================] - 12s 545us/step - loss: 0.1188 - acc: 0.8693 - val_loss: 0.1181 - val_acc: 0.8559\n",
      "Epoch 75/500\n",
      "22272/22272 [==============================] - 13s 604us/step - loss: 0.1174 - acc: 0.8724 - val_loss: 0.1167 - val_acc: 0.8653\n",
      "Epoch 76/500\n",
      "22272/22272 [==============================] - 13s 583us/step - loss: 0.1171 - acc: 0.8719 - val_loss: 0.1182 - val_acc: 0.8645\n",
      "Epoch 77/500\n",
      "22272/22272 [==============================] - 14s 627us/step - loss: 0.1166 - acc: 0.8732 - val_loss: 0.1161 - val_acc: 0.8670\n",
      "Epoch 78/500\n",
      "22272/22272 [==============================] - 16s 705us/step - loss: 0.1162 - acc: 0.8734 - val_loss: 0.1142 - val_acc: 0.8806\n",
      "Epoch 79/500\n",
      "22272/22272 [==============================] - 16s 712us/step - loss: 0.1153 - acc: 0.8764 - val_loss: 0.1188 - val_acc: 0.8730\n",
      "Epoch 80/500\n",
      "22272/22272 [==============================] - 15s 674us/step - loss: 0.1147 - acc: 0.8757 - val_loss: 0.1148 - val_acc: 0.8696\n",
      "Epoch 81/500\n",
      "22272/22272 [==============================] - 15s 695us/step - loss: 0.1136 - acc: 0.8772 - val_loss: 0.1121 - val_acc: 0.8883\n",
      "Epoch 82/500\n",
      "22272/22272 [==============================] - 14s 634us/step - loss: 0.1131 - acc: 0.8766 - val_loss: 0.1158 - val_acc: 0.8781\n",
      "Epoch 83/500\n",
      "22272/22272 [==============================] - 13s 602us/step - loss: 0.1127 - acc: 0.8790 - val_loss: 0.1104 - val_acc: 0.8772\n",
      "Epoch 84/500\n",
      "22272/22272 [==============================] - 14s 612us/step - loss: 0.1123 - acc: 0.8798 - val_loss: 0.1096 - val_acc: 0.8841\n",
      "Epoch 85/500\n",
      "22272/22272 [==============================] - 14s 615us/step - loss: 0.1112 - acc: 0.8835 - val_loss: 0.1152 - val_acc: 0.8721\n",
      "Epoch 86/500\n",
      "22272/22272 [==============================] - 14s 607us/step - loss: 0.1105 - acc: 0.8838 - val_loss: 0.1101 - val_acc: 0.8824\n",
      "Epoch 87/500\n",
      "22272/22272 [==============================] - 12s 555us/step - loss: 0.1101 - acc: 0.8832 - val_loss: 0.1090 - val_acc: 0.8832\n",
      "Epoch 88/500\n",
      "22272/22272 [==============================] - 12s 550us/step - loss: 0.1098 - acc: 0.8835 - val_loss: 0.1138 - val_acc: 0.8696\n",
      "Epoch 89/500\n",
      "22272/22272 [==============================] - 13s 604us/step - loss: 0.1093 - acc: 0.8824 - val_loss: 0.1122 - val_acc: 0.8824\n",
      "Epoch 90/500\n",
      "22272/22272 [==============================] - 16s 723us/step - loss: 0.1092 - acc: 0.8844 - val_loss: 0.1076 - val_acc: 0.8832\n",
      "Epoch 91/500\n",
      "22272/22272 [==============================] - 14s 633us/step - loss: 0.1076 - acc: 0.8863 - val_loss: 0.1092 - val_acc: 0.8747\n",
      "Epoch 92/500\n",
      "22272/22272 [==============================] - 13s 561us/step - loss: 0.1069 - acc: 0.8882 - val_loss: 0.1122 - val_acc: 0.8772\n",
      "Epoch 93/500\n",
      "22272/22272 [==============================] - 12s 554us/step - loss: 0.1068 - acc: 0.8889 - val_loss: 0.1049 - val_acc: 0.8917\n",
      "Epoch 94/500\n",
      "22272/22272 [==============================] - 13s 598us/step - loss: 0.1054 - acc: 0.8899 - val_loss: 0.1155 - val_acc: 0.8713\n",
      "Epoch 95/500\n",
      "22272/22272 [==============================] - 14s 644us/step - loss: 0.1057 - acc: 0.8916 - val_loss: 0.1077 - val_acc: 0.8798\n",
      "Epoch 96/500\n",
      "22272/22272 [==============================] - 14s 619us/step - loss: 0.1044 - acc: 0.8913 - val_loss: 0.1069 - val_acc: 0.8772\n",
      "Epoch 97/500\n",
      "22272/22272 [==============================] - 14s 616us/step - loss: 0.1055 - acc: 0.8904 - val_loss: 0.1035 - val_acc: 0.8841\n",
      "Epoch 98/500\n",
      "22272/22272 [==============================] - 13s 592us/step - loss: 0.1032 - acc: 0.8926 - val_loss: 0.1077 - val_acc: 0.8815\n",
      "Epoch 99/500\n",
      "22272/22272 [==============================] - 14s 627us/step - loss: 0.1031 - acc: 0.8926 - val_loss: 0.0999 - val_acc: 0.8875\n",
      "Epoch 100/500\n",
      "22272/22272 [==============================] - 14s 607us/step - loss: 0.1027 - acc: 0.8940 - val_loss: 0.1052 - val_acc: 0.8875\n",
      "Epoch 101/500\n",
      "22272/22272 [==============================] - 14s 621us/step - loss: 0.1027 - acc: 0.8938 - val_loss: 0.1063 - val_acc: 0.8875\n",
      "Epoch 102/500\n",
      "22272/22272 [==============================] - 14s 624us/step - loss: 0.1025 - acc: 0.8962 - val_loss: 0.1027 - val_acc: 0.8858\n",
      "Epoch 103/500\n",
      "22272/22272 [==============================] - 13s 591us/step - loss: 0.1008 - acc: 0.8953 - val_loss: 0.1027 - val_acc: 0.8900\n",
      "Epoch 104/500\n",
      "22272/22272 [==============================] - 13s 584us/step - loss: 0.1013 - acc: 0.8973 - val_loss: 0.1005 - val_acc: 0.8934\n",
      "Epoch 105/500\n",
      "22272/22272 [==============================] - 14s 642us/step - loss: 0.1003 - acc: 0.8954 - val_loss: 0.0985 - val_acc: 0.8994\n",
      "Epoch 106/500\n",
      "22272/22272 [==============================] - 13s 595us/step - loss: 0.0993 - acc: 0.8985 - val_loss: 0.0976 - val_acc: 0.8986\n",
      "Epoch 107/500\n",
      "22272/22272 [==============================] - 14s 623us/step - loss: 0.0990 - acc: 0.8993 - val_loss: 0.0979 - val_acc: 0.8943\n",
      "Epoch 108/500\n",
      "22272/22272 [==============================] - 13s 586us/step - loss: 0.0983 - acc: 0.9012 - val_loss: 0.0981 - val_acc: 0.8900\n",
      "Epoch 109/500\n",
      "22272/22272 [==============================] - 14s 615us/step - loss: 0.0979 - acc: 0.9002 - val_loss: 0.1008 - val_acc: 0.8960\n",
      "Epoch 110/500\n",
      "22272/22272 [==============================] - 15s 659us/step - loss: 0.0974 - acc: 0.9031 - val_loss: 0.0975 - val_acc: 0.8849\n",
      "Epoch 111/500\n",
      "22272/22272 [==============================] - 14s 643us/step - loss: 0.0972 - acc: 0.9015 - val_loss: 0.0988 - val_acc: 0.8994\n",
      "Epoch 112/500\n",
      "22272/22272 [==============================] - 15s 656us/step - loss: 0.0967 - acc: 0.9028 - val_loss: 0.0989 - val_acc: 0.8951\n",
      "Epoch 113/500\n",
      "22272/22272 [==============================] - 15s 687us/step - loss: 0.0963 - acc: 0.9049 - val_loss: 0.0984 - val_acc: 0.8994\n",
      "Epoch 114/500\n",
      "22272/22272 [==============================] - 15s 657us/step - loss: 0.0970 - acc: 0.9017 - val_loss: 0.0979 - val_acc: 0.8934\n",
      "Epoch 115/500\n",
      "22272/22272 [==============================] - 13s 597us/step - loss: 0.0953 - acc: 0.9039 - val_loss: 0.1007 - val_acc: 0.8994\n",
      "Epoch 116/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22272/22272 [==============================] - 12s 555us/step - loss: 0.0948 - acc: 0.9073 - val_loss: 0.0933 - val_acc: 0.9011\n",
      "Epoch 117/500\n",
      "22272/22272 [==============================] - 13s 563us/step - loss: 0.0946 - acc: 0.9051 - val_loss: 0.0956 - val_acc: 0.8968\n",
      "Epoch 118/500\n",
      "22272/22272 [==============================] - 14s 628us/step - loss: 0.0939 - acc: 0.9072 - val_loss: 0.0959 - val_acc: 0.9037\n",
      "Epoch 119/500\n",
      "22272/22272 [==============================] - 14s 618us/step - loss: 0.0932 - acc: 0.9071 - val_loss: 0.0949 - val_acc: 0.9028\n",
      "Epoch 120/500\n",
      "22272/22272 [==============================] - 14s 615us/step - loss: 0.0929 - acc: 0.9100 - val_loss: 0.0918 - val_acc: 0.9096\n",
      "Epoch 121/500\n",
      "22272/22272 [==============================] - 13s 586us/step - loss: 0.0930 - acc: 0.9066 - val_loss: 0.0944 - val_acc: 0.8968\n",
      "Epoch 122/500\n",
      "22272/22272 [==============================] - 13s 573us/step - loss: 0.0921 - acc: 0.9133 - val_loss: 0.0939 - val_acc: 0.8977\n",
      "Epoch 123/500\n",
      "22272/22272 [==============================] - 13s 564us/step - loss: 0.0916 - acc: 0.9105 - val_loss: 0.0901 - val_acc: 0.8968\n",
      "Epoch 124/500\n",
      "22272/22272 [==============================] - 14s 630us/step - loss: 0.0918 - acc: 0.9111 - val_loss: 0.0924 - val_acc: 0.9037\n",
      "Epoch 125/500\n",
      "22272/22272 [==============================] - 14s 625us/step - loss: 0.0918 - acc: 0.9110 - val_loss: 0.0917 - val_acc: 0.9020\n",
      "Epoch 126/500\n",
      "22272/22272 [==============================] - 14s 630us/step - loss: 0.0902 - acc: 0.9117 - val_loss: 0.0903 - val_acc: 0.9096- loss: 0.090\n",
      "Epoch 127/500\n",
      "22272/22272 [==============================] - 13s 600us/step - loss: 0.0892 - acc: 0.9131 - val_loss: 0.0901 - val_acc: 0.9122\n",
      "Epoch 128/500\n",
      "22272/22272 [==============================] - 14s 606us/step - loss: 0.0898 - acc: 0.9133 - val_loss: 0.0897 - val_acc: 0.9079\n",
      "Epoch 129/500\n",
      "22272/22272 [==============================] - 13s 579us/step - loss: 0.0892 - acc: 0.9138 - val_loss: 0.0922 - val_acc: 0.9079\n",
      "Epoch 130/500\n",
      "22272/22272 [==============================] - 14s 621us/step - loss: 0.0884 - acc: 0.9147 - val_loss: 0.0910 - val_acc: 0.9071\n",
      "Epoch 131/500\n",
      "22272/22272 [==============================] - 12s 553us/step - loss: 0.0880 - acc: 0.9152 - val_loss: 0.0907 - val_acc: 0.9020\n",
      "Epoch 132/500\n",
      "22272/22272 [==============================] - 13s 574us/step - loss: 0.0883 - acc: 0.9137 - val_loss: 0.0876 - val_acc: 0.9028\n",
      "Epoch 133/500\n",
      "22272/22272 [==============================] - 12s 557us/step - loss: 0.0881 - acc: 0.9176 - val_loss: 0.0870 - val_acc: 0.9079\n",
      "Epoch 134/500\n",
      "22272/22272 [==============================] - 13s 601us/step - loss: 0.0880 - acc: 0.9155 - val_loss: 0.0880 - val_acc: 0.9062\n",
      "Epoch 135/500\n",
      "22272/22272 [==============================] - 13s 573us/step - loss: 0.0870 - acc: 0.9168 - val_loss: 0.0860 - val_acc: 0.9122\n",
      "Epoch 136/500\n",
      "22272/22272 [==============================] - 13s 590us/step - loss: 0.0870 - acc: 0.9172 - val_loss: 0.0878 - val_acc: 0.9096\n",
      "Epoch 137/500\n",
      "22272/22272 [==============================] - 13s 590us/step - loss: 0.0869 - acc: 0.9182 - val_loss: 0.0900 - val_acc: 0.9071\n",
      "Epoch 138/500\n",
      "22272/22272 [==============================] - 13s 582us/step - loss: 0.0867 - acc: 0.9186 - val_loss: 0.0887 - val_acc: 0.9088\n",
      "Epoch 139/500\n",
      "22272/22272 [==============================] - 13s 599us/step - loss: 0.0864 - acc: 0.9193 - val_loss: 0.0830 - val_acc: 0.9045\n",
      "Epoch 140/500\n",
      "22272/22272 [==============================] - 13s 578us/step - loss: 0.0848 - acc: 0.9203 - val_loss: 0.0856 - val_acc: 0.9156\n",
      "Epoch 141/500\n",
      "22272/22272 [==============================] - 12s 545us/step - loss: 0.0846 - acc: 0.9232 - val_loss: 0.0892 - val_acc: 0.9020\n",
      "Epoch 142/500\n",
      "22272/22272 [==============================] - 13s 561us/step - loss: 0.0846 - acc: 0.9204 - val_loss: 0.0882 - val_acc: 0.9045\n",
      "Epoch 143/500\n",
      "22272/22272 [==============================] - 13s 578us/step - loss: 0.0840 - acc: 0.9211 - val_loss: 0.0849 - val_acc: 0.9147\n",
      "Epoch 144/500\n",
      "22272/22272 [==============================] - 14s 608us/step - loss: 0.0835 - acc: 0.9226 - val_loss: 0.0872 - val_acc: 0.9147\n",
      "Epoch 145/500\n",
      "22272/22272 [==============================] - 13s 588us/step - loss: 0.0836 - acc: 0.9230 - val_loss: 0.0853 - val_acc: 0.9130\n",
      "Epoch 146/500\n",
      "22272/22272 [==============================] - 13s 570us/step - loss: 0.0833 - acc: 0.9215 - val_loss: 0.0847 - val_acc: 0.9165\n",
      "Epoch 147/500\n",
      "22272/22272 [==============================] - 13s 592us/step - loss: 0.0832 - acc: 0.9238 - val_loss: 0.0904 - val_acc: 0.9105\n",
      "Epoch 148/500\n",
      "22272/22272 [==============================] - 14s 622us/step - loss: 0.0829 - acc: 0.9213 - val_loss: 0.0858 - val_acc: 0.9096\n",
      "Epoch 149/500\n",
      "22272/22272 [==============================] - 14s 630us/step - loss: 0.0822 - acc: 0.9229 - val_loss: 0.0844 - val_acc: 0.9224\n",
      "Epoch 150/500\n",
      "22272/22272 [==============================] - 12s 553us/step - loss: 0.0816 - acc: 0.9270 - val_loss: 0.0819 - val_acc: 0.9165\n",
      "Epoch 151/500\n",
      "22272/22272 [==============================] - 12s 539us/step - loss: 0.0818 - acc: 0.9243 - val_loss: 0.0837 - val_acc: 0.9130\n",
      "Epoch 152/500\n",
      "22272/22272 [==============================] - 12s 539us/step - loss: 0.0816 - acc: 0.9276 - val_loss: 0.0814 - val_acc: 0.9199\n",
      "Epoch 153/500\n",
      "22272/22272 [==============================] - 13s 569us/step - loss: 0.0812 - acc: 0.9240 - val_loss: 0.0827 - val_acc: 0.9207\n",
      "Epoch 154/500\n",
      "22272/22272 [==============================] - 14s 622us/step - loss: 0.0816 - acc: 0.9262 - val_loss: 0.0832 - val_acc: 0.9113\n",
      "Epoch 155/500\n",
      "22272/22272 [==============================] - 12s 549us/step - loss: 0.0811 - acc: 0.9253 - val_loss: 0.0827 - val_acc: 0.9165\n",
      "Epoch 156/500\n",
      "22272/22272 [==============================] - 13s 570us/step - loss: 0.0810 - acc: 0.9256 - val_loss: 0.0837 - val_acc: 0.9250\n",
      "Epoch 157/500\n",
      "22272/22272 [==============================] - 12s 535us/step - loss: 0.0800 - acc: 0.9272 - val_loss: 0.0796 - val_acc: 0.9199\n",
      "Epoch 158/500\n",
      "22272/22272 [==============================] - 12s 533us/step - loss: 0.0806 - acc: 0.9257 - val_loss: 0.0845 - val_acc: 0.9173\n",
      "Epoch 159/500\n",
      "22272/22272 [==============================] - 12s 542us/step - loss: 0.0799 - acc: 0.9276 - val_loss: 0.0794 - val_acc: 0.9224\n",
      "Epoch 160/500\n",
      "22272/22272 [==============================] - 12s 548us/step - loss: 0.0794 - acc: 0.9265 - val_loss: 0.0788 - val_acc: 0.9199\n",
      "Epoch 161/500\n",
      "22272/22272 [==============================] - 13s 586us/step - loss: 0.0787 - acc: 0.9289 - val_loss: 0.0811 - val_acc: 0.9216\n",
      "Epoch 162/500\n",
      "22272/22272 [==============================] - 13s 562us/step - loss: 0.0788 - acc: 0.9296 - val_loss: 0.0806 - val_acc: 0.9258\n",
      "Epoch 163/500\n",
      "22272/22272 [==============================] - 12s 525us/step - loss: 0.0783 - acc: 0.9294 - val_loss: 0.0787 - val_acc: 0.9275\n",
      "Epoch 164/500\n",
      "22272/22272 [==============================] - 13s 563us/step - loss: 0.0783 - acc: 0.9292 - val_loss: 0.0761 - val_acc: 0.9344\n",
      "Epoch 165/500\n",
      "22272/22272 [==============================] - 12s 559us/step - loss: 0.0775 - acc: 0.9332 - val_loss: 0.0753 - val_acc: 0.9335\n",
      "Epoch 166/500\n",
      "22272/22272 [==============================] - 12s 558us/step - loss: 0.0778 - acc: 0.9307 - val_loss: 0.0787 - val_acc: 0.9327\n",
      "Epoch 167/500\n",
      "22272/22272 [==============================] - 12s 543us/step - loss: 0.0780 - acc: 0.9300 - val_loss: 0.0806 - val_acc: 0.9173\n",
      "Epoch 168/500\n",
      "22272/22272 [==============================] - 12s 540us/step - loss: 0.0768 - acc: 0.9330 - val_loss: 0.0787 - val_acc: 0.9207\n",
      "Epoch 169/500\n",
      "22272/22272 [==============================] - 13s 562us/step - loss: 0.0768 - acc: 0.9298 - val_loss: 0.0765 - val_acc: 0.9309\n",
      "Epoch 170/500\n",
      "22272/22272 [==============================] - 12s 546us/step - loss: 0.0765 - acc: 0.9321 - val_loss: 0.0746 - val_acc: 0.9361\n",
      "Epoch 171/500\n",
      "22272/22272 [==============================] - 13s 578us/step - loss: 0.0767 - acc: 0.9325 - val_loss: 0.0820 - val_acc: 0.9207\n",
      "Epoch 172/500\n",
      "22272/22272 [==============================] - 13s 586us/step - loss: 0.0755 - acc: 0.9345 - val_loss: 0.0778 - val_acc: 0.9327\n",
      "Epoch 173/500\n",
      "22272/22272 [==============================] - 13s 581us/step - loss: 0.0760 - acc: 0.9299 - val_loss: 0.0749 - val_acc: 0.9267\n",
      "Epoch 174/500\n",
      "22272/22272 [==============================] - 12s 550us/step - loss: 0.0759 - acc: 0.9325 - val_loss: 0.0781 - val_acc: 0.9284\n",
      "Epoch 175/500\n",
      "22272/22272 [==============================] - 13s 561us/step - loss: 0.0758 - acc: 0.9331 - val_loss: 0.0756 - val_acc: 0.9284\n",
      "Epoch 176/500\n",
      "22272/22272 [==============================] - 12s 537us/step - loss: 0.0756 - acc: 0.9362 - val_loss: 0.0738 - val_acc: 0.9352\n",
      "Epoch 177/500\n",
      "22272/22272 [==============================] - 12s 530us/step - loss: 0.0753 - acc: 0.9326 - val_loss: 0.0792 - val_acc: 0.9216\n",
      "Epoch 178/500\n",
      "22272/22272 [==============================] - 12s 521us/step - loss: 0.0743 - acc: 0.9351 - val_loss: 0.0746 - val_acc: 0.9318\n",
      "Epoch 179/500\n",
      "22272/22272 [==============================] - 12s 546us/step - loss: 0.0743 - acc: 0.9355 - val_loss: 0.0775 - val_acc: 0.9267\n",
      "Epoch 180/500\n",
      "22272/22272 [==============================] - 12s 547us/step - loss: 0.0746 - acc: 0.9360 - val_loss: 0.0769 - val_acc: 0.9309\n",
      "Epoch 181/500\n",
      "22272/22272 [==============================] - 13s 576us/step - loss: 0.0744 - acc: 0.9352 - val_loss: 0.0770 - val_acc: 0.9284\n",
      "Epoch 182/500\n",
      "22272/22272 [==============================] - 12s 560us/step - loss: 0.0739 - acc: 0.9344 - val_loss: 0.0753 - val_acc: 0.9318\n",
      "Epoch 183/500\n",
      "22272/22272 [==============================] - 12s 558us/step - loss: 0.0736 - acc: 0.9360 - val_loss: 0.0773 - val_acc: 0.9275\n",
      "Epoch 184/500\n",
      "22272/22272 [==============================] - 12s 554us/step - loss: 0.0736 - acc: 0.9374 - val_loss: 0.0719 - val_acc: 0.9318\n",
      "Epoch 185/500\n",
      "22272/22272 [==============================] - 12s 539us/step - loss: 0.0738 - acc: 0.9344 - val_loss: 0.0723 - val_acc: 0.9318\n",
      "Epoch 186/500\n",
      "22272/22272 [==============================] - 12s 548us/step - loss: 0.0731 - acc: 0.9375 - val_loss: 0.0763 - val_acc: 0.9301\n",
      "Epoch 187/500\n",
      "22272/22272 [==============================] - 12s 547us/step - loss: 0.0729 - acc: 0.9390 - val_loss: 0.0741 - val_acc: 0.9403\n",
      "Epoch 188/500\n",
      "22272/22272 [==============================] - 13s 574us/step - loss: 0.0726 - acc: 0.9366 - val_loss: 0.0713 - val_acc: 0.9352\n",
      "Epoch 189/500\n",
      "22272/22272 [==============================] - 12s 542us/step - loss: 0.0723 - acc: 0.9368 - val_loss: 0.0755 - val_acc: 0.9361\n",
      "Epoch 190/500\n",
      "22272/22272 [==============================] - 12s 538us/step - loss: 0.0727 - acc: 0.9370 - val_loss: 0.0725 - val_acc: 0.9335\n",
      "Epoch 191/500\n",
      "22272/22272 [==============================] - 13s 564us/step - loss: 0.0716 - acc: 0.9403 - val_loss: 0.0705 - val_acc: 0.9292\n",
      "Epoch 192/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0717 - acc: 0.9388 - val_loss: 0.0748 - val_acc: 0.9352\n",
      "Epoch 193/500\n",
      "22272/22272 [==============================] - 13s 577us/step - loss: 0.0713 - acc: 0.9389 - val_loss: 0.0713 - val_acc: 0.9327\n",
      "Epoch 194/500\n",
      "22272/22272 [==============================] - 13s 564us/step - loss: 0.0719 - acc: 0.9392 - val_loss: 0.0704 - val_acc: 0.9327\n",
      "Epoch 195/500\n",
      "22272/22272 [==============================] - 13s 566us/step - loss: 0.0712 - acc: 0.9388 - val_loss: 0.0703 - val_acc: 0.9361\n",
      "Epoch 196/500\n",
      "22272/22272 [==============================] - 12s 544us/step - loss: 0.0710 - acc: 0.9382 - val_loss: 0.0717 - val_acc: 0.9395\n",
      "Epoch 197/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0705 - acc: 0.9393 - val_loss: 0.0703 - val_acc: 0.9292\n",
      "Epoch 198/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0707 - acc: 0.9404 - val_loss: 0.0730 - val_acc: 0.9361\n",
      "Epoch 199/500\n",
      "22272/22272 [==============================] - 12s 543us/step - loss: 0.0698 - acc: 0.9406 - val_loss: 0.0690 - val_acc: 0.9335\n",
      "Epoch 200/500\n",
      "22272/22272 [==============================] - 12s 530us/step - loss: 0.0702 - acc: 0.9416 - val_loss: 0.0707 - val_acc: 0.9412\n",
      "Epoch 201/500\n",
      "22272/22272 [==============================] - 12s 561us/step - loss: 0.0700 - acc: 0.9415 - val_loss: 0.0693 - val_acc: 0.9429\n",
      "Epoch 202/500\n",
      "22272/22272 [==============================] - 12s 558us/step - loss: 0.0695 - acc: 0.9413 - val_loss: 0.0698 - val_acc: 0.9420\n",
      "Epoch 203/500\n",
      "22272/22272 [==============================] - 12s 560us/step - loss: 0.0699 - acc: 0.9396 - val_loss: 0.0702 - val_acc: 0.9429\n",
      "Epoch 204/500\n",
      "22272/22272 [==============================] - 12s 548us/step - loss: 0.0695 - acc: 0.9422 - val_loss: 0.0707 - val_acc: 0.9369\n",
      "Epoch 205/500\n",
      "22272/22272 [==============================] - 12s 528us/step - loss: 0.0698 - acc: 0.9408 - val_loss: 0.0726 - val_acc: 0.9327\n",
      "Epoch 206/500\n",
      "22272/22272 [==============================] - 12s 553us/step - loss: 0.0689 - acc: 0.9426 - val_loss: 0.0679 - val_acc: 0.9429\n",
      "Epoch 207/500\n",
      "22272/22272 [==============================] - 13s 565us/step - loss: 0.0687 - acc: 0.9436 - val_loss: 0.0722 - val_acc: 0.9369\n",
      "Epoch 208/500\n",
      "22272/22272 [==============================] - 13s 574us/step - loss: 0.0689 - acc: 0.9435 - val_loss: 0.0688 - val_acc: 0.9369\n",
      "Epoch 209/500\n",
      "22272/22272 [==============================] - 12s 542us/step - loss: 0.0689 - acc: 0.9408 - val_loss: 0.0682 - val_acc: 0.9412\n",
      "Epoch 210/500\n",
      "22272/22272 [==============================] - 12s 541us/step - loss: 0.0688 - acc: 0.9434 - val_loss: 0.0671 - val_acc: 0.9429\n",
      "Epoch 211/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.0680 - acc: 0.9457 - val_loss: 0.0695 - val_acc: 0.9335\n",
      "Epoch 212/500\n",
      "22272/22272 [==============================] - 13s 564us/step - loss: 0.0679 - acc: 0.9446 - val_loss: 0.0697 - val_acc: 0.9386\n",
      "Epoch 213/500\n",
      "22272/22272 [==============================] - 12s 558us/step - loss: 0.0678 - acc: 0.9470 - val_loss: 0.0688 - val_acc: 0.9378\n",
      "Epoch 214/500\n",
      "22272/22272 [==============================] - 13s 597us/step - loss: 0.0678 - acc: 0.9449 - val_loss: 0.0663 - val_acc: 0.9471\n",
      "Epoch 215/500\n",
      "22272/22272 [==============================] - 13s 562us/step - loss: 0.0680 - acc: 0.9437 - val_loss: 0.0714 - val_acc: 0.9412\n",
      "Epoch 216/500\n",
      "22272/22272 [==============================] - 12s 555us/step - loss: 0.0678 - acc: 0.9452 - val_loss: 0.0683 - val_acc: 0.9318\n",
      "Epoch 217/500\n",
      "22272/22272 [==============================] - 13s 576us/step - loss: 0.0672 - acc: 0.9434 - val_loss: 0.0672 - val_acc: 0.9429\n",
      "Epoch 218/500\n",
      "22272/22272 [==============================] - 12s 555us/step - loss: 0.0671 - acc: 0.9454 - val_loss: 0.0680 - val_acc: 0.9369\n",
      "Epoch 219/500\n",
      "22272/22272 [==============================] - 12s 540us/step - loss: 0.0667 - acc: 0.9448 - val_loss: 0.0680 - val_acc: 0.9437\n",
      "Epoch 220/500\n",
      "22272/22272 [==============================] - 13s 573us/step - loss: 0.0667 - acc: 0.9432 - val_loss: 0.0651 - val_acc: 0.9395\n",
      "Epoch 221/500\n",
      "22272/22272 [==============================] - 13s 577us/step - loss: 0.0664 - acc: 0.9455 - val_loss: 0.0647 - val_acc: 0.9429\n",
      "Epoch 222/500\n",
      "22272/22272 [==============================] - 14s 607us/step - loss: 0.0663 - acc: 0.9471 - val_loss: 0.0668 - val_acc: 0.9420\n",
      "Epoch 223/500\n",
      "22272/22272 [==============================] - 12s 543us/step - loss: 0.0663 - acc: 0.9457 - val_loss: 0.0675 - val_acc: 0.9429\n",
      "Epoch 224/500\n",
      "22272/22272 [==============================] - 12s 553us/step - loss: 0.0660 - acc: 0.9462 - val_loss: 0.0662 - val_acc: 0.9403\n",
      "Epoch 225/500\n",
      "22272/22272 [==============================] - 12s 534us/step - loss: 0.0659 - acc: 0.9459 - val_loss: 0.0647 - val_acc: 0.9386\n",
      "Epoch 226/500\n",
      "22272/22272 [==============================] - 13s 569us/step - loss: 0.0655 - acc: 0.9457 - val_loss: 0.0655 - val_acc: 0.9395\n",
      "Epoch 227/500\n",
      "22272/22272 [==============================] - 12s 559us/step - loss: 0.0651 - acc: 0.9457 - val_loss: 0.0675 - val_acc: 0.9429\n",
      "Epoch 228/500\n",
      "22272/22272 [==============================] - 12s 534us/step - loss: 0.0657 - acc: 0.9463 - val_loss: 0.0643 - val_acc: 0.9378\n",
      "Epoch 229/500\n",
      "22272/22272 [==============================] - 11s 512us/step - loss: 0.0647 - acc: 0.9468 - val_loss: 0.0652 - val_acc: 0.9437\n",
      "Epoch 230/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0650 - acc: 0.9450 - val_loss: 0.0627 - val_acc: 0.9446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231/500\n",
      "22272/22272 [==============================] - 12s 545us/step - loss: 0.0654 - acc: 0.9466 - val_loss: 0.0635 - val_acc: 0.9420\n",
      "Epoch 232/500\n",
      "22272/22272 [==============================] - 13s 570us/step - loss: 0.0650 - acc: 0.9464 - val_loss: 0.0653 - val_acc: 0.9378\n",
      "Epoch 233/500\n",
      "22272/22272 [==============================] - 13s 576us/step - loss: 0.0652 - acc: 0.9465 - val_loss: 0.0654 - val_acc: 0.9395\n",
      "Epoch 234/500\n",
      "22272/22272 [==============================] - 12s 540us/step - loss: 0.0638 - acc: 0.9468 - val_loss: 0.0647 - val_acc: 0.9403\n",
      "Epoch 235/500\n",
      "22272/22272 [==============================] - 12s 547us/step - loss: 0.0645 - acc: 0.9489 - val_loss: 0.0636 - val_acc: 0.9429\n",
      "Epoch 236/500\n",
      "22272/22272 [==============================] - 12s 526us/step - loss: 0.0647 - acc: 0.9482 - val_loss: 0.0672 - val_acc: 0.9420\n",
      "Epoch 237/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0646 - acc: 0.9475 - val_loss: 0.0633 - val_acc: 0.9386\n",
      "Epoch 238/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0636 - acc: 0.9479 - val_loss: 0.0630 - val_acc: 0.9506\n",
      "Epoch 239/500\n",
      "22272/22272 [==============================] - 12s 528us/step - loss: 0.0645 - acc: 0.9485 - val_loss: 0.0640 - val_acc: 0.9378\n",
      "Epoch 240/500\n",
      "22272/22272 [==============================] - 12s 534us/step - loss: 0.0636 - acc: 0.9491 - val_loss: 0.0641 - val_acc: 0.9403\n",
      "Epoch 241/500\n",
      "22272/22272 [==============================] - 13s 576us/step - loss: 0.0635 - acc: 0.9488 - val_loss: 0.0622 - val_acc: 0.9488\n",
      "Epoch 242/500\n",
      "22272/22272 [==============================] - 12s 560us/step - loss: 0.0632 - acc: 0.9490 - val_loss: 0.0628 - val_acc: 0.9378\n",
      "Epoch 243/500\n",
      "22272/22272 [==============================] - 13s 567us/step - loss: 0.0628 - acc: 0.9484 - val_loss: 0.0615 - val_acc: 0.9480\n",
      "Epoch 244/500\n",
      "22272/22272 [==============================] - 12s 546us/step - loss: 0.0627 - acc: 0.9505 - val_loss: 0.0630 - val_acc: 0.9454\n",
      "Epoch 245/500\n",
      "22272/22272 [==============================] - 13s 576us/step - loss: 0.0621 - acc: 0.9501 - val_loss: 0.0633 - val_acc: 0.9429\n",
      "Epoch 246/500\n",
      "22272/22272 [==============================] - 12s 555us/step - loss: 0.0635 - acc: 0.9481 - val_loss: 0.0636 - val_acc: 0.9361\n",
      "Epoch 247/500\n",
      "22272/22272 [==============================] - 13s 588us/step - loss: 0.0630 - acc: 0.9486 - val_loss: 0.0628 - val_acc: 0.9344\n",
      "Epoch 248/500\n",
      "22272/22272 [==============================] - 12s 549us/step - loss: 0.0624 - acc: 0.9500 - val_loss: 0.0661 - val_acc: 0.9344\n",
      "Epoch 249/500\n",
      "22272/22272 [==============================] - 12s 539us/step - loss: 0.0628 - acc: 0.9476 - val_loss: 0.0616 - val_acc: 0.9395\n",
      "Epoch 250/500\n",
      "22272/22272 [==============================] - 12s 533us/step - loss: 0.0624 - acc: 0.9493 - val_loss: 0.0623 - val_acc: 0.9369\n",
      "Epoch 251/500\n",
      "22272/22272 [==============================] - 11s 500us/step - loss: 0.0626 - acc: 0.9484 - val_loss: 0.0655 - val_acc: 0.9386\n",
      "Epoch 252/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0624 - acc: 0.9485 - val_loss: 0.0619 - val_acc: 0.9403\n",
      "Epoch 253/500\n",
      "22272/22272 [==============================] - 12s 534us/step - loss: 0.0618 - acc: 0.9516 - val_loss: 0.0620 - val_acc: 0.9463\n",
      "Epoch 254/500\n",
      "22272/22272 [==============================] - 12s 535us/step - loss: 0.0617 - acc: 0.9535 - val_loss: 0.0623 - val_acc: 0.9480\n",
      "Epoch 255/500\n",
      "22272/22272 [==============================] - 12s 539us/step - loss: 0.0616 - acc: 0.9511 - val_loss: 0.0641 - val_acc: 0.9471\n",
      "Epoch 256/500\n",
      "22272/22272 [==============================] - 12s 531us/step - loss: 0.0619 - acc: 0.9506 - val_loss: 0.0619 - val_acc: 0.9420\n",
      "Epoch 257/500\n",
      "22272/22272 [==============================] - 12s 554us/step - loss: 0.0620 - acc: 0.9512 - val_loss: 0.0603 - val_acc: 0.9420\n",
      "Epoch 258/500\n",
      "22272/22272 [==============================] - 12s 536us/step - loss: 0.0614 - acc: 0.9519 - val_loss: 0.0608 - val_acc: 0.9437\n",
      "Epoch 259/500\n",
      "22272/22272 [==============================] - 13s 573us/step - loss: 0.0614 - acc: 0.9507 - val_loss: 0.0617 - val_acc: 0.9429\n",
      "Epoch 260/500\n",
      "22272/22272 [==============================] - 12s 549us/step - loss: 0.0613 - acc: 0.9506 - val_loss: 0.0612 - val_acc: 0.9446\n",
      "Epoch 261/500\n",
      "22272/22272 [==============================] - 13s 588us/step - loss: 0.0614 - acc: 0.9497 - val_loss: 0.0618 - val_acc: 0.9471\n",
      "Epoch 262/500\n",
      "22272/22272 [==============================] - 12s 557us/step - loss: 0.0611 - acc: 0.9522 - val_loss: 0.0604 - val_acc: 0.9506\n",
      "Epoch 263/500\n",
      "22272/22272 [==============================] - 12s 548us/step - loss: 0.0609 - acc: 0.9508 - val_loss: 0.0607 - val_acc: 0.9463\n",
      "Epoch 264/500\n",
      "22272/22272 [==============================] - 12s 548us/step - loss: 0.0598 - acc: 0.9518 - val_loss: 0.0599 - val_acc: 0.9471\n",
      "Epoch 265/500\n",
      "22272/22272 [==============================] - 13s 582us/step - loss: 0.0611 - acc: 0.9522 - val_loss: 0.0598 - val_acc: 0.9429\n",
      "Epoch 266/500\n",
      "22272/22272 [==============================] - 13s 585us/step - loss: 0.0601 - acc: 0.9524 - val_loss: 0.0594 - val_acc: 0.9480\n",
      "Epoch 267/500\n",
      "22272/22272 [==============================] - 14s 622us/step - loss: 0.0606 - acc: 0.9525 - val_loss: 0.0603 - val_acc: 0.9497\n",
      "Epoch 268/500\n",
      "22272/22272 [==============================] - 13s 585us/step - loss: 0.0606 - acc: 0.9532 - val_loss: 0.0608 - val_acc: 0.9497\n",
      "Epoch 269/500\n",
      "22272/22272 [==============================] - 14s 635us/step - loss: 0.0605 - acc: 0.9511 - val_loss: 0.0635 - val_acc: 0.9403\n",
      "Epoch 270/500\n",
      "22272/22272 [==============================] - 13s 576us/step - loss: 0.0593 - acc: 0.9536 - val_loss: 0.0603 - val_acc: 0.9471\n",
      "Epoch 271/500\n",
      "22272/22272 [==============================] - 13s 568us/step - loss: 0.0598 - acc: 0.9527 - val_loss: 0.0619 - val_acc: 0.9446\n",
      "Epoch 272/500\n",
      "22272/22272 [==============================] - 13s 569us/step - loss: 0.0595 - acc: 0.9542 - val_loss: 0.0595 - val_acc: 0.9463\n",
      "Epoch 273/500\n",
      "22272/22272 [==============================] - 12s 534us/step - loss: 0.0592 - acc: 0.9548 - val_loss: 0.0596 - val_acc: 0.9506\n",
      "Epoch 274/500\n",
      "22272/22272 [==============================] - 13s 582us/step - loss: 0.0591 - acc: 0.9519 - val_loss: 0.0587 - val_acc: 0.9506\n",
      "Epoch 275/500\n",
      "22272/22272 [==============================] - 12s 551us/step - loss: 0.0595 - acc: 0.9535 - val_loss: 0.0594 - val_acc: 0.9488\n",
      "Epoch 276/500\n",
      "22272/22272 [==============================] - 12s 551us/step - loss: 0.0590 - acc: 0.9544 - val_loss: 0.0589 - val_acc: 0.9497\n",
      "Epoch 277/500\n",
      "22272/22272 [==============================] - 12s 540us/step - loss: 0.0592 - acc: 0.9538 - val_loss: 0.0596 - val_acc: 0.9506\n",
      "Epoch 278/500\n",
      "22272/22272 [==============================] - 13s 574us/step - loss: 0.0592 - acc: 0.9542 - val_loss: 0.0586 - val_acc: 0.9497\n",
      "Epoch 279/500\n",
      "22272/22272 [==============================] - 12s 541us/step - loss: 0.0590 - acc: 0.9524 - val_loss: 0.0592 - val_acc: 0.9446\n",
      "Epoch 280/500\n",
      "22272/22272 [==============================] - 12s 558us/step - loss: 0.0584 - acc: 0.9525 - val_loss: 0.0593 - val_acc: 0.9463\n",
      "Epoch 281/500\n",
      "22272/22272 [==============================] - 12s 547us/step - loss: 0.0584 - acc: 0.9529 - val_loss: 0.0594 - val_acc: 0.9454\n",
      "Epoch 282/500\n",
      "22272/22272 [==============================] - 12s 559us/step - loss: 0.0587 - acc: 0.9535 - val_loss: 0.0593 - val_acc: 0.9523\n",
      "Epoch 283/500\n",
      "22272/22272 [==============================] - 13s 563us/step - loss: 0.0582 - acc: 0.9569 - val_loss: 0.0587 - val_acc: 0.9437\n",
      "Epoch 284/500\n",
      "22272/22272 [==============================] - 13s 562us/step - loss: 0.0577 - acc: 0.9548 - val_loss: 0.0598 - val_acc: 0.9497\n",
      "Epoch 285/500\n",
      "22272/22272 [==============================] - 12s 552us/step - loss: 0.0582 - acc: 0.9556 - val_loss: 0.0629 - val_acc: 0.9446\n",
      "Epoch 286/500\n",
      "22272/22272 [==============================] - 13s 580us/step - loss: 0.0584 - acc: 0.9542 - val_loss: 0.0583 - val_acc: 0.9506\n",
      "Epoch 287/500\n",
      "22272/22272 [==============================] - 13s 592us/step - loss: 0.0576 - acc: 0.9559 - val_loss: 0.0575 - val_acc: 0.9446\n",
      "Epoch 288/500\n",
      "22272/22272 [==============================] - 13s 567us/step - loss: 0.0576 - acc: 0.9558 - val_loss: 0.0565 - val_acc: 0.9480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289/500\n",
      "22272/22272 [==============================] - 12s 557us/step - loss: 0.0578 - acc: 0.9554 - val_loss: 0.0578 - val_acc: 0.9471\n",
      "Epoch 290/500\n",
      "22272/22272 [==============================] - 13s 590us/step - loss: 0.0577 - acc: 0.9559 - val_loss: 0.0564 - val_acc: 0.9471\n",
      "Epoch 291/500\n",
      "22272/22272 [==============================] - 13s 603us/step - loss: 0.0575 - acc: 0.9561 - val_loss: 0.0592 - val_acc: 0.9480\n",
      "Epoch 292/500\n",
      "22272/22272 [==============================] - 13s 583us/step - loss: 0.0579 - acc: 0.9559 - val_loss: 0.0604 - val_acc: 0.9480\n",
      "Epoch 293/500\n",
      "22272/22272 [==============================] - 12s 542us/step - loss: 0.0575 - acc: 0.9555 - val_loss: 0.0564 - val_acc: 0.9514\n",
      "Epoch 294/500\n",
      "22272/22272 [==============================] - 13s 593us/step - loss: 0.0566 - acc: 0.9560 - val_loss: 0.0566 - val_acc: 0.9412\n",
      "Epoch 295/500\n",
      "22272/22272 [==============================] - 13s 574us/step - loss: 0.0577 - acc: 0.9538 - val_loss: 0.0560 - val_acc: 0.9497\n",
      "Epoch 296/500\n",
      "22272/22272 [==============================] - 12s 546us/step - loss: 0.0572 - acc: 0.9539 - val_loss: 0.0566 - val_acc: 0.9514\n",
      "Epoch 297/500\n",
      "22272/22272 [==============================] - 13s 579us/step - loss: 0.0575 - acc: 0.9545 - val_loss: 0.0604 - val_acc: 0.9395\n",
      "Epoch 298/500\n",
      "22272/22272 [==============================] - 13s 595us/step - loss: 0.0565 - acc: 0.9552 - val_loss: 0.0571 - val_acc: 0.9471\n",
      "Epoch 299/500\n",
      "22272/22272 [==============================] - 13s 570us/step - loss: 0.0570 - acc: 0.9562 - val_loss: 0.0566 - val_acc: 0.9488\n",
      "Epoch 300/500\n",
      "22272/22272 [==============================] - 12s 547us/step - loss: 0.0569 - acc: 0.9541 - val_loss: 0.0561 - val_acc: 0.9497\n",
      "Epoch 301/500\n",
      "22272/22272 [==============================] - 12s 553us/step - loss: 0.0565 - acc: 0.9562 - val_loss: 0.0560 - val_acc: 0.9523\n",
      "Epoch 302/500\n",
      "22272/22272 [==============================] - 12s 553us/step - loss: 0.0569 - acc: 0.9570 - val_loss: 0.0572 - val_acc: 0.9488\n",
      "Epoch 303/500\n",
      "22272/22272 [==============================] - 12s 521us/step - loss: 0.0562 - acc: 0.9581 - val_loss: 0.0573 - val_acc: 0.9497\n",
      "Epoch 304/500\n",
      "22272/22272 [==============================] - 13s 562us/step - loss: 0.0566 - acc: 0.9537 - val_loss: 0.0564 - val_acc: 0.9523\n",
      "Epoch 305/500\n",
      "22272/22272 [==============================] - 12s 545us/step - loss: 0.0563 - acc: 0.9572 - val_loss: 0.0551 - val_acc: 0.9454\n",
      "Epoch 306/500\n",
      "22272/22272 [==============================] - 12s 560us/step - loss: 0.0563 - acc: 0.9553 - val_loss: 0.0579 - val_acc: 0.9531\n",
      "Epoch 307/500\n",
      "22272/22272 [==============================] - 13s 583us/step - loss: 0.0561 - acc: 0.9570 - val_loss: 0.0559 - val_acc: 0.9540\n",
      "Epoch 308/500\n",
      "22272/22272 [==============================] - 13s 585us/step - loss: 0.0562 - acc: 0.9568 - val_loss: 0.0542 - val_acc: 0.9531\n",
      "Epoch 309/500\n",
      "22272/22272 [==============================] - 13s 578us/step - loss: 0.0562 - acc: 0.9575 - val_loss: 0.0565 - val_acc: 0.9463\n",
      "Epoch 310/500\n",
      "22272/22272 [==============================] - 12s 559us/step - loss: 0.0561 - acc: 0.9572 - val_loss: 0.0550 - val_acc: 0.9514\n",
      "Epoch 311/500\n",
      "22272/22272 [==============================] - 13s 578us/step - loss: 0.0560 - acc: 0.9567 - val_loss: 0.0563 - val_acc: 0.9429\n",
      "Epoch 312/500\n",
      "22272/22272 [==============================] - 13s 569us/step - loss: 0.0557 - acc: 0.9563 - val_loss: 0.0553 - val_acc: 0.9531\n",
      "Epoch 313/500\n",
      "22272/22272 [==============================] - 13s 594us/step - loss: 0.0555 - acc: 0.9573 - val_loss: 0.0538 - val_acc: 0.9506\n",
      "Epoch 314/500\n",
      "22272/22272 [==============================] - 12s 546us/step - loss: 0.0554 - acc: 0.9580 - val_loss: 0.0581 - val_acc: 0.9497\n",
      "Epoch 315/500\n",
      "22272/22272 [==============================] - 13s 571us/step - loss: 0.0552 - acc: 0.9580 - val_loss: 0.0542 - val_acc: 0.9497\n",
      "Epoch 316/500\n",
      "22272/22272 [==============================] - 12s 555us/step - loss: 0.0556 - acc: 0.9573 - val_loss: 0.0563 - val_acc: 0.9531\n",
      "Epoch 317/500\n",
      "22272/22272 [==============================] - 13s 572us/step - loss: 0.0551 - acc: 0.9566 - val_loss: 0.0567 - val_acc: 0.9506\n",
      "Epoch 318/500\n",
      "22272/22272 [==============================] - 12s 523us/step - loss: 0.0552 - acc: 0.9578 - val_loss: 0.0547 - val_acc: 0.9497\n",
      "Epoch 319/500\n",
      "22272/22272 [==============================] - 12s 549us/step - loss: 0.0551 - acc: 0.9599 - val_loss: 0.0552 - val_acc: 0.9471\n",
      "Epoch 320/500\n",
      "22272/22272 [==============================] - 12s 540us/step - loss: 0.0550 - acc: 0.9572 - val_loss: 0.0538 - val_acc: 0.9471\n",
      "Epoch 321/500\n",
      "22272/22272 [==============================] - 12s 521us/step - loss: 0.0554 - acc: 0.9566 - val_loss: 0.0531 - val_acc: 0.9531\n",
      "Epoch 322/500\n",
      "22272/22272 [==============================] - 12s 524us/step - loss: 0.0552 - acc: 0.9576 - val_loss: 0.0544 - val_acc: 0.9471\n",
      "Epoch 323/500\n",
      "22272/22272 [==============================] - 11s 507us/step - loss: 0.0556 - acc: 0.9561 - val_loss: 0.0539 - val_acc: 0.9514\n",
      "Epoch 324/500\n",
      "22272/22272 [==============================] - 11s 506us/step - loss: 0.0549 - acc: 0.9573 - val_loss: 0.0560 - val_acc: 0.9497\n",
      "Epoch 325/500\n",
      "22272/22272 [==============================] - 12s 540us/step - loss: 0.0546 - acc: 0.9585 - val_loss: 0.0536 - val_acc: 0.9514\n",
      "Epoch 326/500\n",
      "22272/22272 [==============================] - 12s 527us/step - loss: 0.0543 - acc: 0.9578 - val_loss: 0.0555 - val_acc: 0.9514\n",
      "Epoch 327/500\n",
      "22272/22272 [==============================] - 13s 575us/step - loss: 0.0545 - acc: 0.9572 - val_loss: 0.0556 - val_acc: 0.9454\n",
      "Epoch 328/500\n",
      "22272/22272 [==============================] - 13s 590us/step - loss: 0.0548 - acc: 0.9577 - val_loss: 0.0534 - val_acc: 0.9454\n",
      "Epoch 329/500\n",
      "22272/22272 [==============================] - 12s 544us/step - loss: 0.0543 - acc: 0.9577 - val_loss: 0.0534 - val_acc: 0.9497\n",
      "Epoch 330/500\n",
      "22272/22272 [==============================] - 11s 511us/step - loss: 0.0538 - acc: 0.9579 - val_loss: 0.0550 - val_acc: 0.9514\n",
      "Epoch 331/500\n",
      "22272/22272 [==============================] - 11s 510us/step - loss: 0.0544 - acc: 0.9601 - val_loss: 0.0534 - val_acc: 0.9540\n",
      "Epoch 332/500\n",
      "22272/22272 [==============================] - 12s 544us/step - loss: 0.0543 - acc: 0.9575 - val_loss: 0.0553 - val_acc: 0.9506\n",
      "Epoch 333/500\n",
      "22272/22272 [==============================] - 12s 541us/step - loss: 0.0540 - acc: 0.9597 - val_loss: 0.0531 - val_acc: 0.9506\n",
      "Epoch 334/500\n",
      "22272/22272 [==============================] - 13s 599us/step - loss: 0.0538 - acc: 0.9572 - val_loss: 0.0540 - val_acc: 0.9540\n",
      "Epoch 335/500\n",
      "22272/22272 [==============================] - 15s 665us/step - loss: 0.0539 - acc: 0.9612 - val_loss: 0.0532 - val_acc: 0.9471\n",
      "Epoch 336/500\n",
      "22272/22272 [==============================] - 13s 587us/step - loss: 0.0538 - acc: 0.9579 - val_loss: 0.0550 - val_acc: 0.9523\n",
      "Epoch 337/500\n",
      "22272/22272 [==============================] - 12s 553us/step - loss: 0.0534 - acc: 0.9593 - val_loss: 0.0533 - val_acc: 0.9531\n",
      "Epoch 338/500\n",
      "22272/22272 [==============================] - 12s 520us/step - loss: 0.0537 - acc: 0.9586 - val_loss: 0.0530 - val_acc: 0.9531\n",
      "Epoch 339/500\n",
      "22272/22272 [==============================] - 12s 526us/step - loss: 0.0542 - acc: 0.9596 - val_loss: 0.0531 - val_acc: 0.9523\n",
      "Epoch 340/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0537 - acc: 0.9594 - val_loss: 0.0521 - val_acc: 0.9506\n",
      "Epoch 341/500\n",
      "22272/22272 [==============================] - 12s 533us/step - loss: 0.0533 - acc: 0.9601 - val_loss: 0.0531 - val_acc: 0.9540\n",
      "Epoch 342/500\n",
      "22272/22272 [==============================] - 12s 523us/step - loss: 0.0535 - acc: 0.9591 - val_loss: 0.0533 - val_acc: 0.9506\n",
      "Epoch 343/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0534 - acc: 0.9606 - val_loss: 0.0529 - val_acc: 0.9523\n",
      "Epoch 344/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0535 - acc: 0.9595 - val_loss: 0.0530 - val_acc: 0.9540\n",
      "Epoch 345/500\n",
      "22272/22272 [==============================] - 12s 534us/step - loss: 0.0530 - acc: 0.9591 - val_loss: 0.0516 - val_acc: 0.9463\n",
      "Epoch 346/500\n",
      "22272/22272 [==============================] - 12s 535us/step - loss: 0.0532 - acc: 0.9597 - val_loss: 0.0530 - val_acc: 0.9506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 347/500\n",
      "22272/22272 [==============================] - 12s 547us/step - loss: 0.0534 - acc: 0.9581 - val_loss: 0.0516 - val_acc: 0.9548\n",
      "Epoch 348/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.0531 - acc: 0.9613 - val_loss: 0.0531 - val_acc: 0.9531\n",
      "Epoch 349/500\n",
      "22272/22272 [==============================] - 12s 550us/step - loss: 0.0532 - acc: 0.9605 - val_loss: 0.0525 - val_acc: 0.9540\n",
      "Epoch 350/500\n",
      "22272/22272 [==============================] - 12s 544us/step - loss: 0.0525 - acc: 0.9610 - val_loss: 0.0530 - val_acc: 0.9514\n",
      "Epoch 351/500\n",
      "22272/22272 [==============================] - 13s 590us/step - loss: 0.0525 - acc: 0.9618 - val_loss: 0.0533 - val_acc: 0.9514\n",
      "Epoch 352/500\n",
      "22272/22272 [==============================] - 13s 585us/step - loss: 0.0527 - acc: 0.9599 - val_loss: 0.0545 - val_acc: 0.9506\n",
      "Epoch 353/500\n",
      "22272/22272 [==============================] - 12s 548us/step - loss: 0.0527 - acc: 0.9620 - val_loss: 0.0523 - val_acc: 0.9548\n",
      "Epoch 354/500\n",
      "22272/22272 [==============================] - 12s 544us/step - loss: 0.0525 - acc: 0.9597 - val_loss: 0.0519 - val_acc: 0.9506\n",
      "Epoch 355/500\n",
      "22272/22272 [==============================] - 14s 621us/step - loss: 0.0522 - acc: 0.9617 - val_loss: 0.0508 - val_acc: 0.9557\n",
      "Epoch 356/500\n",
      "22272/22272 [==============================] - 13s 585us/step - loss: 0.0522 - acc: 0.9594 - val_loss: 0.0523 - val_acc: 0.9548\n",
      "Epoch 357/500\n",
      "22272/22272 [==============================] - 13s 583us/step - loss: 0.0523 - acc: 0.9606 - val_loss: 0.0512 - val_acc: 0.9523\n",
      "Epoch 358/500\n",
      "22272/22272 [==============================] - 14s 626us/step - loss: 0.0524 - acc: 0.9622 - val_loss: 0.0543 - val_acc: 0.9523\n",
      "Epoch 359/500\n",
      "22272/22272 [==============================] - 14s 644us/step - loss: 0.0519 - acc: 0.9621 - val_loss: 0.0523 - val_acc: 0.9548\n",
      "Epoch 360/500\n",
      "22272/22272 [==============================] - 15s 688us/step - loss: 0.0520 - acc: 0.9614 - val_loss: 0.0523 - val_acc: 0.9565\n",
      "Epoch 361/500\n",
      "22272/22272 [==============================] - 14s 609us/step - loss: 0.0523 - acc: 0.9615 - val_loss: 0.0515 - val_acc: 0.9540\n",
      "Epoch 362/500\n",
      "22272/22272 [==============================] - 13s 590us/step - loss: 0.0524 - acc: 0.9594 - val_loss: 0.0530 - val_acc: 0.9471\n",
      "Epoch 363/500\n",
      "22272/22272 [==============================] - 14s 648us/step - loss: 0.0520 - acc: 0.9615 - val_loss: 0.0502 - val_acc: 0.9557\n",
      "Epoch 364/500\n",
      "22272/22272 [==============================] - 14s 640us/step - loss: 0.0521 - acc: 0.9619 - val_loss: 0.0504 - val_acc: 0.9557\n",
      "Epoch 365/500\n",
      "22272/22272 [==============================] - 15s 680us/step - loss: 0.0521 - acc: 0.9620 - val_loss: 0.0513 - val_acc: 0.9548\n",
      "Epoch 366/500\n",
      "22272/22272 [==============================] - 14s 651us/step - loss: 0.0516 - acc: 0.9632 - val_loss: 0.0505 - val_acc: 0.9548\n",
      "Epoch 367/500\n",
      "22272/22272 [==============================] - 13s 576us/step - loss: 0.0518 - acc: 0.9616 - val_loss: 0.0520 - val_acc: 0.9557\n",
      "Epoch 368/500\n",
      "22272/22272 [==============================] - 12s 550us/step - loss: 0.0515 - acc: 0.9609 - val_loss: 0.0529 - val_acc: 0.9540\n",
      "Epoch 369/500\n",
      "22272/22272 [==============================] - 12s 549us/step - loss: 0.0513 - acc: 0.9625 - val_loss: 0.0507 - val_acc: 0.9574\n",
      "Epoch 370/500\n",
      "22272/22272 [==============================] - 12s 553us/step - loss: 0.0514 - acc: 0.9621 - val_loss: 0.0508 - val_acc: 0.9565\n",
      "Epoch 371/500\n",
      "22272/22272 [==============================] - 13s 565us/step - loss: 0.0515 - acc: 0.9607 - val_loss: 0.0517 - val_acc: 0.9531\n",
      "Epoch 372/500\n",
      "22272/22272 [==============================] - 12s 560us/step - loss: 0.0514 - acc: 0.9621 - val_loss: 0.0517 - val_acc: 0.9582\n",
      "Epoch 373/500\n",
      "22272/22272 [==============================] - 13s 597us/step - loss: 0.0514 - acc: 0.9612 - val_loss: 0.0511 - val_acc: 0.9548\n",
      "Epoch 374/500\n",
      "22272/22272 [==============================] - 13s 573us/step - loss: 0.0514 - acc: 0.9626 - val_loss: 0.0520 - val_acc: 0.9565\n",
      "Epoch 375/500\n",
      "22272/22272 [==============================] - 12s 536us/step - loss: 0.0511 - acc: 0.9624 - val_loss: 0.0528 - val_acc: 0.9548\n",
      "Epoch 376/500\n",
      "22272/22272 [==============================] - 12s 531us/step - loss: 0.0511 - acc: 0.9624 - val_loss: 0.0497 - val_acc: 0.9540\n",
      "Epoch 377/500\n",
      "22272/22272 [==============================] - 13s 578us/step - loss: 0.0510 - acc: 0.9622 - val_loss: 0.0517 - val_acc: 0.9497\n",
      "Epoch 378/500\n",
      "22272/22272 [==============================] - 14s 613us/step - loss: 0.0512 - acc: 0.9637 - val_loss: 0.0507 - val_acc: 0.9540\n",
      "Epoch 379/500\n",
      "22272/22272 [==============================] - 13s 583us/step - loss: 0.0510 - acc: 0.9632 - val_loss: 0.0501 - val_acc: 0.9565\n",
      "Epoch 380/500\n",
      "22272/22272 [==============================] - 14s 607us/step - loss: 0.0505 - acc: 0.9626 - val_loss: 0.0506 - val_acc: 0.9480\n",
      "Epoch 381/500\n",
      "22272/22272 [==============================] - 12s 544us/step - loss: 0.0513 - acc: 0.9613 - val_loss: 0.0511 - val_acc: 0.9557\n",
      "Epoch 382/500\n",
      "22272/22272 [==============================] - 12s 526us/step - loss: 0.0507 - acc: 0.9621 - val_loss: 0.0497 - val_acc: 0.9497\n",
      "Epoch 383/500\n",
      "22272/22272 [==============================] - 12s 541us/step - loss: 0.0503 - acc: 0.9635 - val_loss: 0.0504 - val_acc: 0.9523\n",
      "Epoch 384/500\n",
      "22272/22272 [==============================] - 13s 581us/step - loss: 0.0505 - acc: 0.9618 - val_loss: 0.0497 - val_acc: 0.9608\n",
      "Epoch 385/500\n",
      "22272/22272 [==============================] - 15s 678us/step - loss: 0.0508 - acc: 0.9602 - val_loss: 0.0517 - val_acc: 0.9523\n",
      "Epoch 386/500\n",
      "22272/22272 [==============================] - 13s 595us/step - loss: 0.0506 - acc: 0.9604 - val_loss: 0.0496 - val_acc: 0.9531\n",
      "Epoch 387/500\n",
      "22272/22272 [==============================] - 12s 534us/step - loss: 0.0508 - acc: 0.9625 - val_loss: 0.0517 - val_acc: 0.9565\n",
      "Epoch 388/500\n",
      "22272/22272 [==============================] - 12s 528us/step - loss: 0.0506 - acc: 0.9613 - val_loss: 0.0490 - val_acc: 0.9531\n",
      "Epoch 389/500\n",
      "22272/22272 [==============================] - 14s 641us/step - loss: 0.0500 - acc: 0.9613 - val_loss: 0.0500 - val_acc: 0.9523\n",
      "Epoch 390/500\n",
      "22272/22272 [==============================] - 12s 556us/step - loss: 0.0503 - acc: 0.9624 - val_loss: 0.0505 - val_acc: 0.9523\n",
      "Epoch 391/500\n",
      "22272/22272 [==============================] - 12s 524us/step - loss: 0.0500 - acc: 0.9612 - val_loss: 0.0489 - val_acc: 0.9582\n",
      "Epoch 392/500\n",
      "22272/22272 [==============================] - 12s 533us/step - loss: 0.0505 - acc: 0.9631 - val_loss: 0.0498 - val_acc: 0.9557\n",
      "Epoch 393/500\n",
      "22272/22272 [==============================] - 12s 522us/step - loss: 0.0497 - acc: 0.9621 - val_loss: 0.0501 - val_acc: 0.9574\n",
      "Epoch 394/500\n",
      "22272/22272 [==============================] - 12s 530us/step - loss: 0.0502 - acc: 0.9626 - val_loss: 0.0488 - val_acc: 0.9540\n",
      "Epoch 395/500\n",
      "22272/22272 [==============================] - 12s 518us/step - loss: 0.0506 - acc: 0.9617 - val_loss: 0.0492 - val_acc: 0.9599\n",
      "Epoch 396/500\n",
      "22272/22272 [==============================] - 12s 519us/step - loss: 0.0504 - acc: 0.9624 - val_loss: 0.0491 - val_acc: 0.9608\n",
      "Epoch 397/500\n",
      "22272/22272 [==============================] - 12s 538us/step - loss: 0.0499 - acc: 0.9637 - val_loss: 0.0474 - val_acc: 0.9625\n",
      "Epoch 398/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.0503 - acc: 0.9629 - val_loss: 0.0484 - val_acc: 0.9540\n",
      "Epoch 399/500\n",
      "22272/22272 [==============================] - 12s 521us/step - loss: 0.0497 - acc: 0.9637 - val_loss: 0.0481 - val_acc: 0.9582\n",
      "Epoch 400/500\n",
      "22272/22272 [==============================] - 13s 597us/step - loss: 0.0500 - acc: 0.9640 - val_loss: 0.0496 - val_acc: 0.9591\n",
      "Epoch 401/500\n",
      "22272/22272 [==============================] - 14s 615us/step - loss: 0.0496 - acc: 0.9639 - val_loss: 0.0495 - val_acc: 0.9531\n",
      "Epoch 402/500\n",
      "22272/22272 [==============================] - 14s 608us/step - loss: 0.0496 - acc: 0.9635 - val_loss: 0.0491 - val_acc: 0.9591\n",
      "Epoch 403/500\n",
      "22272/22272 [==============================] - 12s 549us/step - loss: 0.0494 - acc: 0.9640 - val_loss: 0.0498 - val_acc: 0.9582\n",
      "Epoch 404/500\n",
      "22272/22272 [==============================] - 14s 644us/step - loss: 0.0497 - acc: 0.9633 - val_loss: 0.0494 - val_acc: 0.9557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 405/500\n",
      "22272/22272 [==============================] - 13s 564us/step - loss: 0.0496 - acc: 0.9636 - val_loss: 0.0473 - val_acc: 0.9582\n",
      "Epoch 406/500\n",
      "22272/22272 [==============================] - 11s 516us/step - loss: 0.0494 - acc: 0.9623 - val_loss: 0.0493 - val_acc: 0.9625\n",
      "Epoch 407/500\n",
      "22272/22272 [==============================] - 12s 530us/step - loss: 0.0502 - acc: 0.9612 - val_loss: 0.0480 - val_acc: 0.9540\n",
      "Epoch 408/500\n",
      "22272/22272 [==============================] - 12s 539us/step - loss: 0.0494 - acc: 0.9626 - val_loss: 0.0487 - val_acc: 0.9531\n",
      "Epoch 409/500\n",
      "22272/22272 [==============================] - 12s 527us/step - loss: 0.0495 - acc: 0.9654 - val_loss: 0.0473 - val_acc: 0.9625\n",
      "Epoch 410/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0488 - acc: 0.9627 - val_loss: 0.0497 - val_acc: 0.9616\n",
      "Epoch 411/500\n",
      "22272/22272 [==============================] - 12s 545us/step - loss: 0.0491 - acc: 0.9653 - val_loss: 0.0473 - val_acc: 0.9591\n",
      "Epoch 412/500\n",
      "22272/22272 [==============================] - 12s 524us/step - loss: 0.0489 - acc: 0.9649 - val_loss: 0.0492 - val_acc: 0.9625\n",
      "Epoch 413/500\n",
      "22272/22272 [==============================] - 12s 545us/step - loss: 0.0485 - acc: 0.9640 - val_loss: 0.0479 - val_acc: 0.9582\n",
      "Epoch 414/500\n",
      "22272/22272 [==============================] - 13s 592us/step - loss: 0.0490 - acc: 0.9626 - val_loss: 0.0486 - val_acc: 0.9557\n",
      "Epoch 415/500\n",
      "22272/22272 [==============================] - 13s 585us/step - loss: 0.0489 - acc: 0.9633 - val_loss: 0.0498 - val_acc: 0.9642\n",
      "Epoch 416/500\n",
      "22272/22272 [==============================] - 13s 595us/step - loss: 0.0494 - acc: 0.9633 - val_loss: 0.0508 - val_acc: 0.9557\n",
      "Epoch 417/500\n",
      "22272/22272 [==============================] - 13s 598us/step - loss: 0.0493 - acc: 0.9626 - val_loss: 0.0507 - val_acc: 0.9548\n",
      "Epoch 418/500\n",
      "22272/22272 [==============================] - 13s 561us/step - loss: 0.0489 - acc: 0.9642 - val_loss: 0.0475 - val_acc: 0.9625\n",
      "Epoch 419/500\n",
      "22272/22272 [==============================] - 13s 578us/step - loss: 0.0488 - acc: 0.9643 - val_loss: 0.0488 - val_acc: 0.9616\n",
      "Epoch 420/500\n",
      "22272/22272 [==============================] - 13s 575us/step - loss: 0.0485 - acc: 0.9630 - val_loss: 0.0479 - val_acc: 0.9642\n",
      "Epoch 421/500\n",
      "22272/22272 [==============================] - 15s 656us/step - loss: 0.0482 - acc: 0.9659 - val_loss: 0.0474 - val_acc: 0.9582\n",
      "Epoch 422/500\n",
      "22272/22272 [==============================] - 14s 634us/step - loss: 0.0487 - acc: 0.9662 - val_loss: 0.0468 - val_acc: 0.9591\n",
      "Epoch 423/500\n",
      "22272/22272 [==============================] - 13s 579us/step - loss: 0.0483 - acc: 0.9643 - val_loss: 0.0488 - val_acc: 0.9565\n",
      "Epoch 424/500\n",
      "22272/22272 [==============================] - 12s 543us/step - loss: 0.0482 - acc: 0.9643 - val_loss: 0.0479 - val_acc: 0.9616\n",
      "Epoch 425/500\n",
      "22272/22272 [==============================] - 12s 537us/step - loss: 0.0483 - acc: 0.9644 - val_loss: 0.0491 - val_acc: 0.9574\n",
      "Epoch 426/500\n",
      "22272/22272 [==============================] - 14s 615us/step - loss: 0.0489 - acc: 0.9648 - val_loss: 0.0489 - val_acc: 0.9540\n",
      "Epoch 427/500\n",
      "22272/22272 [==============================] - 13s 573us/step - loss: 0.0483 - acc: 0.9657 - val_loss: 0.0478 - val_acc: 0.9557\n",
      "Epoch 428/500\n",
      "22272/22272 [==============================] - 13s 603us/step - loss: 0.0486 - acc: 0.9646 - val_loss: 0.0472 - val_acc: 0.9565\n",
      "Epoch 429/500\n",
      "22272/22272 [==============================] - 15s 663us/step - loss: 0.0484 - acc: 0.9643 - val_loss: 0.0479 - val_acc: 0.9582\n",
      "Epoch 430/500\n",
      "22272/22272 [==============================] - 12s 557us/step - loss: 0.0478 - acc: 0.9635 - val_loss: 0.0469 - val_acc: 0.9633\n",
      "Epoch 431/500\n",
      "22272/22272 [==============================] - 12s 542us/step - loss: 0.0481 - acc: 0.9647 - val_loss: 0.0471 - val_acc: 0.9668\n",
      "Epoch 432/500\n",
      "22272/22272 [==============================] - 12s 553us/step - loss: 0.0481 - acc: 0.9652 - val_loss: 0.0459 - val_acc: 0.9565\n",
      "Epoch 433/500\n",
      "22272/22272 [==============================] - 12s 540us/step - loss: 0.0484 - acc: 0.9654 - val_loss: 0.0473 - val_acc: 0.9599\n",
      "Epoch 434/500\n",
      "22272/22272 [==============================] - 13s 577us/step - loss: 0.0475 - acc: 0.9668 - val_loss: 0.0468 - val_acc: 0.9591\n",
      "Epoch 435/500\n",
      "22272/22272 [==============================] - 14s 626us/step - loss: 0.0477 - acc: 0.9650 - val_loss: 0.0466 - val_acc: 0.9616\n",
      "Epoch 436/500\n",
      "22272/22272 [==============================] - 13s 581us/step - loss: 0.0483 - acc: 0.9662 - val_loss: 0.0472 - val_acc: 0.9616\n",
      "Epoch 437/500\n",
      "22272/22272 [==============================] - 13s 577us/step - loss: 0.0479 - acc: 0.9649 - val_loss: 0.0474 - val_acc: 0.9599\n",
      "Epoch 438/500\n",
      "22272/22272 [==============================] - 12s 554us/step - loss: 0.0481 - acc: 0.9665 - val_loss: 0.0463 - val_acc: 0.9608\n",
      "Epoch 439/500\n",
      "22272/22272 [==============================] - 12s 557us/step - loss: 0.0478 - acc: 0.9665 - val_loss: 0.0469 - val_acc: 0.9557\n",
      "Epoch 440/500\n",
      "22272/22272 [==============================] - 12s 559us/step - loss: 0.0476 - acc: 0.9646 - val_loss: 0.0476 - val_acc: 0.9625\n",
      "Epoch 441/500\n",
      "22272/22272 [==============================] - 12s 517us/step - loss: 0.0480 - acc: 0.9660 - val_loss: 0.0460 - val_acc: 0.9574\n",
      "Epoch 442/500\n",
      "22272/22272 [==============================] - 12s 524us/step - loss: 0.0474 - acc: 0.9662 - val_loss: 0.0468 - val_acc: 0.9599\n",
      "Epoch 443/500\n",
      "22272/22272 [==============================] - 12s 532us/step - loss: 0.0475 - acc: 0.9658 - val_loss: 0.0469 - val_acc: 0.9582\n",
      "Epoch 444/500\n",
      "22272/22272 [==============================] - 12s 554us/step - loss: 0.0474 - acc: 0.9662 - val_loss: 0.0461 - val_acc: 0.9616\n",
      "Epoch 445/500\n",
      "22272/22272 [==============================] - 13s 573us/step - loss: 0.0477 - acc: 0.9659 - val_loss: 0.0476 - val_acc: 0.9608\n",
      "Epoch 446/500\n",
      "22272/22272 [==============================] - 13s 580us/step - loss: 0.0474 - acc: 0.9671 - val_loss: 0.0488 - val_acc: 0.9582\n",
      "Epoch 447/500\n",
      "22272/22272 [==============================] - 12s 559us/step - loss: 0.0476 - acc: 0.9656 - val_loss: 0.0468 - val_acc: 0.9616\n",
      "Epoch 448/500\n",
      "22272/22272 [==============================] - 12s 555us/step - loss: 0.0475 - acc: 0.9655 - val_loss: 0.0462 - val_acc: 0.9582\n",
      "Epoch 449/500\n",
      "22272/22272 [==============================] - 13s 578us/step - loss: 0.0471 - acc: 0.9653 - val_loss: 0.0467 - val_acc: 0.9557\n",
      "Epoch 450/500\n",
      "22272/22272 [==============================] - 13s 569us/step - loss: 0.0474 - acc: 0.9639 - val_loss: 0.0462 - val_acc: 0.9582\n",
      "Epoch 451/500\n",
      "22272/22272 [==============================] - 13s 597us/step - loss: 0.0475 - acc: 0.9640 - val_loss: 0.0472 - val_acc: 0.9599\n",
      "Epoch 452/500\n",
      "22272/22272 [==============================] - 12s 555us/step - loss: 0.0474 - acc: 0.9636 - val_loss: 0.0471 - val_acc: 0.9599\n",
      "Epoch 453/500\n",
      "22272/22272 [==============================] - 13s 588us/step - loss: 0.0471 - acc: 0.9634 - val_loss: 0.0473 - val_acc: 0.9599\n",
      "Epoch 454/500\n",
      "22272/22272 [==============================] - 14s 630us/step - loss: 0.0468 - acc: 0.9665 - val_loss: 0.0458 - val_acc: 0.9616\n",
      "Epoch 455/500\n",
      "22272/22272 [==============================] - 13s 574us/step - loss: 0.0470 - acc: 0.9669 - val_loss: 0.0458 - val_acc: 0.9642\n",
      "Epoch 456/500\n",
      "22272/22272 [==============================] - 13s 569us/step - loss: 0.0470 - acc: 0.9674 - val_loss: 0.0457 - val_acc: 0.9591\n",
      "Epoch 457/500\n",
      "22272/22272 [==============================] - 12s 556us/step - loss: 0.0468 - acc: 0.9661 - val_loss: 0.0477 - val_acc: 0.9608\n",
      "Epoch 458/500\n",
      "22272/22272 [==============================] - 13s 604us/step - loss: 0.0469 - acc: 0.9672 - val_loss: 0.0457 - val_acc: 0.9642\n",
      "Epoch 459/500\n",
      "22272/22272 [==============================] - 13s 593us/step - loss: 0.0473 - acc: 0.9638 - val_loss: 0.0462 - val_acc: 0.9642\n",
      "Epoch 460/500\n",
      "22272/22272 [==============================] - 12s 534us/step - loss: 0.0472 - acc: 0.9667 - val_loss: 0.0464 - val_acc: 0.9574\n",
      "Epoch 461/500\n",
      "22272/22272 [==============================] - 12s 530us/step - loss: 0.0473 - acc: 0.9643 - val_loss: 0.0448 - val_acc: 0.9548\n",
      "Epoch 462/500\n",
      "22272/22272 [==============================] - 12s 538us/step - loss: 0.0472 - acc: 0.9653 - val_loss: 0.0449 - val_acc: 0.9616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 463/500\n",
      "22272/22272 [==============================] - 12s 529us/step - loss: 0.0464 - acc: 0.9667 - val_loss: 0.0455 - val_acc: 0.9608\n",
      "Epoch 464/500\n",
      "22272/22272 [==============================] - 12s 523us/step - loss: 0.0466 - acc: 0.9672 - val_loss: 0.0466 - val_acc: 0.9599\n",
      "Epoch 465/500\n",
      "22272/22272 [==============================] - 11s 505us/step - loss: 0.0469 - acc: 0.9648 - val_loss: 0.0456 - val_acc: 0.9642\n",
      "Epoch 466/500\n",
      "22272/22272 [==============================] - 11s 509us/step - loss: 0.0466 - acc: 0.9681 - val_loss: 0.0449 - val_acc: 0.9591\n",
      "Epoch 467/500\n",
      "22272/22272 [==============================] - 11s 508us/step - loss: 0.0460 - acc: 0.9648 - val_loss: 0.0455 - val_acc: 0.9591\n",
      "Epoch 468/500\n",
      "22272/22272 [==============================] - 11s 515us/step - loss: 0.0466 - acc: 0.9662 - val_loss: 0.0454 - val_acc: 0.9591\n",
      "Epoch 469/500\n",
      "22272/22272 [==============================] - 11s 514us/step - loss: 0.0463 - acc: 0.9659 - val_loss: 0.0463 - val_acc: 0.9633\n",
      "Epoch 470/500\n",
      "22272/22272 [==============================] - 11s 513us/step - loss: 0.0463 - acc: 0.9660 - val_loss: 0.0447 - val_acc: 0.9625\n",
      "Epoch 471/500\n",
      "22272/22272 [==============================] - 12s 528us/step - loss: 0.0466 - acc: 0.9672 - val_loss: 0.0448 - val_acc: 0.9599\n",
      "Epoch 472/500\n",
      "22272/22272 [==============================] - 15s 658us/step - loss: 0.0465 - acc: 0.9648 - val_loss: 0.0466 - val_acc: 0.9608\n",
      "Epoch 473/500\n",
      "22272/22272 [==============================] - 12s 561us/step - loss: 0.0471 - acc: 0.9652 - val_loss: 0.0457 - val_acc: 0.9616\n",
      "Epoch 474/500\n",
      "22272/22272 [==============================] - 12s 546us/step - loss: 0.0462 - acc: 0.9666 - val_loss: 0.0457 - val_acc: 0.9582\n",
      "Epoch 475/500\n",
      "22272/22272 [==============================] - 13s 578us/step - loss: 0.0461 - acc: 0.9664 - val_loss: 0.0447 - val_acc: 0.9608\n",
      "Epoch 476/500\n",
      "22272/22272 [==============================] - 12s 553us/step - loss: 0.0460 - acc: 0.9674 - val_loss: 0.0448 - val_acc: 0.9608\n",
      "Epoch 477/500\n",
      "22272/22272 [==============================] - 12s 557us/step - loss: 0.0465 - acc: 0.9677 - val_loss: 0.0438 - val_acc: 0.9625\n",
      "Epoch 478/500\n",
      "22272/22272 [==============================] - 14s 637us/step - loss: 0.0460 - acc: 0.9675 - val_loss: 0.0448 - val_acc: 0.9650\n",
      "Epoch 479/500\n",
      "22272/22272 [==============================] - 14s 628us/step - loss: 0.0461 - acc: 0.9674 - val_loss: 0.0472 - val_acc: 0.9599\n",
      "Epoch 480/500\n",
      "22272/22272 [==============================] - 13s 591us/step - loss: 0.0459 - acc: 0.9678 - val_loss: 0.0456 - val_acc: 0.9599\n",
      "Epoch 481/500\n",
      "22272/22272 [==============================] - 14s 643us/step - loss: 0.0461 - acc: 0.9664 - val_loss: 0.0451 - val_acc: 0.9608\n",
      "Epoch 482/500\n",
      "22272/22272 [==============================] - 14s 623us/step - loss: 0.0460 - acc: 0.9660 - val_loss: 0.0454 - val_acc: 0.9676\n",
      "Epoch 483/500\n",
      "22272/22272 [==============================] - 13s 587us/step - loss: 0.0463 - acc: 0.9685 - val_loss: 0.0452 - val_acc: 0.9625\n",
      "Epoch 484/500\n",
      "22272/22272 [==============================] - 12s 555us/step - loss: 0.0460 - acc: 0.9666 - val_loss: 0.0447 - val_acc: 0.9608\n",
      "Epoch 485/500\n",
      "22272/22272 [==============================] - 12s 533us/step - loss: 0.0460 - acc: 0.9665 - val_loss: 0.0456 - val_acc: 0.9616\n",
      "Epoch 486/500\n",
      "22272/22272 [==============================] - 13s 583us/step - loss: 0.0454 - acc: 0.9672 - val_loss: 0.0451 - val_acc: 0.9616\n",
      "Epoch 487/500\n",
      "22272/22272 [==============================] - 12s 560us/step - loss: 0.0460 - acc: 0.9662 - val_loss: 0.0469 - val_acc: 0.9574\n",
      "Epoch 488/500\n",
      "22272/22272 [==============================] - 12s 554us/step - loss: 0.0461 - acc: 0.9669 - val_loss: 0.0457 - val_acc: 0.9599\n",
      "Epoch 489/500\n",
      "22272/22272 [==============================] - 15s 664us/step - loss: 0.0456 - acc: 0.9661 - val_loss: 0.0464 - val_acc: 0.9616\n",
      "Epoch 490/500\n",
      "22272/22272 [==============================] - 13s 603us/step - loss: 0.0455 - acc: 0.9696 - val_loss: 0.0445 - val_acc: 0.9633\n",
      "Epoch 491/500\n",
      "22272/22272 [==============================] - 14s 630us/step - loss: 0.0452 - acc: 0.9645 - val_loss: 0.0441 - val_acc: 0.9616\n",
      "Epoch 492/500\n",
      "22272/22272 [==============================] - 13s 575us/step - loss: 0.0453 - acc: 0.9682 - val_loss: 0.0464 - val_acc: 0.9608\n",
      "Epoch 493/500\n",
      "22272/22272 [==============================] - 12s 546us/step - loss: 0.0455 - acc: 0.9677 - val_loss: 0.0445 - val_acc: 0.9565\n",
      "Epoch 494/500\n",
      "22272/22272 [==============================] - 12s 534us/step - loss: 0.0456 - acc: 0.9679 - val_loss: 0.0443 - val_acc: 0.9608\n",
      "Epoch 495/500\n",
      "22272/22272 [==============================] - 12s 537us/step - loss: 0.0453 - acc: 0.9671 - val_loss: 0.0454 - val_acc: 0.9633\n",
      "Epoch 496/500\n",
      "22272/22272 [==============================] - 14s 624us/step - loss: 0.0455 - acc: 0.9687 - val_loss: 0.0450 - val_acc: 0.9633\n",
      "Epoch 497/500\n",
      "22272/22272 [==============================] - 13s 606us/step - loss: 0.0454 - acc: 0.9685 - val_loss: 0.0446 - val_acc: 0.9633\n",
      "Epoch 498/500\n",
      "22272/22272 [==============================] - 12s 535us/step - loss: 0.0459 - acc: 0.9666 - val_loss: 0.0447 - val_acc: 0.9685\n",
      "Epoch 499/500\n",
      "22272/22272 [==============================] - 12s 539us/step - loss: 0.0452 - acc: 0.9679 - val_loss: 0.0441 - val_acc: 0.9668\n",
      "Epoch 500/500\n",
      "22272/22272 [==============================] - 12s 538us/step - loss: 0.0459 - acc: 0.9652 - val_loss: 0.0438 - val_acc: 0.9650\n"
     ]
    }
   ],
   "source": [
    "##Fitting the GRU to the Training set\n",
    "hist6=model6.fit(X_train, y_train, batch_size=300, epochs=500, validation_split=0.05, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The computational time of GRU model is :- 2.1706716410268663\n"
     ]
    }
   ],
   "source": [
    "#Getting the predicted locations \n",
    "from timeit import default_timer as timer\n",
    "start3 = timer()\n",
    "pred_gru3=model3.predict(pred_x)\n",
    "duration3 = timer() - start3\n",
    "print('The computational time of GRU model is :-', duration3)\n",
    "\n",
    "predictions_trans2=y_scale.inverse_transform(pred_gru3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_gru3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f9e1f8c07b11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Performance Evaluations in 3 hiden layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RMSE:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_gru3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYy_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MAE:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_gru3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYy_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pred_gru3' is not defined"
     ]
    }
   ],
   "source": [
    "#Performance Evaluations in 3 hiden layers\n",
    "print(\"RMSE:\", sqrt(mean_squared_error(pred_gru3,Yy_pred)))\n",
    "print(\"MAE:\", mean_absolute_error(pred_gru3,Yy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The computational time of GRU model is :- 1.2381122234775694\n"
     ]
    }
   ],
   "source": [
    "#Getting the predicted locations \n",
    "from timeit import default_timer as timer\n",
    "start4 = timer()\n",
    "pred_gru4=model4.predict(pred_x)\n",
    "duration4 = timer() - start4\n",
    "print('The computational time of GRU model is :-', duration4)\n",
    "\n",
    "predictions_trans4=y_scale.inverse_transform(pred_gru4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The computational time of GRU model is :- 1.4357265615133348\n"
     ]
    }
   ],
   "source": [
    "#Getting the predicted locations \n",
    "from timeit import default_timer as timer\n",
    "start5 = timer()\n",
    "pred_gru5=model5.predict(pred_x)\n",
    "duration5 = timer() - start5\n",
    "print('The computational time of GRU model is :-', duration5)\n",
    "\n",
    "predictions_trans5=y_scale.inverse_transform(pred_gru5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The computational time of GRU model is :- 1.1507548580225215\n"
     ]
    }
   ],
   "source": [
    "#Getting the predicted locations \n",
    "from timeit import default_timer as timer\n",
    "start6 = timer()\n",
    "pred_gru6=model6.predict(pred_x)\n",
    "duration6 = timer() - start6\n",
    "print('The computational time of GRU model is :-', duration6)\n",
    "\n",
    "predictions_trans6=y_scale.inverse_transform(pred_gru6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loss graph \n",
    "plt.figure(figsize=(7,6))\n",
    "#plt.plot(hist3.history['loss'], 'r-', linewidth=1)\n",
    "plt.plot(hist3.history['val_loss'],'B', linewidth=1)\n",
    "\n",
    "\n",
    "#plt.plot(hist4.history['loss'],'k', linewidth=1)\n",
    "plt.plot(hist4.history['val_loss'],'y', linewidth=1)\n",
    "\n",
    "\n",
    "#plt.plot(hist5.history['loss'],'m', linewidth=1)\n",
    "plt.plot(hist5.history['val_loss'],'g', linewidth=1)\n",
    "\n",
    "#plt.plot(hist6.history['loss'],'m', linewidth=1)\n",
    "plt.plot(hist6.history['val_loss'],'g', linewidth=1)\n",
    "\n",
    "plt.title(\"Optimizers in GRU network\")\n",
    "plt.ylabel('Optimizer Losses (%)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['GRU_Adam_Validation', 'GRU_sgd_Validation', 'GRU_Rmsprop_Validation', 'GRU_adamax_Validation'], loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.savefig('MLP_LSTM_GRU_Trackloss.png', format='png', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Performance Graph\n",
    "plt.figure(figsize=(7,6))\n",
    "#plt.plot(hist1.history['acc'], 'r-', linewidth=1)\n",
    "plt.plot(hist3.history['val_acc'],'B', linewidth=1)\n",
    "\n",
    "\n",
    "#plt.plot(hist2.history['acc'],'k', linewidth=2)\n",
    "plt.plot(hist4.history['val_acc'],'y', linewidth=1)\n",
    "\n",
    "\n",
    "#plt.plot(hist3.history['acc'],'m', linewidth=2)\n",
    "plt.plot(hist5.history['val_acc'],'g', linewidth=1)\n",
    "\n",
    "plt.plot(hist6.history['val_acc'],'r', linewidth=1)\n",
    "\n",
    "\n",
    "plt.title(\"Loss of Training and Testing  Using DNN Algorithms\")\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['GRU_Adam_Validation', 'GRU_sgd_Validation', 'GRU_Rmsprop_Validation', 'GRU_adamax_Validation'], loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.savefig('MLP_LSTM_GRU_Track.png', format='png', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
